{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfeY_HEReJCP",
        "outputId": "3ef0a508-a57d-4d32-b18b-4af1e761c00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mesa\n",
            "  Downloading mesa-3.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mesa) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from mesa) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mesa) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mesa) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->mesa) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->mesa) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->mesa) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->mesa) (1.17.0)\n",
            "Downloading mesa-3.4.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mesa\n",
            "Successfully installed mesa-3.4.0\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install mesa\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spektral"
      ],
      "metadata": {
        "id": "c1MWtABIduNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5db60d-6174-472e-a80f-376608c73ce3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spektral\n",
            "  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from spektral) (1.5.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from spektral) (6.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from spektral) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spektral) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from spektral) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from spektral) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from spektral) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spektral) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from spektral) (4.67.1)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from spektral) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow>=2.2.0->spektral) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->spektral) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->spektral) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->spektral) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->spektral) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->spektral) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->spektral) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.18.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2.0->spektral) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2.0->spektral) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow>=2.2.0->spektral) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.2.0->spektral) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.2.0->spektral) (0.1.2)\n",
            "Downloading spektral-1.3.1-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spektral\n",
            "Successfully installed spektral-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDaGx7Tamhi2",
        "outputId": "38a8a23e-2977-49e8-f24c-3e11bba97268"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mesa\n",
        "import gymnasium as gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "ekOjZUm5eTot"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scaler(x,a,b):\n",
        "    return (x-a)/(b-a)\n",
        "def log_scaler(x):\n",
        "    return np.log(x+1)\n",
        "\n",
        "def sigmoid(max_effect, saturation_rate, t):\n",
        "        return max_effect / (1 + np.exp(-saturation_rate * (t - 10)))\n",
        "\n",
        "def sigmoid_transfer(x, steepness=10):\n",
        "    return 1 / (1 + np.exp(-steepness * np.array(x, dtype=float)))\n",
        "\n",
        "def get_effect_growth(value, ranges):\n",
        "    ranges = sorted(ranges, key=lambda x: x[0])\n",
        "\n",
        "    for i in range(len(ranges)):\n",
        "        low, high, effect, growth = ranges[i]\n",
        "\n",
        "        if low <= value < high:\n",
        "            proportion = (value - low) / (high - low)\n",
        "            if i + 1 < len(ranges):\n",
        "                next_effect = ranges[i+1][2]\n",
        "                next_growth = ranges[i+1][3]\n",
        "                smooth_effect = effect + (proportion * (next_effect - effect))\n",
        "                smooth_growth = growth + (proportion * (next_growth - growth))\n",
        "                return smooth_effect, smooth_growth\n",
        "\n",
        "            return effect, growth\n",
        "\n",
        "    return 0, 0"
      ],
      "metadata": {
        "id": "uwGOqxRQFNLa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "class GeographicRegion(gym.Env):\n",
        "    def __init__(self, seed=None):\n",
        "        self.seed = seed\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Observation space\n",
        "        self.state_space = gym.spaces.Dict({\n",
        "            \"Temperature\": gym.spaces.Box(low=-30, high=60, dtype=np.float32),\n",
        "            \"Precipitation\": gym.spaces.Box(low=0.25, high=50, dtype=np.float32),\n",
        "            \"Humidity\": gym.spaces.Box(low=0, high=100,  dtype=np.float32),\n",
        "            \"Air Pollution Index\": gym.spaces.Box(low=0, high=500,dtype=np.int16),\n",
        "            \"Water Quality Index\": gym.spaces.Box(low=0, high=100, dtype=np.int16),\n",
        "            \"Carbon Dioxide Emissions Per Capita\": gym.spaces.Box(low=1, high=15, dtype=np.float32),\n",
        "            \"Electricity Consumption Per Capita\": gym.spaces.Box(low=250, high=12000, dtype=np.float32),\n",
        "            \"Renewable Share\": gym.spaces.Box(low=0, high=1, dtype=np.float32),\n",
        "            \"Water Consumption Per Capita\": gym.spaces.Box(low=20, high=350, dtype=np.float32),\n",
        "            \"Population\": gym.spaces.Box(low=1, high=37e6, dtype=np.float32),\n",
        "            \"GDP\": gym.spaces.Box(low=1e9, high=1e12,dtype=np.float32),\n",
        "            \"Employment Rate\": gym.spaces.Box(low=0, high=1, dtype=np.float32),\n",
        "            \"Waste Management Efficiency\": gym.spaces.Box(low=0, high=1, dtype=np.float32),\n",
        "            \"Energy Efficiency\": gym.spaces.Box(low=0, high=1,dtype=np.float32),\n",
        "        })\n",
        "\n",
        "        # Action space\n",
        "        self.action_space = gym.spaces.Dict({\n",
        "            \"Carbon Tax\": gym.spaces.Box(low=0, high=200, dtype=np.float32),\n",
        "            \"Renewable energy subsidies\": gym.spaces.Box(low=0, high=1, dtype=np.float32),\n",
        "            \"Water Consumption Tax\": gym.spaces.Box(low=0.05, high=6, dtype=np.float32),\n",
        "            \"Energy Efficiency Incentives\": gym.spaces.Box(low=0, high=1,dtype=np.float32),\n",
        "            \"Fossil Fuel Phase-Out Regulations\": gym.spaces.Discrete(3),\n",
        "            \"Electric Vehicle (EV) Subsidies\": gym.spaces.Box(low=0, high=1,dtype=np.float32),\n",
        "            \"Fuel Economy Standards\": gym.spaces.Discrete(4),\n",
        "            \"Urban Green Space Expansion\": gym.spaces.Box(low=1, high=200,dtype=np.float32),\n",
        "            \"Climate-Resilient Infrastructure Investment\": gym.spaces.Box(low=1e6, high=500e6, dtype=np.float32),\n",
        "            \"Sustainable Land-Use Zoning\": gym.spaces.Box(low=0.1e6, high=100e6,dtype=np.float32),\n",
        "            \"Waste Management Reforms\": gym.spaces.Box(low=1e6, high=100e6,dtype=np.float32),\n",
        "            \"Public transport expansion\": gym.spaces.Box(low=1e6, high=1e9,dtype=np.float32),\n",
        "            \"Vehicle emission standards\": gym.spaces.Discrete(4),\n",
        "            \"Flood defense infrastructure\": gym.spaces.Box(low=0.25e6, high=300e6,dtype=np.float32),\n",
        "            \"Heatwave resilience\": gym.spaces.Box(low=0.2e6, high=200e6, dtype=np.float32),\n",
        "            \"Water conservation measures\": gym.spaces.Box(low=0.1e6, high=100e6, dtype=np.float32),\n",
        "            \"Recycling Rate\": gym.spaces.Box(low=0, high=1,  dtype=np.float32),\n",
        "            \"Single-use plastics bans\": gym.spaces.Discrete(3),\n",
        "            \"Green Business Investments\": gym.spaces.Box(low=0.1e6, high=100e6,dtype=np.float32)\n",
        "        })\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = {\n",
        "            \"Temperature\": np.random.uniform(15, 30),\n",
        "            \"Precipitation\": np.random.uniform(0.5, 5),\n",
        "            \"Humidity\": np.random.uniform(40, 70),\n",
        "            \"Air Pollution Index\": np.random.uniform(100,200),\n",
        "            \"Water Quality Index\": np.random.uniform(30,60),\n",
        "            \"Carbon Dioxide Emissions Per Capita\": np.random.uniform(5, 8),\n",
        "            \"Electricity Consumption Per Capita\": np.random.uniform(500, 2000),\n",
        "            \"Renewable Share\": np.random.uniform(0.2, 0.4),\n",
        "            \"Water Consumption Per Capita\": np.random.uniform(50, 150),\n",
        "            \"Population\": np.random.randint(1e6, 10e6),\n",
        "            \"GDP\": np.random.uniform(1e10, 5e11),\n",
        "            \"Employment Rate\": np.random.uniform(0.8, 0.95),\n",
        "            \"Waste Management Efficiency\": np.random.uniform(0.6, 0.8),\n",
        "            \"Energy Efficiency\": np.random.uniform(0.5, 0.7),\n",
        "            \"Urban Green Space Expansion\": np.random.uniform(20, 50),\n",
        "            \"Flood defense infrastructure\": np.random.uniform(1e6, 50e6),\n",
        "            \"Heatwave resilience\": np.random.uniform(1e5, 20e6),\n",
        "            \"Sustainable Land-Use Zoning\": np.random.uniform(1e5, 20e6),\n",
        "            \"Single-use plastics bans\": 0,\n",
        "            \"Green Business Investments\": np.random.uniform(1e5, 20e6)\n",
        "        }\n",
        "        self.t = 0\n",
        "        return self.state,{}\n",
        "\n",
        "    def scale_state(self):\n",
        "        state=self.state.copy()\n",
        "        for key, value in self.state.items():\n",
        "            if key in [\"Temperature\", \"Precipitation\", \"Humidity\", \"Air Pollution Index\",\n",
        "                      \"Water Quality Index\", \"Carbon Dioxide Emissions Per Capita\",\n",
        "                      \"Electricity Consumption Per Capita\",\"Water Consumption Per Capita\"]:\n",
        "                min_value = self.state_space[key].low\n",
        "                max_value = self.state_space[key].high\n",
        "                state[key] = min_max_scaler(value, min_value, max_value)\n",
        "            elif key in [\"Population\", \"GDP\"]:\n",
        "                state[key] = log_scaler(value)\n",
        "            else:\n",
        "                state[key] = value\n",
        "        return state\n",
        "\n",
        "    def scale_action(self, action):\n",
        "      for key, value in action.items():\n",
        "          if key in [\"Carbon Tax\", \"Water Consumption Tax\", \"Urban Green Space Expansion\",\n",
        "                      \"Climate-Resilient Infrastructure Investment\",  \"Sustainable Land-Use Zoning\",\n",
        "                      \"Waste Management Reforms\", \"Public transport expansion\", \"Flood defense infrastructure\",\n",
        "                      \"Heatwave resilience\", \"Water conservation measures\", \"Green Business Investments\"]:\n",
        "              min_value = self.action_space[key].low\n",
        "              max_value = self.action_space[key].high\n",
        "              if isinstance(value, np.ndarray):\n",
        "                  value = value.item()\n",
        "              action[key] = min_max_scaler(value, min_value, max_value)\n",
        "              action[key] = np.clip(value, min_value, max_value)\n",
        "      return action\n",
        "\n",
        "    def environmental_impact(self, action, t):\n",
        "\n",
        "        carbon = -((action[\"Carbon Tax\"] - 150) / 100)**2\n",
        "        vehicle= -((action[\"Vehicle emission standards\"]-3)/100)**2\n",
        "        fossil= -((action[\"Fossil Fuel Phase-Out Regulations\"]-2)/100)**2\n",
        "        plastic= -((action[\"Single-use plastics bans\"]-2)/100)**2\n",
        "\n",
        "        total_environmental_impact = (\n",
        "            0.3 * carbon +\n",
        "            0.2 * vehicle +\n",
        "            0.2 * fossil +\n",
        "            0.1 * plastic\n",
        "        )\n",
        "        return total_environmental_impact\n",
        "\n",
        "    def energy_efficiency(self, action, t):\n",
        "        gdp = self.state[\"GDP\"]\n",
        "        consumption = self.state[\"Electricity Consumption Per Capita\"]\n",
        "        intensity_score = (np.log1p(gdp / (consumption + 1e-6)) * 0.5)\n",
        "\n",
        "        env = self.environmental_impact(action, t)\n",
        "        initial_base = 4000\n",
        "        current_base = initial_base\n",
        "        efficiency_rate = 0.05\n",
        "\n",
        "        efficiency_rate = 0.3 * (sigmoid_transfer(env) - 0.5) + 0.05\n",
        "        current_base = initial_base * (1.2 - sigmoid_transfer(env) * 0.4)\n",
        "        electricity_consumption = current_base * (1 - efficiency_rate * t)\n",
        "        consumption_efficiency_score = (initial_base - electricity_consumption) / initial_base\n",
        "\n",
        "        subsidy_input = action[\"Renewable energy subsidies\"]\n",
        "        renewable_impact = np.log1p(subsidy_input) / 5.0\n",
        "        renewable = renewable_impact * sigmoid(0.7, 0.05, t)\n",
        "\n",
        "        total_energy_impact = (\n",
        "            0.4 * intensity_score +\n",
        "            0.3 * float(consumption_efficiency_score) +\n",
        "            0.3 * renewable\n",
        "        )\n",
        "\n",
        "        return total_energy_impact\n",
        "\n",
        "    def resource_management(self, action, t):\n",
        "        water_val = action[\"Water conservation measures\"]\n",
        "        recycling_val = action[\"Recycling Rate\"]\n",
        "        waste_val = action[\"Waste Management Reforms\"]\n",
        "\n",
        "        water = (2 / (1 + np.exp(-5 * (water_val - 0.4))) - 1) * (1 + 0.1 * t)\n",
        "        recycling = (2 / (1 + np.exp(-5 * (recycling_val - 0.4))) - 1) * (1 + 0.1 * t)\n",
        "        waste = (2 / (1 + np.exp(-5 * (waste_val / 1e8 - 0.5))) - 1) * (1 + 0.1 * t)\n",
        "\n",
        "        return (water**0.4 * recycling**0.35 * waste**0.25) if (water > 0 and recycling > 0 and waste > 0) else (0.4 * water + 0.35 * recycling + 0.25 * waste)\n",
        "\n",
        "    def economic_growth(self, action, t):\n",
        "        infra = action[\"Climate-Resilient Infrastructure Investment\"]\n",
        "        transport = action[\"Public transport expansion\"]\n",
        "        green = action[\"Green Business Investments\"]\n",
        "\n",
        "        infra_score = (np.log1p(infra / 1e8) / 2.0 - 0.5) * (1 + 0.05 * t)\n",
        "        transport_score = (np.log1p(transport / 2e8) / 2.0 - 0.4) * (1 + 0.05 * t)\n",
        "        green_score = (np.log1p(green / 5e7) / 2.0 - 0.3) * (1 + 0.05 * t)\n",
        "\n",
        "        normalized_gdp = np.log1p(self.state[\"GDP\"]) / 25.0\n",
        "        employment = self.state[\"Employment Rate\"]\n",
        "\n",
        "        economic_utility = (normalized_gdp**0.6 * employment**0.4)\n",
        "        policy_impact = (0.4 * infra_score + 0.3 * transport_score + 0.3 * green_score)\n",
        "\n",
        "        return economic_utility + policy_impact\n",
        "\n",
        "    def climate_resilience(self, action, t):\n",
        "        flood_val = np.clip(action[\"Flood defense infrastructure\"] / 3e8, 1e-6, 1.0)\n",
        "        heat_val = np.clip(action[\"Heatwave resilience\"] / 2e8, 1e-6, 1.0)\n",
        "        urban_val = np.clip(action[\"Urban Green Space Expansion\"] / 200, 1e-6, 1.0)\n",
        "\n",
        "        resilience_base = (flood_val * heat_val * urban_val) ** (1/3)\n",
        "        boosted_base = (resilience_base ** 2) * 5.0\n",
        "        time_growth = 1 + (0.15 * t / 50)\n",
        "        return (boosted_base * time_growth) - 0.5\n",
        "\n",
        "    def _get_info(self, action=None):\n",
        "        return {\n",
        "            \"environmental_impact\": self.environmental_impact(action, self.t),\n",
        "            \"energy_efficiency\": self.energy_efficiency(action, self.t),\n",
        "            \"resource_management\": self.resource_management(action, self.t),\n",
        "            \"economic_growth\": self.economic_growth(action, self.t),\n",
        "            \"climate_resilience\": self.climate_resilience(action, self.t),\n",
        "            # raw state\n",
        "            \"air_pollution_index\": self.state[\"Air Pollution Index\"],\n",
        "            \"carbon_emissions_per_capita\": self.state[\"Carbon Dioxide Emissions Per Capita\"],\n",
        "            \"gdp\": self.state[\"GDP\"],\n",
        "            \"employment_rate\": self.state[\"Employment Rate\"],\n",
        "            \"electricity_consumption\": self.state[\"Electricity Consumption Per Capita\"],\n",
        "            \"water_quality\": self.state[\"Water Quality Index\"],\n",
        "            \"waste_efficiency\": self.state[\"Waste Management Efficiency\"],\n",
        "        }\n",
        "\n",
        "    def reward(self, action):\n",
        "        s = self.state\n",
        "        env_score = self.environmental_impact(action, self.t)\n",
        "        econ_score = self.economic_growth(action, self.t)\n",
        "        energy_score = self.energy_efficiency(action, self.t)\n",
        "        res_score = self.resource_management(action, self.t)\n",
        "        clim_score = self.climate_resilience(action, self.t)\n",
        "\n",
        "        total_utility = (\n",
        "        (env_score * 5.0) +\n",
        "        (energy_score * 2.0) +\n",
        "        (res_score * 2.0) +\n",
        "        (clim_score * 2.0)\n",
        "    )\n",
        "        current_potential = (0.4 * s[\"Employment Rate\"] + 0.6 * (1 - s[\"Carbon Dioxide Emissions Per Capita\"]/12))\n",
        "        policy_momentum = (current_potential - getattr(self, \"prev_potential\", current_potential)) * 20 # Multiplier\n",
        "        self.prev_potential = current_potential\n",
        "\n",
        "        boundary_penalty = 0\n",
        "        if s[\"Temperature\"] > 35:\n",
        "            boundary_penalty -= np.exp((s[\"Temperature\"] - 35) / 5)\n",
        "        if s[\"Carbon Dioxide Emissions Per Capita\"] > 9:\n",
        "            boundary_penalty -= np.exp(s[\"Carbon Dioxide Emissions Per Capita\"] - 9)\n",
        "\n",
        "        return total_utility + policy_momentum + boundary_penalty\n",
        "\n",
        "    def step(self, action):\n",
        "      self.t += 1\n",
        "      self.last_action = action\n",
        "\n",
        "      n_act = {}\n",
        "      for k, v in action.items():\n",
        "          val = v.item() if isinstance(v, (np.ndarray, np.int64)) else v\n",
        "          if k in self.action_space.spaces and isinstance(self.action_space[k], gym.spaces.Box):\n",
        "              low, high = self.action_space[k].low[0], self.action_space[k].high[0]\n",
        "              n_act[k] = np.clip((val - low) / (high - low + 1e-8), 0, 1)\n",
        "          else:\n",
        "              n_act[k] = val\n",
        "\n",
        "      state = self.state\n",
        "\n",
        "      old_state_val = (state[\"Employment Rate\"] + (1 - state[\"Carbon Dioxide Emissions Per Capita\"]/12)) / 2\n",
        "      state[\"Carbon Dioxide Emissions Per Capita\"] *= (1.0 - (0.02 * n_act[\"Carbon Tax\"] + 0.05 * n_act[\"Renewable energy subsidies\"]))\n",
        "      state[\"Electricity Consumption Per Capita\"] *= (1.0 - (0.01 * n_act[\"Energy Efficiency Incentives\"] + 0.008 * n_act[\"Renewable energy subsidies\"]))\n",
        "\n",
        "      state[\"Air Pollution Index\"] *= (1.0 - (0.04 * n_act[\"Vehicle emission standards\"] + 0.03 * n_act[\"Fossil Fuel Phase-Out Regulations\"]))\n",
        "      state[\"Water Quality Index\"] *= (1.0 + 0.003 * n_act[\"Water conservation measures\"] + 0.002 * n_act[\"Water Consumption Tax\"])\n",
        "\n",
        "      pollution_drag = np.log1p(state[\"Air Pollution Index\"] / 100) * 0.005\n",
        "      growth_multiplier = (1.0 + 0.000000005 * n_act[\"Climate-Resilient Infrastructure Investment\"]\n",
        "                              + 0.00000001 * n_act[\"Public transport expansion\"]\n",
        "                              + 0.000000015 * n_act[\"Green Business Investments\"]) - pollution_drag\n",
        "      state[\"GDP\"] *= growth_multiplier\n",
        "\n",
        "      norm_gdp = state[\"GDP\"] / 1e11\n",
        "      state[\"Employment Rate\"] = np.clip((norm_gdp**0.1) * (state[\"Waste Management Efficiency\"]**0.05), 0.0, 1.0)\n",
        "\n",
        "      state[\"Waste Management Efficiency\"] *= (1.0 + 0.01 * n_act[\"Recycling Rate\"] + 0.005 * n_act[\"Waste Management Reforms\"])\n",
        "      state[\"Urban Green Space Expansion\"] *= (1.0 + 0.01 * n_act[\"Urban Green Space Expansion\"])\n",
        "      state[\"Flood defense infrastructure\"] *= (1.0 + 0.01 * n_act[\"Flood defense infrastructure\"])\n",
        "      state[\"Heatwave resilience\"] *= (1.0 + 0.01 * n_act[\"Heatwave resilience\"])\n",
        "      state[\"Sustainable Land-Use Zoning\"] *= (1.0 + 0.01 * n_act[\"Sustainable Land-Use Zoning\"])\n",
        "      state[\"Single-use plastics bans\"] *= (1.0 + 0.05 * n_act[\"Single-use plastics bans\"])\n",
        "      state[\"Green Business Investments\"] *= (1.0 + 0.01 * n_act[\"Green Business Investments\"])\n",
        "\n",
        "      cooling = (0.1 * n_act[\"Carbon Tax\"] + 0.2 * n_act[\"Renewable energy subsidies\"] +\n",
        "                0.05 * n_act[\"Fossil Fuel Phase-Out Regulations\"] + 0.1 * n_act[\"Heatwave resilience\"] +\n",
        "                0.2 * n_act[\"Urban Green Space Expansion\"] + 0.05 * n_act[\"Sustainable Land-Use Zoning\"])\n",
        "      heating = 0.1 * n_act[\"Green Business Investments\"] + (state[\"Air Pollution Index\"] / 1000)\n",
        "\n",
        "      state[\"Temperature\"] += (heating - cooling) + 0.5 * np.sin(np.pi * (self.t / 365))\n",
        "\n",
        "      carbon_barrier = -np.log(max(1e-6, 12 - state[\"Carbon Dioxide Emissions Per Capita\"])) * 0.2\n",
        "      temp_barrier = -np.log(max(1e-6, 50 - state[\"Temperature\"])) * 0.2\n",
        "\n",
        "      for key, value in state.items():\n",
        "          if key in self.state_space:\n",
        "              state[key] = np.clip(value, self.state_space[key].low[0], self.state_space[key].high[0])\n",
        "\n",
        "      scaled_state = self.scale_state().copy()\n",
        "\n",
        "      new_state_val = (state[\"Employment Rate\"] + (1 - state[\"Carbon Dioxide Emissions Per Capita\"]/12)) / 2\n",
        "      policy_change_reward = (new_state_val - old_state_val) * 10\n",
        "\n",
        "      reward = self.reward(action) + policy_change_reward + carbon_barrier + temp_barrier\n",
        "\n",
        "      done = (state[\"Temperature\"] > 50 or state[\"GDP\"] < 1e9 or state[\"Air Pollution Index\"] > 300 or self.t >= 50)\n",
        "      truncated = self.t >= 50\n",
        "\n",
        "      return state, scaled_state, reward, truncated, done, self._get_info(action)\n"
      ],
      "metadata": {
        "id": "pgfROUfcp0-u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Synthetic Dataset**"
      ],
      "metadata": {
        "id": "S5e8XpKRsdJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "np.random.seed(42)\n",
        "N = 400\n",
        "unique_id = [i for i in range(N)]\n",
        "business_name = [f\"Company_{i+1}\" for i in range(N)]\n",
        "\n",
        "sectors = [\n",
        "    \"Technology\",\n",
        "    \"Telecommunications\",\n",
        "    \"Industrial Manufacturing\",\n",
        "    \"Automotive & Mobility\",\n",
        "    \"Aerospace\",\n",
        "    \"Defense & Security\",\n",
        "    \"Construction\",\n",
        "    \"Real Estate & Infrastructure\",\n",
        "    \"Energy & Utilities\",\n",
        "    \"Mining & Natural Resources\",\n",
        "    \"Chemicals & Advanced Materials\",\n",
        "    \"Healthcare & Life Sciences\",\n",
        "    \"Agriculture & Agri-business\",\n",
        "    \"Food & Beverage\",\n",
        "    \"Finance & Banking\",\n",
        "    \"Transportation & Logistics\"\n",
        "]\n",
        "\n",
        "business_type = np.tile(sectors, N // len(sectors))\n",
        "np.random.shuffle(business_type)\n",
        "revenue = []\n",
        "valuation = []\n",
        "growth_rate = []\n",
        "sustainability_index = []\n",
        "energy_consumption = []\n",
        "\n",
        "for sector in business_type:\n",
        "    if sector == \"Technology\":\n",
        "        revenue.append(np.random.lognormal(mean=20, sigma=1.0))\n",
        "        growth_rate.append(np.random.randint(10, 50))\n",
        "        sustainability_index.append(np.random.randint(40, 80))\n",
        "        energy_consumption.append(np.random.randint(50_000, 500_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(5, 20))\n",
        "    elif sector == \"Telecommunications\":\n",
        "        revenue.append(np.random.lognormal(mean=18, sigma=0.8))\n",
        "        growth_rate.append(np.random.randint(5, 30))\n",
        "        sustainability_index.append(np.random.randint(30, 70))\n",
        "        energy_consumption.append(np.random.randint(100_000, 800_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(3, 12))\n",
        "    elif sector == \"Industrial Manufacturing\":\n",
        "        revenue.append(np.random.lognormal(mean=19, sigma=0.9))\n",
        "        growth_rate.append(np.random.randint(2, 20))\n",
        "        sustainability_index.append(np.random.randint(10, 40))\n",
        "        energy_consumption.append(np.random.randint(200_000, 1_000_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(2, 8))\n",
        "    elif sector == \"Automotive & Mobility\":\n",
        "        revenue.append(np.random.lognormal(mean=21, sigma=1.2))\n",
        "        growth_rate.append(np.random.randint(5, 25))\n",
        "        sustainability_index.append(np.random.randint(20, 60))\n",
        "        energy_consumption.append(np.random.randint(100_000, 900_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(5, 15))\n",
        "    elif sector == \"Aerospace\":\n",
        "        revenue.append(np.random.lognormal(mean=22, sigma=1.5))\n",
        "        growth_rate.append(np.random.randint(5, 30))\n",
        "        sustainability_index.append(np.random.randint(10, 40))\n",
        "        energy_consumption.append(np.random.randint(200_000, 1_000_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(10, 25))\n",
        "    elif sector == \"Defense & Security\":\n",
        "        revenue.append(np.random.lognormal(mean=23, sigma=1.3))\n",
        "        growth_rate.append(np.random.randint(5, 15))\n",
        "        sustainability_index.append(np.random.randint(5, 25))\n",
        "        energy_consumption.append(np.random.randint(150_000, 800_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(8, 20))\n",
        "    elif sector == \"Construction\":\n",
        "        revenue.append(np.random.lognormal(mean=19, sigma=1.0))\n",
        "        growth_rate.append(np.random.randint(5, 20))\n",
        "        sustainability_index.append(np.random.randint(30, 60))\n",
        "        energy_consumption.append(np.random.randint(200_000, 900_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(4, 12))\n",
        "    elif sector == \"Real Estate & Infrastructure\":\n",
        "        revenue.append(np.random.lognormal(mean=18.5, sigma=0.9))\n",
        "        growth_rate.append(np.random.randint(5, 30))\n",
        "        sustainability_index.append(np.random.randint(40, 70))\n",
        "        energy_consumption.append(np.random.randint(100_000, 500_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(5, 15))\n",
        "    elif sector == \"Energy & Utilities\":\n",
        "        revenue.append(np.random.lognormal(mean=22, sigma=1.1))\n",
        "        growth_rate.append(np.random.randint(2, 15))\n",
        "        sustainability_index.append(np.random.randint(10, 50))\n",
        "        energy_consumption.append(np.random.randint(300_000, 1_500_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(3, 10))\n",
        "    elif sector == \"Mining & Natural Resources\":\n",
        "        revenue.append(np.random.lognormal(mean=20, sigma=1.0))\n",
        "        growth_rate.append(np.random.randint(0, 20))\n",
        "        sustainability_index.append(np.random.randint(5, 30))\n",
        "        energy_consumption.append(np.random.randint(400_000, 2_000_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(5, 18))\n",
        "    elif sector == \"Chemicals & Advanced Materials\":\n",
        "        revenue.append(np.random.lognormal(mean=19, sigma=1.1))\n",
        "        growth_rate.append(np.random.randint(5, 25))\n",
        "        sustainability_index.append(np.random.randint(20, 60))\n",
        "        energy_consumption.append(np.random.randint(200_000, 1_200_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(4, 12))\n",
        "    elif sector == \"Healthcare & Life Sciences\":\n",
        "        revenue.append(np.random.lognormal(mean=18, sigma=0.8))\n",
        "        growth_rate.append(np.random.randint(10, 40))\n",
        "        sustainability_index.append(np.random.randint(50, 90))\n",
        "        energy_consumption.append(np.random.randint(50_000, 500_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(3, 10))\n",
        "    elif sector == \"Agriculture & Agri-business\":\n",
        "        revenue.append(np.random.lognormal(mean=17, sigma=1.0))\n",
        "        growth_rate.append(np.random.randint(0, 20))\n",
        "        sustainability_index.append(np.random.randint(30, 70))\n",
        "        energy_consumption.append(np.random.randint(100_000, 600_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(4, 12))\n",
        "    elif sector == \"Food & Beverage\":\n",
        "        revenue.append(np.random.lognormal(mean=19, sigma=0.9))\n",
        "        growth_rate.append(np.random.randint(5, 30))\n",
        "        sustainability_index.append(np.random.randint(40, 80))\n",
        "        energy_consumption.append(np.random.randint(100_000, 800_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(4, 15))\n",
        "    elif sector == \"Finance & Banking\":\n",
        "        revenue.append(np.random.lognormal(mean=22, sigma=1.2))\n",
        "        growth_rate.append(np.random.randint(2, 15))\n",
        "        sustainability_index.append(np.random.randint(30, 60))\n",
        "        energy_consumption.append(np.random.randint(50_000, 300_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(10, 30))\n",
        "    elif sector == \"Transportation & Logistics\":\n",
        "        revenue.append(np.random.lognormal(mean=20, sigma=1.0))\n",
        "        growth_rate.append(np.random.randint(5, 30))\n",
        "        sustainability_index.append(np.random.randint(10, 50))\n",
        "        energy_consumption.append(np.random.randint(300_000, 1_200_000))\n",
        "        valuation.append(revenue[-1] * np.random.uniform(5, 15))\n",
        "\n",
        "business = pd.DataFrame({\n",
        "    \"unique_id\": unique_id,\n",
        "    \"business_name\": business_name,\n",
        "    \"business_type\": business_type,\n",
        "    \"revenue\": np.round(revenue, 2),\n",
        "    \"valuation\": np.round(valuation, 2),\n",
        "    \"growth_rate\": growth_rate,\n",
        "    \"sustainability_index\": sustainability_index,\n",
        "    \"energy_consumption\": energy_consumption\n",
        "})\n",
        "\n",
        "print(business.head())\n",
        "print(business[\"business_type\"].value_counts())\n",
        "print(business.shape)\n",
        "\n",
        "energy_types = ['Solar', 'Nuclear', 'Hydro', 'Fossil Fuels', 'Experimental', 'Geothermal', 'Wind', 'Biomass']\n",
        "energy_type = np.tile(energy_types, N // len(energy_types))\n",
        "np.random.shuffle(energy_type)\n",
        "\n",
        "capacity = []\n",
        "efficiency = []\n",
        "production = []\n",
        "carbon_intensity = []\n",
        "emissions = []\n",
        "\n",
        "for etype in energy_type:\n",
        "    if etype == 'Solar':\n",
        "        capacity.append(np.random.uniform(10, 100))\n",
        "        efficiency.append(np.random.uniform(0.15, 0.25))\n",
        "        carbon_intensity.append(np.random.normal(0.05, 0.02))\n",
        "        emissions.append(np.random.normal(0.01, 0.005))\n",
        "        production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "    elif etype == 'Nuclear':\n",
        "        capacity.append(np.random.uniform(1000, 5000))\n",
        "        efficiency.append(np.random.uniform(0.85, 0.95))\n",
        "        carbon_intensity.append(np.random.normal(0.1, 0.05))\n",
        "        emissions.append(np.random.normal(0.05, 0.02))\n",
        "        production.append(np.random.lognormal(mean=6, sigma=0.7))\n",
        "    elif etype == 'Hydro':\n",
        "        capacity.append(np.random.uniform(500, 3000))\n",
        "        efficiency.append(np.random.uniform(0.6, 0.85))\n",
        "        carbon_intensity.append(np.random.normal(0.1, 0.05))\n",
        "        emissions.append(np.random.normal(0.05, 0.02))\n",
        "        production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "    elif etype == 'Fossil Fuels':\n",
        "        capacity.append(np.random.uniform(500, 5000))\n",
        "        efficiency.append(np.random.uniform(0.3, 0.45))\n",
        "        carbon_intensity.append(np.random.normal(0.9, 0.1))\n",
        "        emissions.append(np.random.normal(2.5, 0.5))\n",
        "        production.append(np.random.lognormal(mean=6, sigma=1.0))\n",
        "    elif etype == 'Experimental':\n",
        "        if np.random.rand() < 0.5:\n",
        "            capacity.append(np.random.uniform(500, 3000))\n",
        "            efficiency.append(np.random.uniform(0.7, 0.9))\n",
        "            carbon_intensity.append(np.random.normal(0.2, 0.05))\n",
        "            emissions.append(np.random.normal(1.0, 0.3))\n",
        "            production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "        else:\n",
        "            capacity.append(np.random.uniform(100, 1000))\n",
        "            efficiency.append(np.random.uniform(0.2, 0.5))\n",
        "            carbon_intensity.append(np.random.normal(1.0, 0.2))\n",
        "            emissions.append(np.random.normal(3.0, 0.7))\n",
        "            production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "    elif etype == 'Geothermal':\n",
        "        capacity.append(np.random.uniform(300, 2000))\n",
        "        efficiency.append(np.random.uniform(0.7, 0.85))\n",
        "        carbon_intensity.append(np.random.normal(0.1, 0.05))\n",
        "        emissions.append(np.random.normal(0.1, 0.03))\n",
        "        production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "    elif etype == 'Wind':\n",
        "        capacity.append(np.random.uniform(10, 1000))\n",
        "        efficiency.append(np.random.uniform(0.3, 0.5))\n",
        "        carbon_intensity.append(np.random.normal(0.05, 0.02))  #\n",
        "        emissions.append(np.random.normal(0.02, 0.01))\n",
        "        production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "    elif etype == 'Biomass':\n",
        "        capacity.append(np.random.uniform(100, 1500))\n",
        "        efficiency.append(np.random.uniform(0.4, 0.7))\n",
        "        carbon_intensity.append(np.random.normal(0.3, 0.1))\n",
        "        emissions.append(np.random.normal(1.5, 0.5))\n",
        "        production.append(np.random.lognormal(mean=5, sigma=0.8))\n",
        "\n",
        "power_plant = pd.DataFrame({\n",
        "    \"unique_id\": unique_id,\n",
        "    \"energy_type\": energy_type,\n",
        "    \"capacity\": capacity,\n",
        "    \"efficiency\": efficiency,\n",
        "    \"production\": production,\n",
        "    \"carbon_intensity\": carbon_intensity,\n",
        "    \"emissions\": emissions\n",
        "})\n",
        "\n",
        "print(power_plant.head())\n",
        "print(power_plant[\"energy_type\"].value_counts())\n",
        "print(power_plant.shape)\n",
        "\n",
        "\n",
        "authority_names = [\n",
        "    \"Department of Health\", \"Department of Energy\", \"Department of Education\",\n",
        "    \"Department of Transport\", \"Department of Urban Planning\", \"Department of Environment\",\n",
        "    \"Department of Agriculture\", \"Department of Housing\", \"Public Safety Department\",\n",
        "    \"Department of Finance\"\n",
        "]\n",
        "\n",
        "sectors = [\n",
        "    \"Health\", \"Energy\", \"Education\", \"Transport\", \"Urban Planning\", \"Environment\",\n",
        "    \"Agriculture\", \"Housing\", \"Public Safety\", \"Finance\"\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Parameter generator\n",
        "# -----------------------------\n",
        "def generate_sector_based_params(sector):\n",
        "    if sector == \"Health\":\n",
        "        budget = np.random.uniform(1e6, 10e6)\n",
        "        economic_growth_priority = np.random.uniform(0.5, 1.0)\n",
        "        emissions_priority = np.random.uniform(0, 0.2)\n",
        "        energy_priority = np.random.uniform(0, 0.1)\n",
        "        social_welfare_priority = np.random.uniform(0.7, 1.0)\n",
        "        sustainability_priority = np.random.uniform(0.4, 0.7)\n",
        "    elif sector == \"Energy\":\n",
        "        budget = np.random.uniform(10e6, 50e6)\n",
        "        economic_growth_priority = np.random.uniform(0.3, 0.7)\n",
        "        emissions_priority = np.random.uniform(0.5, 1.0)\n",
        "        energy_priority = np.random.uniform(0.7, 1.0)\n",
        "        social_welfare_priority = np.random.uniform(0.2, 0.5)\n",
        "        sustainability_priority = np.random.uniform(0.3, 0.6)\n",
        "    elif sector == \"Education\":\n",
        "        budget = np.random.uniform(5e6, 20e6)\n",
        "        economic_growth_priority = np.random.uniform(0.6, 1.0)\n",
        "        emissions_priority = np.random.uniform(0, 0.1)\n",
        "        energy_priority = np.random.uniform(0, 0.1)\n",
        "        social_welfare_priority = np.random.uniform(0.6, 1.0)\n",
        "        sustainability_priority = np.random.uniform(0.5, 0.8)\n",
        "    elif sector == \"Environment\":\n",
        "        budget = np.random.uniform(5e6, 25e6)\n",
        "        economic_growth_priority = np.random.uniform(0.3, 0.7)\n",
        "        emissions_priority = np.random.uniform(0.7, 1.0)\n",
        "        energy_priority = np.random.uniform(0.2, 0.5)\n",
        "        social_welfare_priority = np.random.uniform(0.5, 0.8)\n",
        "        sustainability_priority = np.random.uniform(0.7, 1.0)\n",
        "    else:\n",
        "        budget = np.random.uniform(1e6, 50e6)\n",
        "        economic_growth_priority = np.random.uniform(0.3, 0.9)\n",
        "        emissions_priority = np.random.uniform(0, 1)\n",
        "        energy_priority = np.random.uniform(0, 1)\n",
        "        social_welfare_priority = np.random.uniform(0, 1)\n",
        "        sustainability_priority = np.random.uniform(0, 1)\n",
        "\n",
        "    return {\n",
        "        \"budget\": budget,\n",
        "        \"tax_capacity\": np.random.uniform(0.05, 0.25),\n",
        "        \"subsidy_capacity\": np.random.uniform(1e5, 1e7),\n",
        "        \"investment_ceiling\": np.random.uniform(1e6, 1e7),\n",
        "        \"investment_floor\": np.random.uniform(1e5, 5e6),\n",
        "        \"economic_growth_priority\": economic_growth_priority,\n",
        "        \"emissions_priority\": emissions_priority,\n",
        "        \"sustainability_priority\": sustainability_priority,\n",
        "        \"energy_priority\": energy_priority,\n",
        "        \"social_welfare_priority\": social_welfare_priority,\n",
        "        \"regulation_strictness\": np.random.uniform(0, 1),\n",
        "        \"penalty_severity\": np.random.uniform(0, 1),\n",
        "        \"incentive_intensity\": np.random.uniform(0, 1)\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Build dataframe safely\n",
        "# -----------------------------\n",
        "rows = []\n",
        "for uid, (name, sector) in enumerate(zip(authority_names, sectors)):\n",
        "    params = generate_sector_based_params(sector)\n",
        "    rows.append({\n",
        "        \"unique_id\": uid,\n",
        "        \"authority\": name,\n",
        "        \"sector\": sector,\n",
        "        **params\n",
        "    })\n",
        "\n",
        "authority_data = pd.DataFrame(rows)\n",
        "\n",
        "print(authority_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZkFgc2TOrlu",
        "outputId": "712012f8-d036-4540-bc19-86825d19d132"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   unique_id business_name             business_type       revenue  \\\n",
            "0          0     Company_1        Telecommunications  4.258325e+07   \n",
            "1          1     Company_2        Energy & Utilities  8.425303e+09   \n",
            "2          2     Company_3        Telecommunications  6.026394e+07   \n",
            "3          3     Company_4  Industrial Manufacturing  1.152258e+08   \n",
            "4          4     Company_5           Food & Beverage  8.868283e+08   \n",
            "\n",
            "      valuation  growth_rate  sustainability_index  energy_consumption  \n",
            "0  4.504545e+08           16                    34              711523  \n",
            "1  6.653113e+10            5                    39              762894  \n",
            "2  2.233446e+08           23                    57              727769  \n",
            "3  2.863636e+08            6                    35              208308  \n",
            "4  8.905081e+09            5                    40              277247  \n",
            "business_type\n",
            "Telecommunications                25\n",
            "Energy & Utilities                25\n",
            "Industrial Manufacturing          25\n",
            "Food & Beverage                   25\n",
            "Aerospace                         25\n",
            "Mining & Natural Resources        25\n",
            "Finance & Banking                 25\n",
            "Chemicals & Advanced Materials    25\n",
            "Construction                      25\n",
            "Real Estate & Infrastructure      25\n",
            "Transportation & Logistics        25\n",
            "Technology                        25\n",
            "Agriculture & Agri-business       25\n",
            "Healthcare & Life Sciences        25\n",
            "Defense & Security                25\n",
            "Automotive & Mobility             25\n",
            "Name: count, dtype: int64\n",
            "(400, 8)\n",
            "   unique_id   energy_type     capacity  efficiency  production  \\\n",
            "0          0  Fossil Fuels  1820.699704    0.439763  166.297717   \n",
            "1          1       Nuclear  2380.892571    0.900741  856.883885   \n",
            "2          2  Fossil Fuels  3660.164447    0.448923  101.727658   \n",
            "3          3         Solar    46.991897    0.240761  505.252849   \n",
            "4          4          Wind   316.278991    0.464758   84.214678   \n",
            "\n",
            "   carbon_intensity  emissions  \n",
            "0          0.822085   2.402834  \n",
            "1          0.113433   0.049629  \n",
            "2          0.860089   2.173552  \n",
            "3          0.012840   0.013860  \n",
            "4          0.052897   0.013852  \n",
            "energy_type\n",
            "Fossil Fuels    50\n",
            "Nuclear         50\n",
            "Solar           50\n",
            "Wind            50\n",
            "Biomass         50\n",
            "Hydro           50\n",
            "Experimental    50\n",
            "Geothermal      50\n",
            "Name: count, dtype: int64\n",
            "(400, 7)\n",
            "   unique_id                     authority          sector        budget  \\\n",
            "0          0          Department of Health          Health  8.641882e+06   \n",
            "1          1          Department of Energy          Energy  1.421606e+07   \n",
            "2          2       Department of Education       Education  1.447064e+07   \n",
            "3          3       Department of Transport       Transport  1.786049e+07   \n",
            "4          4  Department of Urban Planning  Urban Planning  2.168742e+07   \n",
            "5          5     Department of Environment     Environment  1.370525e+07   \n",
            "6          6     Department of Agriculture     Agriculture  4.312633e+06   \n",
            "7          7         Department of Housing         Housing  1.240369e+07   \n",
            "8          8      Public Safety Department   Public Safety  3.912427e+07   \n",
            "9          9         Department of Finance         Finance  1.342131e+06   \n",
            "\n",
            "   tax_capacity  subsidy_capacity  investment_ceiling  investment_floor  \\\n",
            "0      0.194077      5.160025e+06        6.438817e+06      2.358402e+06   \n",
            "1      0.195074      1.214646e+06        4.670002e+06      4.443264e+06   \n",
            "2      0.128624      1.478615e+06        1.096438e+06      2.766861e+06   \n",
            "3      0.077674      9.829121e+06        3.249756e+06      4.594839e+05   \n",
            "4      0.141606      2.640807e+06        7.769029e+06      2.613086e+06   \n",
            "5      0.093927      1.022477e+05        9.235014e+06      4.235009e+06   \n",
            "6      0.119766      3.810767e+06        6.591976e+06      3.621719e+06   \n",
            "7      0.118249      4.497416e+06        7.080281e+06      2.593581e+06   \n",
            "8      0.099721      5.430110e+06        9.929221e+06      4.301900e+06   \n",
            "9      0.050120      3.147410e+06        3.170487e+06      1.244480e+06   \n",
            "\n",
            "   economic_growth_priority  emissions_priority  sustainability_priority  \\\n",
            "0                  0.691699            0.033656                 0.456759   \n",
            "1                  0.635542            0.835506                 0.563784   \n",
            "2                  0.708224            0.024879                 0.732652   \n",
            "3                  0.612326            0.440665                 0.507758   \n",
            "4                  0.771038            0.195713                 0.418970   \n",
            "5                  0.668277            0.803505                 0.825609   \n",
            "6                  0.734418            0.928808                 0.512393   \n",
            "7                  0.332298            0.189875                 0.802402   \n",
            "8                  0.576440            0.988841                 0.754569   \n",
            "9                  0.542089            0.234754                 0.827524   \n",
            "\n",
            "   energy_priority  social_welfare_priority  regulation_strictness  \\\n",
            "0         0.083644                 0.864673               0.828709   \n",
            "1         0.909107                 0.308895               0.047197   \n",
            "2         0.094206                 0.719309               0.975779   \n",
            "3         0.879915                 0.505521               0.663146   \n",
            "4         0.820253                 0.648747               0.320855   \n",
            "5         0.216420                 0.561273               0.742292   \n",
            "6         0.430536                 0.119141               0.110604   \n",
            "7         0.038958                 0.391811               0.861446   \n",
            "8         0.483232                 0.986896               0.097367   \n",
            "9         0.818869                 0.337331               0.068187   \n",
            "\n",
            "   penalty_severity  incentive_intensity  \n",
            "0          0.829563             0.709389  \n",
            "1          0.961417             0.679163  \n",
            "2          0.369136             0.357192  \n",
            "3          0.595366             0.415146  \n",
            "4          0.596050             0.995734  \n",
            "5          0.167815             0.120364  \n",
            "6          0.035590             0.994330  \n",
            "7          0.866036             0.009911  \n",
            "8          0.706372             0.345580  \n",
            "9          0.187124             0.555550  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "business_type = np.tile(sectors, N // len(sectors))\n",
        "np.random.shuffle(business_type)\n",
        "\n",
        "production_level = []\n",
        "net_emissions = []\n",
        "net_carbon_intensity = []\n",
        "energy_consumption = []\n",
        "water_consumption = []\n",
        "resources_consumption = []\n",
        "waste_creation = []\n",
        "waste_recycling = []\n",
        "\n",
        "for sector in business_type:\n",
        "    if sector == \"Energy & Utilities\":\n",
        "        production_level.append(np.random.uniform(500, 5000))\n",
        "        net_emissions.append(np.random.normal(1000, 100))\n",
        "        energy_consumption.append(np.random.normal(1000000, 200000))\n",
        "        water_consumption.append(np.random.normal(3000, 500))\n",
        "        resources_consumption.append(np.random.normal(8000, 1000))\n",
        "        waste_creation.append(np.random.normal(4000, 500))\n",
        "        waste_recycling.append(np.random.normal(3000, 400))\n",
        "    elif sector == \"Mining & Natural Resources\":\n",
        "        production_level.append(np.random.uniform(1000, 5000))\n",
        "        net_emissions.append(np.random.normal(1200, 150))\n",
        "        energy_consumption.append(np.random.normal(1200000, 250000))\n",
        "        water_consumption.append(np.random.normal(5000, 800))\n",
        "        resources_consumption.append(np.random.normal(10000, 1500))\n",
        "        waste_creation.append(np.random.normal(5000, 800))\n",
        "        waste_recycling.append(np.random.normal(2000, 300))\n",
        "    elif sector == \"Industrial Manufacturing\":\n",
        "        production_level.append(np.random.uniform(200, 3000))\n",
        "        net_emissions.append(np.random.normal(800, 100))\n",
        "        energy_consumption.append(np.random.normal(700000, 150000))\n",
        "        water_consumption.append(np.random.normal(2000, 300))\n",
        "        resources_consumption.append(np.random.normal(6000, 1000))\n",
        "        waste_creation.append(np.random.normal(3000, 400))\n",
        "        waste_recycling.append(np.random.normal(2000, 200))\n",
        "    elif sector == \"Automotive & Mobility\":\n",
        "        production_level.append(np.random.uniform(500, 3000))\n",
        "        net_emissions.append(np.random.normal(900, 120))\n",
        "        energy_consumption.append(np.random.normal(800000, 200000))\n",
        "        water_consumption.append(np.random.normal(2000, 500))\n",
        "        resources_consumption.append(np.random.normal(7000, 1200))\n",
        "        waste_creation.append(np.random.normal(3500, 600))\n",
        "        waste_recycling.append(np.random.normal(2500, 300))\n",
        "    elif sector == \"Aerospace\":\n",
        "        production_level.append(np.random.uniform(200, 1500))\n",
        "        net_emissions.append(np.random.normal(700, 80))\n",
        "        energy_consumption.append(np.random.normal(600000, 100000))\n",
        "        water_consumption.append(np.random.normal(1500, 400))\n",
        "        resources_consumption.append(np.random.normal(5000, 800))\n",
        "        waste_creation.append(np.random.normal(2500, 300))\n",
        "        waste_recycling.append(np.random.normal(2000, 250))\n",
        "    elif sector == \"Defense & Security\":\n",
        "        production_level.append(np.random.uniform(100, 2000))\n",
        "        net_emissions.append(np.random.normal(1000, 200))\n",
        "        energy_consumption.append(np.random.normal(1000000, 250000))\n",
        "        water_consumption.append(np.random.normal(3000, 500))\n",
        "        resources_consumption.append(np.random.normal(9000, 1200))\n",
        "        waste_creation.append(np.random.normal(4000, 700))\n",
        "        waste_recycling.append(np.random.normal(3000, 400))\n",
        "    elif sector == \"Construction\":\n",
        "        production_level.append(np.random.uniform(100, 2000))\n",
        "        net_emissions.append(np.random.normal(400, 80))\n",
        "        energy_consumption.append(np.random.normal(500000, 100000))\n",
        "        water_consumption.append(np.random.normal(3000, 600))\n",
        "        resources_consumption.append(np.random.normal(7000, 1000))\n",
        "        waste_creation.append(np.random.normal(4000, 500))\n",
        "        waste_recycling.append(np.random.normal(2000, 300))\n",
        "    elif sector == \"Healthcare & Life Sciences\":\n",
        "        production_level.append(np.random.uniform(50, 1000))\n",
        "        net_emissions.append(np.random.normal(200, 50))\n",
        "        energy_consumption.append(np.random.normal(200000, 50000))\n",
        "        water_consumption.append(np.random.normal(1000, 300))\n",
        "        resources_consumption.append(np.random.normal(3000, 500))\n",
        "        waste_creation.append(np.random.normal(1000, 200))\n",
        "        waste_recycling.append(np.random.normal(900, 150))\n",
        "    elif sector == \"Food & Beverage\":\n",
        "        production_level.append(np.random.uniform(200, 3000))\n",
        "        net_emissions.append(np.random.normal(500, 70))\n",
        "        energy_consumption.append(np.random.normal(600000, 100000))\n",
        "        water_consumption.append(np.random.normal(3000, 600))\n",
        "        resources_consumption.append(np.random.normal(5000, 800))\n",
        "        waste_creation.append(np.random.normal(2500, 300))\n",
        "        waste_recycling.append(np.random.normal(2000, 250))\n",
        "    elif sector == \"Agriculture & Agri-business\":\n",
        "        production_level.append(np.random.uniform(1000, 5000))\n",
        "        net_emissions.append(np.random.normal(1200, 300))\n",
        "        energy_consumption.append(np.random.normal(900000, 150000))\n",
        "        water_consumption.append(np.random.normal(6000, 1000))\n",
        "        resources_consumption.append(np.random.normal(12000, 2000))\n",
        "        waste_creation.append(np.random.normal(7000, 1000))\n",
        "        waste_recycling.append(np.random.normal(3000, 400))\n",
        "    else:\n",
        "        production_level.append(np.random.uniform(100, 2000))\n",
        "        net_emissions.append(np.random.normal(500, 60))\n",
        "        energy_consumption.append(np.random.normal(400000, 80000))\n",
        "        water_consumption.append(np.random.normal(1000, 300))\n",
        "        resources_consumption.append(np.random.normal(4000, 700))\n",
        "        waste_creation.append(np.random.normal(1500, 300))\n",
        "        waste_recycling.append(np.random.normal(1200, 200))\n",
        "\n",
        "    net_carbon_intensity.append(np.round(net_emissions[-1] / (production_level[-1] if production_level[-1] > 0 else 1), 2))\n",
        "\n",
        "sustainability_data = pd.DataFrame({\n",
        "    \"unique_id\": unique_id,\n",
        "    \"sector\": business_type,\n",
        "    \"production_level\": np.round(production_level, 2),\n",
        "    \"net_emissions\": np.round(net_emissions, 2),\n",
        "    \"net_carbon_intensity\": net_carbon_intensity,\n",
        "    \"energy_consumption\": np.round(energy_consumption, 2),\n",
        "    \"water_consumption\": np.round(water_consumption, 2),\n",
        "    \"resources_consumption\": np.round(resources_consumption, 2),\n",
        "    \"waste_creation\": np.round(waste_creation, 2),\n",
        "    \"waste_recycling\": np.round(waste_recycling, 2),\n",
        "})\n",
        "\n",
        "print(sustainability_data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "phufkS9XB8bI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f383432-542d-4a21-ff7e-84e99b50db23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   unique_id     sector  production_level  net_emissions  \\\n",
            "0          0  Transport           1629.83         615.41   \n",
            "1          1    Housing           1872.45         500.28   \n",
            "2          2    Housing            739.01         598.60   \n",
            "3          3  Transport           1266.95         494.00   \n",
            "4          4     Energy           1059.33         440.90   \n",
            "\n",
            "   net_carbon_intensity  energy_consumption  water_consumption  \\\n",
            "0                  0.38           338030.80             493.25   \n",
            "1                  0.27           465591.05            1354.97   \n",
            "2                  0.81           428851.83             740.95   \n",
            "3                  0.39           272349.06             771.87   \n",
            "4                  0.42           415894.40             976.28   \n",
            "\n",
            "   resources_consumption  waste_creation  waste_recycling  \n",
            "0                3670.12          907.35          1350.22  \n",
            "1                3398.83         1491.08          1090.78  \n",
            "2                3978.16         1505.41          1294.53  \n",
            "3                3911.28         1285.48          1078.02  \n",
            "4                3594.61         1355.75          1339.23  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "N = 400\n",
        "\n",
        "consumer_classes = [\n",
        "    \"Lower Class\",\n",
        "    \"Lower Middle Class\",\n",
        "    \"Middle Class\",\n",
        "    \"Upper Middle Class\",\n",
        "    \"Upper Class\",\n",
        "    \"Elite\"\n",
        "]\n",
        "\n",
        "class_proportions = {\n",
        "    \"Lower Class\": 0.35,\n",
        "    \"Lower Middle Class\": 0.25,\n",
        "    \"Middle Class\": 0.2,\n",
        "    \"Upper Middle Class\": 0.15,\n",
        "    \"Upper Class\": 0.04,\n",
        "    \"Elite\": 0.01\n",
        "}\n",
        "\n",
        "class_population = {cls: int(N * proportion) for cls, proportion in class_proportions.items()}\n",
        "\n",
        "class_wealth = []\n",
        "\n",
        "for consumer_class in consumer_classes:\n",
        "    if consumer_class == \"Lower Class\":\n",
        "        wealth_range = (1000, 5000)\n",
        "    elif consumer_class == \"Lower Middle Class\":\n",
        "        wealth_range = (5000, 15000)\n",
        "    elif consumer_class == \"Middle Class\":\n",
        "        wealth_range = (15000, 35000)\n",
        "    elif consumer_class == \"Upper Middle Class\":\n",
        "        wealth_range = (35000, 70000)\n",
        "    elif consumer_class == \"Upper Class\":\n",
        "        wealth_range = (70000, 150000)\n",
        "    elif consumer_class == \"Elite\":\n",
        "        wealth_range = (150000, 1000000)\n",
        "\n",
        "    for _ in range(class_population[consumer_class]):\n",
        "        class_wealth.append(np.random.randint(wealth_range[0], wealth_range[1]))\n",
        "\n",
        "consumer_data = pd.DataFrame({\n",
        "    \"unique_id\": range(6),\n",
        "    \"consumer_class\": list(class_population.keys()),\n",
        "    \"class_population\": list(class_population.values()),\n",
        "    \"class_wealth\": [\n",
        "        np.mean(class_wealth[i:i + class_population[class_name]]) for i, class_name in enumerate(class_population.keys())\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(consumer_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJZaQxkt0NHf",
        "outputId": "9062773b-b4b0-41c9-ec84-d3f4feb428aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   unique_id      consumer_class  class_population  class_wealth\n",
            "0          0         Lower Class               140   3111.728571\n",
            "1          1  Lower Middle Class               100   3125.070000\n",
            "2          2        Middle Class                80   3070.050000\n",
            "3          3  Upper Middle Class                60   3065.566667\n",
            "4          4         Upper Class                16   2996.625000\n",
            "5          5               Elite                 4   3399.250000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset={\n",
        "    \"business\": business,\n",
        "    \"energy\": power_plant,\n",
        "    \"decision\": authority_data,\n",
        "    \"sustainability\": sustainability_data,\n",
        "    \"consumer\": consumer_data\n",
        "}"
      ],
      "metadata": {
        "id": "oivRraHm0zQD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Linear Machine Learning**"
      ],
      "metadata": {
        "id": "lz8RkoQe1mnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import mesa\n",
        "from mesa import Agent\n",
        "\n",
        "\n",
        "class BusinessAgents(Agent):\n",
        "    def __init__(self, model, unique_id):\n",
        "        super().__init__(model)\n",
        "        self.model = model\n",
        "        self.unique_id = unique_id\n",
        "        self.business_name = \"\"\n",
        "        self.business_type = \"\"\n",
        "        self.valuation = 0.0\n",
        "        self.revenue = 0.0\n",
        "        self.growth_rate = 0.0\n",
        "        self.sustainability_index = 0.0\n",
        "        self.energy_consumption = 0.0\n",
        "\n",
        "        dummy_state = self.get_state()\n",
        "        self.state_dim = len(dummy_state)\n",
        "\n",
        "        self.policy_weights = {}\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim, space.n))\n",
        "            else:\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim,))\n",
        "\n",
        "        self.epsilon = 0.3\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.trajectory = []\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x_clean = np.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        e_x = np.exp(x_clean - np.max(x_clean))\n",
        "        return e_x / (e_x.sum(axis=-1) + 1e-8)\n",
        "\n",
        "    def get_action_probs(self, state_dict):\n",
        "        state_values = []\n",
        "        for v in state_dict.values():\n",
        "            try:\n",
        "                val = float(np.nan_to_num(v, nan=0.0))\n",
        "                state_values.append(val)\n",
        "            except:\n",
        "                state_values.append(0.0)\n",
        "\n",
        "        if len(state_values) > self.state_dim:\n",
        "            state_values = state_values[:self.state_dim]\n",
        "        elif len(state_values) < self.state_dim:\n",
        "            state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "        state_vector = np.array(state_values, dtype=np.float64)\n",
        "        state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "        state_vector = np.nan_to_num(state_vector)\n",
        "\n",
        "        std = np.std(state_vector)\n",
        "        if std > 1e-6:\n",
        "            state_vector = (state_vector - np.mean(state_vector)) / (std + 1e-8)\n",
        "        else:\n",
        "            state_vector = state_vector - np.mean(state_vector)\n",
        "\n",
        "        probs = {}\n",
        "        for action_name, action_space in self.model.env.action_space.spaces.items():\n",
        "            weights = self.policy_weights.get(action_name)\n",
        "            if isinstance(action_space, gym.spaces.Discrete):\n",
        "                logits = np.dot(state_vector, weights)\n",
        "                probs[action_name] = self.softmax(logits)\n",
        "            else:\n",
        "                dot_val = np.dot(state_vector, weights)\n",
        "                dot_val = np.clip(np.nan_to_num(dot_val), -15, 15)\n",
        "                probs[action_name] = 1.0 / (1.0 + np.exp(-dot_val))\n",
        "        return probs\n",
        "\n",
        "    def business_definition(self, row):\n",
        "        self.business_name = row[\"business_name\"]\n",
        "        self.business_type = row[\"business_type\"]\n",
        "        self.valuation = float(row[\"valuation\"])\n",
        "        self.revenue = float(row[\"revenue\"])\n",
        "        self.growth_rate = float(row[\"growth_rate\"])\n",
        "        self.sustainability_index = float(row[\"sustainability_index\"])\n",
        "        self.energy_consumption = float(row[\"energy_consumption\"])\n",
        "\n",
        "    def business_state(self):\n",
        "        self.growth_rate *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.sustainability_index *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.valuation *= (1 + self.growth_rate)\n",
        "        self.revenue *= (1 + self.growth_rate)\n",
        "\n",
        "    def get_state(self):\n",
        "        agent_state = {\n",
        "            \"valuation\": np.sign(self.valuation) * np.log1p(abs(self.valuation)),\n",
        "            \"revenue\": np.sign(self.revenue) * np.log1p(abs(self.revenue)),\n",
        "            \"growth_rate\": self.growth_rate,\n",
        "            \"sustainability_index\": self.sustainability_index,\n",
        "            \"energy_consumption\": np.log1p(abs(self.energy_consumption)),\n",
        "        }\n",
        "        scaled_env_state = {k: (np.sign(v) * np.log1p(abs(v)) if isinstance(v, (int, float)) else v)\n",
        "                           for k, v in self.model.env.state.items()}\n",
        "        return {**agent_state, **scaled_env_state}\n",
        "\n",
        "    def choose_action(self):\n",
        "        state_dict = self.get_state()\n",
        "        action = {}\n",
        "        all_probs = self.get_action_probs(state_dict)\n",
        "        for action_name, action_type in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(action_type, gym.spaces.Discrete):\n",
        "                action_probs = all_probs[action_name]\n",
        "                action_probs = np.nan_to_num(action_probs, nan=1.0/len(action_probs))\n",
        "                action_probs /= (action_probs.sum() + 1e-8)\n",
        "                action[action_name] = np.random.choice(len(action_probs), p=action_probs)\n",
        "            else:\n",
        "                p_val = all_probs[action_name]\n",
        "                low, high = action_type.low[0], action_type.high[0]\n",
        "                val = low + (high - low) * p_val\n",
        "                action[action_name] = np.array([val], dtype=np.float32)\n",
        "        return action\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gradient_pairs = []\n",
        "        total_return = 0\n",
        "        norm_reward = 1e-6\n",
        "        for i in reversed(range(len(self.trajectory))):\n",
        "            state, action_dict, reward = self.trajectory[i]\n",
        "            reward_scaled = np.nan_to_num(reward) * norm_reward\n",
        "            state_values = [float(v) for v in state.values()]\n",
        "\n",
        "            if len(state_values) > self.state_dim:\n",
        "                state_values = state_values[:self.state_dim]\n",
        "            elif len(state_values) < self.state_dim:\n",
        "                state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "            state_vector = np.array(state_values, dtype=np.float64)\n",
        "            state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "            state_vector = np.nan_to_num(state_vector)\n",
        "\n",
        "            total_return = reward_scaled + self.gamma * total_return\n",
        "            all_probs = self.get_action_probs(state)\n",
        "            for name, val in action_dict.items():\n",
        "                if name in self.policy_weights:\n",
        "                    if isinstance(self.model.env.action_space[name], gym.spaces.Discrete):\n",
        "                        probs = all_probs[name]\n",
        "                        grad_logits = -probs.copy()\n",
        "                        grad_logits[int(val)] += 1.0\n",
        "                        full_grad = np.outer(state_vector, grad_logits) * total_return\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad), -1, 1)))\n",
        "                    else:\n",
        "                        full_grad = np.outer(state_vector, np.atleast_1d(total_return))\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad.flatten()), -1, 1)))\n",
        "        return gradient_pairs\n",
        "\n",
        "    def update(self):\n",
        "        grads = self.compute_gradients()\n",
        "        if not grads: return\n",
        "        for action_name, g in grads:\n",
        "            if g.shape == self.policy_weights[action_name].shape:\n",
        "                self.policy_weights[action_name] += self.learning_rate * g\n",
        "        self.trajectory = []\n",
        "\n",
        "    def step(self):\n",
        "        self.business_state()\n",
        "\n",
        "class EnergyAgents(Agent):\n",
        "    def __init__(self, model, unique_id):\n",
        "        super().__init__(model)\n",
        "        self.model = model\n",
        "        self.unique_id = unique_id\n",
        "        self.energy_type = \"\"\n",
        "        self.capacity = 0.0\n",
        "        self.efficiency = 0.0\n",
        "        self.production = 0.0\n",
        "        self.carbon_intensity = 0.0\n",
        "        self.emissions = 0.0\n",
        "\n",
        "        dummy_state = self.get_state()\n",
        "        self.state_dim = len(dummy_state)\n",
        "\n",
        "        self.policy_weights = {}\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim, space.n))\n",
        "            else:\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim,))\n",
        "\n",
        "        self.epsilon = 0.3\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.trajectory = []\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x_clean = np.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        e_x = np.exp(x_clean - np.max(x_clean))\n",
        "        return e_x / (e_x.sum(axis=-1) + 1e-8)\n",
        "\n",
        "    def get_action_probs(self, state_dict):\n",
        "        state_values = []\n",
        "        for v in state_dict.values():\n",
        "            try:\n",
        "                val = float(np.nan_to_num(v, nan=0.0))\n",
        "                state_values.append(val)\n",
        "            except:\n",
        "                state_values.append(0.0)\n",
        "\n",
        "        if len(state_values) > self.state_dim:\n",
        "            state_values = state_values[:self.state_dim]\n",
        "        elif len(state_values) < self.state_dim:\n",
        "            state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "        state_vector = np.array(state_values, dtype=np.float64)\n",
        "        state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "        state_vector = np.nan_to_num(state_vector)\n",
        "        std = np.std(state_vector)\n",
        "        if std > 1e-6:\n",
        "            state_vector = (state_vector - np.mean(state_vector)) / (std + 1e-8)\n",
        "        else:\n",
        "            state_vector = state_vector - np.mean(state_vector)\n",
        "        probs = {}\n",
        "        for action_name, action_space in self.model.env.action_space.spaces.items():\n",
        "            weights = self.policy_weights.get(action_name)\n",
        "            if isinstance(action_space, gym.spaces.Discrete):\n",
        "                logits = np.dot(state_vector, weights)\n",
        "                probs[action_name] = self.softmax(logits)\n",
        "            else:\n",
        "                dot_val = np.dot(state_vector, weights)\n",
        "                dot_val = np.clip(np.nan_to_num(dot_val), -15, 15)\n",
        "                probs[action_name] = 1.0 / (1.0 + np.exp(-dot_val))\n",
        "        return probs\n",
        "\n",
        "    def choose_action(self):\n",
        "        state_dict = self.get_state()\n",
        "        action = {}\n",
        "        all_probs = self.get_action_probs(state_dict)\n",
        "        for action_name, action_type in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(action_type, gym.spaces.Discrete):\n",
        "                action_probs = all_probs[action_name]\n",
        "                action_probs = np.nan_to_num(action_probs, nan=1.0/len(action_probs))\n",
        "                action_probs /= (action_probs.sum() + 1e-8)\n",
        "                action[action_name] = np.random.choice(len(action_probs), p=action_probs)\n",
        "            else:\n",
        "                p_val = all_probs[action_name]\n",
        "                low, high = action_type.low[0], action_type.high[0]\n",
        "                val = low + (high - low) * p_val\n",
        "                action[action_name] = np.array([val], dtype=np.float32)\n",
        "        return action\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gradient_pairs = []\n",
        "        total_return = 0\n",
        "        norm_reward = 1e-6\n",
        "        for i in reversed(range(len(self.trajectory))):\n",
        "            state, action_dict, reward = self.trajectory[i]\n",
        "            reward_scaled = np.nan_to_num(reward) * norm_reward\n",
        "            state_values = [float(v) for v in state.values()]\n",
        "\n",
        "            if len(state_values) > self.state_dim:\n",
        "                state_values = state_values[:self.state_dim]\n",
        "            elif len(state_values) < self.state_dim:\n",
        "                state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "            state_vector = np.array(state_values, dtype=np.float64)\n",
        "            state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "            state_vector = np.nan_to_num(state_vector)\n",
        "            total_return = reward_scaled + self.gamma * total_return\n",
        "            all_probs = self.get_action_probs(state)\n",
        "            for name, val in action_dict.items():\n",
        "                if name in self.policy_weights:\n",
        "                    if isinstance(self.model.env.action_space[name], gym.spaces.Discrete):\n",
        "                        probs = all_probs[name]\n",
        "                        grad_logits = -probs.copy()\n",
        "                        grad_logits[int(val)] += 1.0\n",
        "                        full_grad = np.outer(state_vector, grad_logits) * total_return\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad), -1, 1)))\n",
        "                    else:\n",
        "                        full_grad = np.outer(state_vector, np.atleast_1d(total_return))\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad.flatten()), -1, 1)))\n",
        "        return gradient_pairs\n",
        "\n",
        "    def update(self):\n",
        "        grads = self.compute_gradients()\n",
        "        if not grads: return\n",
        "        for action_name, g in grads:\n",
        "            if g.shape == self.policy_weights[action_name].shape:\n",
        "                self.policy_weights[action_name] += self.learning_rate * g\n",
        "        self.trajectory = []\n",
        "\n",
        "    def power_plant_definition(self, row):\n",
        "        self.energy_type = row[\"energy_type\"]\n",
        "        self.capacity = float(row[\"capacity\"])\n",
        "        self.efficiency = float(row[\"efficiency\"])\n",
        "        self.production = float(row[\"production\"])\n",
        "        self.carbon_intensity = float(row[\"carbon_intensity\"])\n",
        "        self.emissions = float(row[\"emissions\"])\n",
        "\n",
        "    def power_plant_state(self):\n",
        "        self.production *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.carbon_intensity *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.emissions *= (1 + random.uniform(-0.2, 0.5))\n",
        "\n",
        "    def get_state(self):\n",
        "        agent_state = {\n",
        "            \"capacity\": np.log1p(abs(self.capacity)),\n",
        "            \"production\": np.log1p(abs(self.production)),\n",
        "            \"efficiency\": self.efficiency,\n",
        "            \"carbon_intensity\": self.carbon_intensity,\n",
        "            \"emissions\": np.log1p(abs(self.emissions)),\n",
        "        }\n",
        "        scaled_env_state = {k: (np.sign(v) * np.log1p(abs(v)) if isinstance(v, (int, float)) else v)\n",
        "                           for k, v in self.model.env.state.items()}\n",
        "        return {**agent_state, **scaled_env_state}\n",
        "\n",
        "    def step(self):\n",
        "        self.power_plant_state()\n",
        "\n",
        "class DecisionAgents(Agent):\n",
        "    def __init__(self, model, unique_id):\n",
        "        super().__init__(model)\n",
        "        self.model = model\n",
        "        self.unique_id = unique_id\n",
        "        self.budget = 0.0\n",
        "        self.tax_capacity = 0.0\n",
        "        self.subsidy_capacity = 0.0\n",
        "        self.investment_ceiling = 0.0\n",
        "        self.investment_floor = 0.0\n",
        "        self.economic_growth_priority = 0.0\n",
        "        self.emissions_priority = 0.0\n",
        "        self.sustainability_priority = 0.0\n",
        "        self.energy_priority = 0.0\n",
        "        self.social_welfare_priority = 0.0\n",
        "        self.regulation_strictness = 0.0\n",
        "        self.penalty_severity = 0.0\n",
        "        self.incentive_intensity = 0.0\n",
        "\n",
        "        dummy_state = self.get_state()\n",
        "        self.state_dim = len(dummy_state)\n",
        "\n",
        "        self.policy_weights = {}\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim, space.n))\n",
        "            else:\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim,))\n",
        "\n",
        "        self.epsilon = 0.3\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.trajectory = []\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x_clean = np.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        e_x = np.exp(x_clean - np.max(x_clean))\n",
        "        return e_x / (e_x.sum(axis=-1) + 1e-8)\n",
        "\n",
        "    def get_action_probs(self, state_dict):\n",
        "        state_values = []\n",
        "        for v in state_dict.values():\n",
        "            try:\n",
        "                val = float(np.nan_to_num(v, nan=0.0))\n",
        "                state_values.append(val)\n",
        "            except:\n",
        "                state_values.append(0.0)\n",
        "\n",
        "        if len(state_values) > self.state_dim:\n",
        "            state_values = state_values[:self.state_dim]\n",
        "        elif len(state_values) < self.state_dim:\n",
        "            state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "        state_vector = np.array(state_values, dtype=np.float64)\n",
        "        state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "        state_vector = np.nan_to_num(state_vector)\n",
        "        std = np.std(state_vector)\n",
        "        if std > 1e-6:\n",
        "            state_vector = (state_vector - np.mean(state_vector)) / (std + 1e-8)\n",
        "        else:\n",
        "            state_vector = state_vector - np.mean(state_vector)\n",
        "        probs = {}\n",
        "        for action_name, action_space in self.model.env.action_space.spaces.items():\n",
        "            weights = self.policy_weights.get(action_name)\n",
        "            if isinstance(action_space, gym.spaces.Discrete):\n",
        "                logits = np.dot(state_vector, weights)\n",
        "                probs[action_name] = self.softmax(logits)\n",
        "            else:\n",
        "                dot_val = np.dot(state_vector, weights)\n",
        "                dot_val = np.clip(np.nan_to_num(dot_val), -15, 15)\n",
        "                probs[action_name] = 1.0 / (1.0 + np.exp(-dot_val))\n",
        "        return probs\n",
        "\n",
        "    def choose_action(self):\n",
        "        state_dict = self.get_state()\n",
        "        action = {}\n",
        "        all_probs = self.get_action_probs(state_dict)\n",
        "        for action_name, action_type in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(action_type, gym.spaces.Discrete):\n",
        "                action_probs = all_probs[action_name]\n",
        "                action_probs = np.nan_to_num(action_probs, nan=1.0/len(action_probs))\n",
        "                action_probs /= (action_probs.sum() + 1e-8)\n",
        "                action[action_name] = np.random.choice(len(action_probs), p=action_probs)\n",
        "            else:\n",
        "                p_val = all_probs[action_name]\n",
        "                low, high = action_type.low[0], action_type.high[0]\n",
        "                val = low + (high - low) * p_val\n",
        "                action[action_name] = np.array([val], dtype=np.float32)\n",
        "        return action\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def compute_gradients(self):\n",
        "        gradient_pairs = []\n",
        "        total_return = 0\n",
        "        norm_reward = 1e-6\n",
        "        for i in reversed(range(len(self.trajectory))):\n",
        "            state, action_dict, reward = self.trajectory[i]\n",
        "            reward_scaled = np.nan_to_num(reward) * norm_reward\n",
        "            state_values = [float(v) for v in state.values()]\n",
        "\n",
        "            if len(state_values) > self.state_dim:\n",
        "                state_values = state_values[:self.state_dim]\n",
        "            elif len(state_values) < self.state_dim:\n",
        "                state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "            state_vector = np.array(state_values, dtype=np.float64)\n",
        "            state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "            state_vector = np.nan_to_num(state_vector)\n",
        "            total_return = reward_scaled + self.gamma * total_return\n",
        "            all_probs = self.get_action_probs(state)\n",
        "            for name, val in action_dict.items():\n",
        "                if name in self.policy_weights:\n",
        "                    if isinstance(self.model.env.action_space[name], gym.spaces.Discrete):\n",
        "                        probs = all_probs[name]\n",
        "                        grad_logits = -probs.copy()\n",
        "                        grad_logits[int(val)] += 1.0\n",
        "                        full_grad = np.outer(state_vector, grad_logits) * total_return\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad), -1, 1)))\n",
        "                    else:\n",
        "                        full_grad = np.outer(state_vector, np.atleast_1d(total_return))\n",
        "                        gradient_pairs.append((name, np.clip(np.nan_to_num(full_grad.flatten()), -1, 1)))\n",
        "        return gradient_pairs\n",
        "\n",
        "    def update(self):\n",
        "        grads = self.compute_gradients()\n",
        "        if not grads: return\n",
        "        for action_name, g in grads:\n",
        "            if g.shape == self.policy_weights[action_name].shape:\n",
        "                self.policy_weights[action_name] += self.learning_rate * g\n",
        "        self.trajectory = []\n",
        "\n",
        "    def decision_definition(self, row):\n",
        "        self.name = row[\"authority\"]\n",
        "        self.budget = float(row[\"budget\"])\n",
        "        self.sector = row[\"sector\"]\n",
        "        self.tax_capacity = float(row[\"tax_capacity\"])\n",
        "        self.subsidy_capacity = float(row[\"subsidy_capacity\"])\n",
        "        self.investment_ceiling = float(row[\"investment_ceiling\"])\n",
        "        self.investment_floor = float(row[\"investment_floor\"])\n",
        "        self.economic_growth_priority = float(row[\"economic_growth_priority\"])\n",
        "        self.emissions_priority = float(row[\"emissions_priority\"])\n",
        "        self.sustainability_priority = float(row[\"sustainability_priority\"])\n",
        "        self.energy_priority = float(row[\"energy_priority\"])\n",
        "        self.social_welfare_priority = float(row[\"social_welfare_priority\"])\n",
        "        self.regulation_strictness = float(row[\"regulation_strictness\"])\n",
        "        self.penalty_severity = float(row[\"penalty_severity\"])\n",
        "        self.incentive_intensity = float(row[\"incentive_intensity\"])\n",
        "\n",
        "    def decision_state(self):\n",
        "        self.budget *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.economic_growth_priority *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.emissions_priority *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.sustainability_priority *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.energy_priority *= (1 + random.uniform(-0.2, 0.5))\n",
        "        self.social_welfare_priority *= (1 + random.uniform(-0.2, 0.5))\n",
        "\n",
        "    def get_state(self):\n",
        "        agent_state = {\n",
        "            \"budget\": np.sign(self.budget) * np.log1p(abs(self.budget)),\n",
        "            \"tax_capacity\": self.tax_capacity,\n",
        "            \"subsidy_capacity\": self.subsidy_capacity,\n",
        "            \"investment_ceiling\": np.log1p(abs(self.investment_ceiling)),\n",
        "            \"investment_floor\": np.log1p(abs(self.investment_floor)),\n",
        "            \"economic_growth_priority\": self.economic_growth_priority,\n",
        "            \"emissions_priority\": self.emissions_priority,\n",
        "            \"sustainability_priority\": self.sustainability_priority,\n",
        "            \"energy_priority\": self.energy_priority,\n",
        "            \"social_welfare_priority\": self.social_welfare_priority,\n",
        "            \"regulation_strictness\": self.regulation_strictness,\n",
        "            \"penalty_severity\": self.penalty_severity,\n",
        "            \"incentive_intensity\": self.incentive_intensity,\n",
        "        }\n",
        "        scaled_env_state = {k: (np.sign(v) * np.log1p(abs(v)) if isinstance(v, (int, float)) else v)\n",
        "                           for k, v in self.model.env.state.items()}\n",
        "        return {**agent_state, **scaled_env_state}\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        self.decision_state()\n",
        "\n",
        "class Sustainability(Agent):\n",
        "    def __init__(self, model, unique_id):\n",
        "        super().__init__(model)\n",
        "        self.model = model\n",
        "        self.unique_id = unique_id\n",
        "        self.production_level = 0.0\n",
        "        self.net_emissions = 0.0\n",
        "        self.net_carbon_intensity = 0.0\n",
        "        self.energy_consumption = 0.0\n",
        "        self.water_consumption = 0.0\n",
        "        self.resources_consumption = 0.0\n",
        "        self.waste_creation = 0.0\n",
        "        self.waste_recycling = 0.0\n",
        "\n",
        "        dummy_state = self.get_state()\n",
        "        self.state_dim = len(dummy_state)\n",
        "\n",
        "        self.policy_weights = {}\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim, space.n))\n",
        "            else:\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim,))\n",
        "\n",
        "        self.epsilon = 0.3\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.trajectory = []\n",
        "\n",
        "    def softmax(self, x):\n",
        "        x_clean = np.nan_to_num(x, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "        e_x = np.exp(x_clean - np.max(x_clean))\n",
        "        return e_x / (e_x.sum(axis=-1) + 1e-8)\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def get_action_probs(self, state_dict):\n",
        "        state_values = []\n",
        "        for v in state_dict.values():\n",
        "            try:\n",
        "                val = float(np.nan_to_num(v, nan=0.0))\n",
        "                state_values.append(val)\n",
        "            except:\n",
        "                state_values.append(0.0)\n",
        "\n",
        "        if len(state_values) > self.state_dim:\n",
        "            state_values = state_values[:self.state_dim]\n",
        "        elif len(state_values) < self.state_dim:\n",
        "            state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "\n",
        "        state_vector = np.array(state_values, dtype=np.float64)\n",
        "        state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "        state_vector = np.nan_to_num(state_vector)\n",
        "        std = np.std(state_vector)\n",
        "        if std > 1e-6:\n",
        "            state_vector = (state_vector - np.mean(state_vector)) / (std + 1e-8)\n",
        "        else:\n",
        "            state_vector = state_vector - np.mean(state_vector)\n",
        "        probs = {}\n",
        "        for action_name, action_space in self.model.env.action_space.spaces.items():\n",
        "            weights = self.policy_weights.get(action_name)\n",
        "            if isinstance(action_space, gym.spaces.Discrete):\n",
        "                logits = np.dot(state_vector, weights)\n",
        "                probs[action_name] = self.softmax(logits)\n",
        "            else:\n",
        "                dot_val = np.dot(state_vector, weights)\n",
        "                dot_val = np.clip(np.nan_to_num(dot_val), -15, 15)\n",
        "                probs[action_name] = 1.0 / (1.0 + np.exp(-dot_val))\n",
        "        return probs\n",
        "\n",
        "    def choose_action(self):\n",
        "        state_dict = self.get_state()\n",
        "        action = {}\n",
        "        all_probs = self.get_action_probs(state_dict)\n",
        "        for action_name, action_type in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(action_type, gym.spaces.Discrete):\n",
        "                action_probs = all_probs[action_name]\n",
        "                action_probs = np.nan_to_num(action_probs, nan=1.0/len(action_probs))\n",
        "                action_probs /= (action_probs.sum() + 1e-8)\n",
        "                action[action_name] = np.random.choice(len(action_probs), p=action_probs)\n",
        "            else:\n",
        "                p_val = all_probs[action_name]\n",
        "                low, high = action_type.low[0], action_type.high[0]\n",
        "                val = low + (high - low) * p_val\n",
        "                action[action_name] = np.array([val], dtype=np.float32)\n",
        "        return action\n",
        "\n",
        "    def sustainable_definition(self, row):\n",
        "        self.sector = row[\"sector\"]\n",
        "        self.production_level = float(row[\"production_level\"])\n",
        "        self.net_emissions = float(row[\"net_emissions\"])\n",
        "        self.net_carbon_intensity = float(row[\"net_carbon_intensity\"])\n",
        "        self.energy_consumption = float(row[\"energy_consumption\"])\n",
        "        self.water_consumption = float(row[\"water_consumption\"])\n",
        "        self.resources_consumption = float(row[\"resources_consumption\"])\n",
        "        self.waste_creation = float(row[\"waste_creation\"])\n",
        "        self.waste_recycling = float(row[\"waste_recycling\"])\n",
        "\n",
        "    def sustainable_state(self):\n",
        "        self.production_level *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.energy_consumption *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.net_emissions *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.net_carbon_intensity *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.water_consumption *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.resources_consumption *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.waste_creation *= 1 + random.uniform(-0.2, 0.5)\n",
        "        self.waste_recycling *= 1 + random.uniform(-0.2, 0.5)\n",
        "\n",
        "    def get_state(self):\n",
        "        agent_state = {\n",
        "            \"production_level\": np.log1p(abs(self.production_level)),\n",
        "            \"net_emissions\": np.log1p(abs(self.net_emissions)),\n",
        "            \"net_carbon_intensity\": self.net_carbon_intensity,\n",
        "            \"energy_consumption\": np.log1p(abs(self.energy_consumption)),\n",
        "            \"water_consumption\": np.log1p(abs(self.water_consumption)),\n",
        "            \"resources_consumption\": np.log1p(abs(self.resources_consumption)),\n",
        "            \"waste_creation\": np.log1p(abs(self.waste_creation)),\n",
        "            \"waste_recycling\": np.log1p(abs(self.waste_recycling)),\n",
        "        }\n",
        "        scaled_env_state = {k: (np.sign(v) * np.log1p(abs(v)) if isinstance(v, (int, float)) else v)\n",
        "                           for k, v in self.model.env.state.items()}\n",
        "        return {**agent_state, **scaled_env_state}\n",
        "\n",
        "    def step(self):\n",
        "        self.sustainable_state()\n",
        "        businesses = [a for a in self.model.agent_list if isinstance(a, BusinessAgents)]\n",
        "        power_plants = [a for a in self.model.agent_list if isinstance(a, EnergyAgents)]\n",
        "        if businesses:\n",
        "            self.production_level = sum(b.revenue for b in businesses)\n",
        "            self.energy_consumption = sum(b.energy_consumption for b in businesses)\n",
        "        if power_plants:\n",
        "            self.net_emissions = sum(p.emissions for p in power_plants)\n",
        "        self.net_carbon_intensity = (self.net_emissions / (self.production_level + 1e-6))\n",
        "\n",
        "    def update(self):\n",
        "        grads = []\n",
        "        total_return = 0\n",
        "        for s, a, r in reversed(self.trajectory):\n",
        "            total_return = r + self.gamma * total_return\n",
        "            all_probs = self.get_action_probs(s)\n",
        "            for name, val in a.items():\n",
        "                weights = self.policy_weights[name]\n",
        "                if isinstance(self.model.env.action_space[name], gym.spaces.Discrete):\n",
        "                    probs = all_probs[name]\n",
        "                    grad_logits = -probs.copy()\n",
        "                    grad_logits[int(val)] += 1.0\n",
        "                    grads.append((name, np.outer(list(s.values())[:self.state_dim], grad_logits) * total_return))\n",
        "                else:\n",
        "                    grads.append((name, np.outer(list(s.values())[:self.state_dim], [total_return]).flatten()))\n",
        "        # Apply gradients\n",
        "        for name, g in grads:\n",
        "            if g.shape == self.policy_weights[name].shape:\n",
        "                self.policy_weights[name] += self.learning_rate * g\n",
        "        self.trajectory = []\n",
        "\n",
        "    def step(self):\n",
        "        self.sustainable_state()  # update internal state\n",
        "\n",
        "class ConsumerAgents(mesa.Agent):\n",
        "    def __init__(self, model, unique_id):\n",
        "        super().__init__(model)\n",
        "        self.unique_id = unique_id\n",
        "        self.class_population = 0\n",
        "        self.class_wealth = 0.0\n",
        "        self.public_satisfaction = 0.0\n",
        "\n",
        "        self.policy_weights = {}\n",
        "        dummy_state = self.get_state()\n",
        "        self.state_dim = len(dummy_state)\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim, space.n))\n",
        "            else:\n",
        "                self.policy_weights[name] = np.random.uniform(-0.2, 0.5, size=(self.state_dim,))\n",
        "        self.trajectory = []\n",
        "        self.epsilon = 0.3\n",
        "        self.gamma = 0.99\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "    def consumer_definition(self, row):\n",
        "        self.consumer_class = row[\"consumer_class\"]\n",
        "        self.class_population = int(row[\"class_population\"])\n",
        "        self.class_wealth = float(row[\"class_wealth\"])\n",
        "        self.public_satisfaction = float(self.class_population * self.class_wealth)\n",
        "\n",
        "    def consumer_state(self):\n",
        "        gdp = self.model.env.state[\"GDP\"]\n",
        "        employment = self.model.env.state[\"Employment Rate\"]\n",
        "        self.class_wealth += 0.5 * gdp * random.uniform(-0.2, 0.5)\n",
        "        self.class_wealth += 0.2 * employment * random.uniform(-0.2, 0.5)\n",
        "        self.class_population += random.randint(0, 10 if gdp > 1e11 else 5)\n",
        "\n",
        "        policy_impact = {\n",
        "            \"Carbon Tax\": (-0.5, -0.1),\n",
        "            \"Renewable energy subsidies\": (0.1, 0.05),\n",
        "            \"Water conservation measures\": (0.02, 0.2)\n",
        "        }\n",
        "\n",
        "        current_actions = getattr(self.model, 'current_action', {})\n",
        "        for policy_name, (g_f, e_f) in policy_impact.items():\n",
        "            if policy_name in current_actions:\n",
        "                val = current_actions[policy_name]\n",
        "                p_val = float(val[0]) if isinstance(val, (np.ndarray, list)) else float(val)\n",
        "                gdp += p_val * random.uniform(g_f, g_f * 1.1)\n",
        "                employment += p_val * random.uniform(e_f, e_f * 1.1)\n",
        "\n",
        "        self.model.env.state[\"GDP\"] = gdp\n",
        "        self.model.env.state[\"Employment Rate\"] = np.clip(employment, 0.0, 1.0)\n",
        "        self.public_satisfaction = np.log1p(abs(self.class_population * self.class_wealth))\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def get_action_probs(self, state_dict):\n",
        "      state_values = [float(np.nan_to_num(v, nan=0.0)) for v in state_dict.values()]\n",
        "      if len(state_values) < self.state_dim:\n",
        "          state_values += [0.0] * (self.state_dim - len(state_values))\n",
        "      state_vector = np.array(state_values[:self.state_dim], dtype=np.float64)\n",
        "      state_vector = np.sign(state_vector) * np.log1p(np.abs(state_vector))\n",
        "      state_vector = np.nan_to_num(state_vector)\n",
        "      std = np.std(state_vector)\n",
        "      if std > 1e-6:\n",
        "          state_vector = (state_vector - np.mean(state_vector)) / (std + 1e-8)\n",
        "      else:\n",
        "          state_vector = state_vector - np.mean(state_vector)\n",
        "      probs = {}\n",
        "      for name, space in self.model.env.action_space.spaces.items():\n",
        "          weights = self.policy_weights[name]\n",
        "          if isinstance(space, gym.spaces.Discrete):\n",
        "              logits = np.dot(state_vector, weights)\n",
        "              e_x = np.exp(logits - np.max(logits))\n",
        "              probs[name] = e_x / (e_x.sum() + 1e-8)\n",
        "          else:\n",
        "              dot_val = np.dot(state_vector, weights)\n",
        "              dot_val = np.clip(dot_val, -15, 15)\n",
        "              probs[name] = 1.0 / (1.0 + np.exp(-dot_val))\n",
        "      return probs\n",
        "\n",
        "    def choose_action(self):\n",
        "        state_dict = self.get_state()\n",
        "        action = {}\n",
        "        all_probs = self.get_action_probs(state_dict)\n",
        "        for name, space in self.model.env.action_space.spaces.items():\n",
        "            if isinstance(space, gym.spaces.Discrete):\n",
        "                probs = all_probs[name]\n",
        "                probs = np.nan_to_num(probs, nan=1.0/len(probs))\n",
        "                probs /= probs.sum() + 1e-8\n",
        "                action[name] = np.random.choice(len(probs), p=probs)\n",
        "            else:\n",
        "                p_val = all_probs[name]\n",
        "                low, high = space.low[0], space.high[0]\n",
        "                action[name] = np.array([low + (high - low) * p_val], dtype=np.float32)\n",
        "        return action\n",
        "\n",
        "    def store_trajectory(self, state, action, reward):\n",
        "        if not hasattr(self, \"trajectory\"):\n",
        "            self.trajectory = []\n",
        "        self.trajectory.append((state, action, reward))\n",
        "\n",
        "    def update(self):\n",
        "        grads = []\n",
        "        total_return = 0\n",
        "        for s, a, r in reversed(self.trajectory):\n",
        "            total_return = r + self.gamma * total_return\n",
        "            all_probs = self.get_action_probs(s)\n",
        "            for name, val in a.items():\n",
        "                weights = self.policy_weights[name]\n",
        "                if isinstance(self.model.env.action_space[name], gym.spaces.Discrete):\n",
        "                    probs = all_probs[name]\n",
        "                    grad_logits = -probs.copy()\n",
        "                    grad_logits[int(val)] += 1.0\n",
        "                    grads.append((name, np.outer(list(s.values())[:self.state_dim], grad_logits) * total_return))\n",
        "                else:\n",
        "                    grads.append((name, np.outer(list(s.values())[:self.state_dim], [total_return]).flatten()))\n",
        "        for name, g in grads:\n",
        "            if g.shape == self.policy_weights[name].shape:\n",
        "                self.policy_weights[name] += self.learning_rate * g\n",
        "        self.trajectory = []\n",
        "\n",
        "    def step(self):\n",
        "        self.consumer_state()\n",
        "\n",
        "\n",
        "    def get_state(self):\n",
        "        agent_state = {\n",
        "            \"class_population\": np.log1p(abs(self.class_population)),\n",
        "            \"class_wealth\": np.log1p(abs(self.class_wealth)),\n",
        "            \"public_satisfaction\": self.public_satisfaction,\n",
        "        }\n",
        "        scaled_env_state = {k: (np.sign(v) * np.log1p(abs(v)) if isinstance(v, (int, float)) else v)\n",
        "                           for k, v in self.model.env.state.items()}\n",
        "        return {**agent_state, **scaled_env_state}\n",
        "\n",
        "    def step(self):\n",
        "        self.consumer_state()"
      ],
      "metadata": {
        "id": "2uGPAOIfyICL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import mesa\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import concurrent.futures\n",
        "class ClimateModel(mesa.Model):\n",
        "    def __init__(self, env, dataset):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.dataset = dataset\n",
        "        self.agent_list = []\n",
        "        self.current_action = {}\n",
        "\n",
        "    def environment(self, env):\n",
        "        self.state = self.env.state\n",
        "        self.t = self.env.t\n",
        "        return self.state, self.t\n",
        "\n",
        "    def build_agents(self, dataset):\n",
        "        self.agent_list = []\n",
        "        for name in self.dataset:\n",
        "            data = self.dataset[name]\n",
        "            for i, row in data.iterrows():\n",
        "                if name == \"business\":\n",
        "                    agent = BusinessAgents(self, row['unique_id'])\n",
        "                    agent.business_definition(row)\n",
        "                elif name == \"energy\":\n",
        "                    agent = EnergyAgents(self, row['unique_id'])\n",
        "                    agent.power_plant_definition(row)\n",
        "                elif name == \"sustainability\":\n",
        "                    agent = Sustainability(self, row['unique_id'])\n",
        "                    agent.sustainable_definition(row)\n",
        "                elif name == \"decision\":\n",
        "                    agent = DecisionAgents(self, row['unique_id'])\n",
        "                    agent.decision_definition(row)\n",
        "                elif name == \"consumer\":\n",
        "                    agent = ConsumerAgents(self, row['unique_id'])\n",
        "                    agent.consumer_definition(row)\n",
        "                self.agent_list.append(agent)\n",
        "\n",
        "    def scaling(self, action_dict):\n",
        "        scaling_map = {\n",
        "            \"Climate-Resilient Infrastructure Investment\": 1e9,\n",
        "            \"Public transport expansion\": 2e9,\n",
        "            \"Waste Management Reforms\": 2e8,\n",
        "            \"Flood defense infrastructure\": 1.5e9,\n",
        "            \"Heatwave resilience\": 1.2e9,\n",
        "            \"Sustainable Land-Use Zoning\": 1e8,\n",
        "            \"Green Business Investments\": 8e7,\n",
        "            \"Carbon Tax\": 1,\n",
        "            \"Water Consumption Tax\": 0.5,\n",
        "            \"Fuel Economy Standards\": 2,\n",
        "            \"Vehicle emission standards\": 3\n",
        "        }\n",
        "        discrete_keys = [\n",
        "            \"Fossil Fuel Phase-Out Regulations\",\n",
        "            \"Single-use plastics bans\",\n",
        "            \"Urban Green Space Expansion\"\n",
        "        ]\n",
        "        processed_action = action_dict.copy()\n",
        "        for k, v in processed_action.items():\n",
        "            val_clean = float(np.nan_to_num(v, nan=0.0))\n",
        "            if k in scaling_map and val_clean <= 1.05:\n",
        "                processed_action[k] = val_clean * scaling_map[k]\n",
        "            else:\n",
        "                processed_action[k] = val_clean\n",
        "            if k in discrete_keys:\n",
        "                processed_action[k] = int(round(processed_action[k]))\n",
        "            if k in self.env.action_space.spaces:\n",
        "                space = self.env.action_space[k]\n",
        "                if hasattr(space, \"low\"):\n",
        "                    processed_action[k] = np.clip(processed_action[k], space.low[0], space.high[0])\n",
        "                elif hasattr(space, \"n\"):\n",
        "                    processed_action[k] = np.clip(int(round(processed_action[k])), 0, space.n - 1)\n",
        "        return processed_action\n",
        "\n",
        "    def log_agent_step(self, dataset, agent, agent_action, next_state, batch_data, params):\n",
        "        c_name = agent.__class__.__name__\n",
        "        state = agent.get_state()\n",
        "\n",
        "        row = {f\"state_{k}\": v for k, v in state.items()}\n",
        "        row.update({f\"action_{k}\": float(v[0] if isinstance(v, (np.ndarray, list)) else v)\n",
        "                    for k, v in agent_action.items()})\n",
        "        row.update(params) # This adds LR, Epsilon, Gamma, Episode, Step\n",
        "\n",
        "        if isinstance(next_state, dict):\n",
        "            row.update({f\"next_{k}\": v for k, v in next_state.items()})\n",
        "        else:\n",
        "            row['next_state'] = next_state\n",
        "\n",
        "        if c_name not in batch_data:\n",
        "            batch_data[c_name] = []\n",
        "        batch_data[c_name].append(row)\n",
        "\n",
        "    def log_episode_results(self, dataset, batch_data, episode, lr=None, eps=None):\n",
        "        suffix = f\"_LR{lr}_EPS{eps}\" if lr is not None else \"\"\n",
        "\n",
        "        for agent_class, data in batch_data.items():\n",
        "            df = pd.DataFrame(data)\n",
        "            file_name = f\"log_{agent_class}{suffix}.csv\"\n",
        "\n",
        "            if episode == 0:\n",
        "                df.to_csv(file_name, index=False)\n",
        "            else:\n",
        "                df.to_csv(file_name, mode='a', header=False, index=False)\n",
        "\n",
        "        batch_data.clear()\n",
        "\n",
        "\n",
        "    def learn(self, episodes=100, lr=None, eps=None, gamma=None):\n",
        "        trackable_classes = [\"DecisionAgents\", \"Sustainability\", \"BusinessAgents\", \"EnergyAgents\", \"ConsumerAgents\"]\n",
        "        if not hasattr(self, 'model_history'):\n",
        "            self.model_history = []\n",
        "\n",
        "        agent_cache = []\n",
        "        for agent in self.agent_list:\n",
        "            c_name = agent.__class__.__name__\n",
        "\n",
        "            if lr is not None and hasattr(agent, 'learning_rate'):\n",
        "                if isinstance(agent,ConsumerAgents):\n",
        "                  if lr!=0.1:\n",
        "                    agent.learning_rate=lr*10\n",
        "                  else:\n",
        "                    agent.learning_rate=lr*5\n",
        "                else:\n",
        "                  agent.learning_rate = lr\n",
        "            if eps is not None and hasattr(agent, 'epsilon'):\n",
        "                agent.epsilon = eps\n",
        "            if gamma is not None and hasattr(agent, 'gamma'):\n",
        "                agent.gamma = gamma\n",
        "\n",
        "            agent_cache.append({\n",
        "                'obj': agent,\n",
        "                'name': c_name,\n",
        "                'id': agent.unique_id,\n",
        "                'trackable': c_name in trackable_classes\n",
        "            })\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            self.env.reset()\n",
        "            done, total_reward, steps = False, 0.0, 0\n",
        "            attribute_tracker = {cls: {} for cls in trackable_classes}\n",
        "            batch_data = {}\n",
        "            decay_rate = 0.95\n",
        "            current_eps = max(0.01, eps * (decay_rate ** episode))\n",
        "\n",
        "            for item in agent_cache:\n",
        "                if hasattr(item['obj'], 'epsilon'):\n",
        "                    item['obj'].epsilon = current_eps\n",
        "\n",
        "            for item in agent_cache:\n",
        "                item['cur_lr'] = getattr(item['obj'], 'learning_rate', 0)\n",
        "                item['cur_eps'] = getattr(item['obj'], 'epsilon', 0)\n",
        "                item['cur_gamma'] = getattr(item['obj'], 'gamma', 0)\n",
        "\n",
        "            while not done:\n",
        "                combined_action = {k: 0.0 for k in self.env.action_space.spaces}\n",
        "                weight_sum = {k: 0.0 for k in combined_action}\n",
        "                agent_actions_step = {}\n",
        "\n",
        "\n",
        "                for item in agent_cache:\n",
        "                    agent = item['obj']\n",
        "\n",
        "                    if item['trackable']:\n",
        "                        u_id, c_name = item['id'], item['name']\n",
        "                        for key, val in agent.__dict__.items():\n",
        "                            if isinstance(val, (int, float)) and key not in ('unique_id', 'state_dim'):\n",
        "                                attr_id = f\"{u_id}_{key}\"\n",
        "                                if attr_id not in attribute_tracker[c_name]:\n",
        "                                    attribute_tracker[c_name][attr_id] = []\n",
        "                                attribute_tracker[c_name][attr_id].append(val)\n",
        "\n",
        "                    if hasattr(agent, \"choose_action\"):\n",
        "                        act = agent.choose_action()\n",
        "                        agent_actions_step[item['id']] = act\n",
        "                        for k, v in act.items():\n",
        "                            val = v[0] if isinstance(v, (np.ndarray, list)) else v\n",
        "                            combined_action[k] += float(val)\n",
        "                            weight_sum[k] += 1.0\n",
        "\n",
        "                # 2. Env Step\n",
        "                scaled_action = {k: (combined_action[k]/weight_sum[k] if weight_sum[k] > 0 else 0.0)\n",
        "                                 for k in combined_action}\n",
        "                obs, _, reward, term, trunc, _ = self.env.step(self.scaling(scaled_action))\n",
        "                total_reward += reward\n",
        "\n",
        "                # 3. LOGGING\n",
        "                for item in agent_cache:\n",
        "                    params = {\n",
        "                        'lr': item['cur_lr'],\n",
        "                        'eps': item['cur_eps'],\n",
        "                        'gamma': item['cur_gamma'],\n",
        "                        'episode': episode,\n",
        "                        'step': steps,\n",
        "                        'reward': reward\n",
        "                    }\n",
        "                    self.log_agent_step(self.dataset, item['obj'],\n",
        "                                        agent_actions_step.get(item['id'], {}),\n",
        "                                        obs, batch_data, params)\n",
        "\n",
        "                    if hasattr(item['obj'], \"step\"):\n",
        "                        item['obj'].step()\n",
        "\n",
        "                steps += 1\n",
        "                done = term or trunc or steps >= 50\n",
        "\n",
        "            inf_scores = self.calculate_influence_scores(attribute_tracker)\n",
        "            self.print_influence_scores(inf_scores)\n",
        "            self.model_history.append({'episode': episode, 'reward': total_reward, **inf_scores})\n",
        "            self.log_episode_results(self.dataset, batch_data, episode,lr,eps)\n",
        "\n",
        "            print(f\"EP {episode + 1} | Reward: {total_reward:.2f} | LR: {lr} EPS: {eps}\")\n",
        "\n",
        "    def calculate_influence_scores(self, attribute_tracker):\n",
        "        influence_scores = {}\n",
        "        for cls, agents_attrs in attribute_tracker.items():\n",
        "            class_variations = []\n",
        "            for attr_id, history in agents_attrs.items():\n",
        "                if len(history) > 1:\n",
        "                    arr = np.array(history, dtype=np.float64)\n",
        "                    safe_arr = np.sign(arr) * np.log1p(np.abs(arr))\n",
        "                    mu = np.nanmean(safe_arr)\n",
        "                    sigma = np.nanstd(safe_arr)\n",
        "                    mu = 0.0 if not np.isfinite(mu) else mu\n",
        "                    sigma = 0.0 if not np.isfinite(sigma) else sigma\n",
        "\n",
        "                    if abs(mu) > 1e-9:\n",
        "                        class_variations.append(sigma / (abs(mu) + 1e-10))\n",
        "\n",
        "            influence_scores[cls] = np.mean(class_variations) if class_variations else 0.0\n",
        "\n",
        "        return influence_scores\n",
        "\n",
        "    def print_influence_scores(self, influence_scores):\n",
        "        total_inf = sum(influence_scores.values()) + 1e-10\n",
        "        print(f\"{'Agent Class':<20} | {'Influence (%)':>15}\")\n",
        "        print(\"-\" * 40)\n",
        "        for cls, val in sorted(influence_scores.items(), key=lambda x: x[1], reverse=True):\n",
        "            share = (val / total_inf) * 100\n",
        "            print(f\"{cls:<20} | {share:>14.2f}%\")\n",
        "\n",
        "\n",
        "    def step_env(self):\n",
        "        attribute_tracker = {cls: {} for cls in [\"DecisionAgents\", \"Sustainability\", \"BusinessAgents\", \"EnergyAgents\", \"ConsumerAgents\"]}\n",
        "        agent_states = {agent.unique_id: agent.get_state() if hasattr(agent, \"get_state\") else None\n",
        "                        for agent in self.agent_list}\n",
        "\n",
        "        combined_action = {k: 0.0 if hasattr(self.env.action_space[k], \"low\") else 0\n",
        "                            for k in self.env.action_space.spaces}\n",
        "        weight_sum = {k: 0.0 for k in combined_action}\n",
        "\n",
        "        default_weights = {\n",
        "            \"BusinessAgents\": 0.2,\n",
        "            \"EnergyAgents\": 0.2,\n",
        "            \"DecisionAgents\": 0.3,\n",
        "            \"Sustainability\": 0.2,\n",
        "            \"ConsumerAgents\": 0.1\n",
        "        }\n",
        "\n",
        "        agent_actions = {}\n",
        "        batch_data = {}\n",
        "\n",
        "        for agent in self.agent_list:\n",
        "            c_name = agent.__class__.__name__\n",
        "            for key, value in agent.__dict__.items():\n",
        "                if isinstance(value, (int, float)) and key not in ['unique_id', 'state_dim']:\n",
        "                    attr_id = f\"{agent.unique_id}_{key}\"\n",
        "                    if attr_id not in attribute_tracker[c_name]:\n",
        "                        attribute_tracker[c_name][attr_id] = []\n",
        "                    attribute_tracker[c_name][attr_id].append(float(value))\n",
        "\n",
        "            if hasattr(agent, \"choose_action\"):\n",
        "                agent_action = agent.choose_action()\n",
        "                for k, v in agent_action.items():\n",
        "                    if k in combined_action:\n",
        "                        val = float(v[0]) if isinstance(v, (np.ndarray, list)) else float(v)\n",
        "                        combined_action[k] += val\n",
        "                        weight_sum[k] += 1.0\n",
        "\n",
        "        scaled_action = combined_action.copy()\n",
        "        for k in combined_action:\n",
        "            if weight_sum[k] > 0:\n",
        "                scaled_action[k] /= weight_sum[k]\n",
        "\n",
        "        state, scaled_obs, reward, terminated, truncated, info = self.env.step(self.scaling(scaled_action))\n",
        "\n",
        "        for agent in self.agent_list:\n",
        "            if hasattr(agent, \"step\"):\n",
        "                agent.step()\n",
        "\n",
        "            self.log_agent_step(dataset, agent, agent_actions.get(agent.unique_id, {}), _, batch_data)\n",
        "\n",
        "        if terminated or truncated:\n",
        "            self.log_episode_results(dataset, batch_data)\n",
        "        print(f\"State: {state}\")\n",
        "        print(f\"Reward: {reward}\")\n",
        "        print(f\"Info:{info}\")\n",
        "        if terminated or truncated:\n",
        "            print(\"Episode Terminated!\")\n",
        "        return state, scaled_obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wwqb7W6JDEp4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=GeographicRegion(seed=42)\n",
        "state,info=env.reset()\n",
        "action=env.action_space.sample()\n",
        "print(\"Sampled action:\", action)\n",
        "\n",
        "state, scaled_state, reward, truncated, done, info=env.step(action)\n",
        "print(state)\n",
        "print(scaled_state)\n",
        "print(reward)\n",
        "print(truncated)\n",
        "print(done)\n",
        "print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApKl-UaP7vkh",
        "outputId": "a29958e6-f57f-43a2-a47c-efb94933773a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled action: {'Carbon Tax': array([44.903137], dtype=float32), 'Climate-Resilient Infrastructure Investment': array([1.644464e+08], dtype=float32), 'Electric Vehicle (EV) Subsidies': array([0.04410264], dtype=float32), 'Energy Efficiency Incentives': array([0.95554954], dtype=float32), 'Flood defense infrastructure': array([1.072968e+08], dtype=float32), 'Fossil Fuel Phase-Out Regulations': np.int64(0), 'Fuel Economy Standards': np.int64(0), 'Green Business Investments': array([35722292.], dtype=float32), 'Heatwave resilience': array([1.1469116e+08], dtype=float32), 'Public transport expansion': array([1.3152037e+08], dtype=float32), 'Recycling Rate': array([0.8117917], dtype=float32), 'Renewable energy subsidies': array([0.97453797], dtype=float32), 'Single-use plastics bans': np.int64(2), 'Sustainable Land-Use Zoning': array([72477640.], dtype=float32), 'Urban Green Space Expansion': array([137.13008], dtype=float32), 'Vehicle emission standards': np.int64(3), 'Waste Management Reforms': array([74020360.], dtype=float32), 'Water Consumption Tax': array([5.5893283], dtype=float32), 'Water conservation measures': array([58475360.], dtype=float32)}\n",
            "{'Temperature': np.float64(15.030303181350888), 'Precipitation': 2.861485971162751, 'Humidity': 51.99582915145766, 'Air Pollution Index': 92.10657836279816, 'Water Quality Index': np.float32(59.426712), 'Carbon Dioxide Emissions Per Capita': np.float32(5.395066), 'Electricity Consumption Per Capita': np.float32(624.8755), 'Renewable Share': 0.32367720186661747, 'Water Consumption Per Capita': 88.24619912671628, 'Population': 9242680, 'GDP': np.float64(32653758175.517235), 'Employment Rate': np.float64(0.873979749789018), 'Waste Management Efficiency': np.float32(0.64159095), 'Energy Efficiency': 0.5130103185970559, 'Urban Green Space Expansion': np.float32(48.798115), 'Flood defense infrastructure': np.float32(48488510.0), 'Heatwave resilience': np.float32(16279864.0), 'Sustainable Land-Use Zoning': np.float32(6206456.5), 'Single-use plastics bans': 0.0, 'Green Business Investments': np.float32(2050962.5)}\n",
            "{'Temperature': array([0.5003367]), 'Precipitation': array([0.05249218], dtype=float32), 'Humidity': array([0.5199583], dtype=float32), 'Air Pollution Index': array([0.18421316]), 'Water Quality Index': array([0.59426713], dtype=float32), 'Carbon Dioxide Emissions Per Capita': array([0.31393328], dtype=float32), 'Electricity Consumption Per Capita': array([0.0319043], dtype=float32), 'Renewable Share': 0.32367720186661747, 'Water Consumption Per Capita': array([0.20680667], dtype=float32), 'Population': np.float64(16.03934255304721), 'GDP': np.float64(24.209225790991713), 'Employment Rate': np.float64(0.873979749789018), 'Waste Management Efficiency': np.float32(0.64159095), 'Energy Efficiency': 0.5130103185970559, 'Urban Green Space Expansion': np.float32(48.798115), 'Flood defense infrastructure': np.float32(48488510.0), 'Heatwave resilience': np.float32(16279864.0), 'Sustainable Land-Use Zoning': np.float32(6206456.5), 'Single-use plastics bans': 0.0, 'Green Business Investments': np.float32(2050962.5)}\n",
            "[7.68482335]\n",
            "False\n",
            "False\n",
            "{'environmental_impact': array([-0.33144055]), 'energy_efficiency': array([3.47783526]), 'resource_management': array([0.8609337], dtype=float32), 'economic_growth': array([0.86766906]), 'climate_resilience': array([0.8561771], dtype=float32), 'air_pollution_index': 92.10657836279816, 'carbon_emissions_per_capita': np.float32(5.395066), 'gdp': np.float64(32653758175.517235), 'employment_rate': np.float64(0.873979749789018), 'electricity_consumption': np.float32(624.8755), 'water_quality': np.float32(59.426712), 'waste_efficiency': np.float32(0.64159095)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-185795451.py:142: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  0.3 * float(consumption_efficiency_score) +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=GeographicRegion(seed=42)\n",
        "model=ClimateModel(env,dataset)\n",
        "model.build_agents(dataset)\n",
        "model.learn(episodes=50,lr=0.001, eps=0.3, gamma=0.99)\n"
      ],
      "metadata": {
        "id": "J7pnlpGn2Bvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "agents= [\"Sustainability\", \"DecisionAgents\", \"BusinessAgents\", \"EnergyAgents\", \"ConsumerAgents\"]\n",
        "data={}\n",
        "suffix = f\"_LR{0.001}_EPS{0.3}\"\n",
        "for agent_class in agents:\n",
        "  file_name = f\"log_{agent_class}{suffix}.csv\"\n",
        "  file_path = \"/content/drive/MyDrive/\" + file_name\n",
        "  dataframe = pd.read_csv(file_path)\n",
        "  data[agent_class]=dataframe\n",
        "  print(dataframe.columns)\n"
      ],
      "metadata": {
        "id": "CSls_3DMHV4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1f477e-784f-4e60-986a-d62d51112b97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['state_production_level', 'state_net_emissions',\n",
            "       'state_net_carbon_intensity', 'state_energy_consumption',\n",
            "       'state_water_consumption', 'state_resources_consumption',\n",
            "       'state_waste_creation', 'state_waste_recycling', 'state_Temperature',\n",
            "       'state_Precipitation', 'state_Humidity', 'state_Air Pollution Index',\n",
            "       'state_Water Quality Index',\n",
            "       'state_Carbon Dioxide Emissions Per Capita',\n",
            "       'state_Electricity Consumption Per Capita', 'state_Renewable Share',\n",
            "       'state_Water Consumption Per Capita', 'state_Population', 'state_GDP',\n",
            "       'state_Employment Rate', 'state_Waste Management Efficiency',\n",
            "       'state_Energy Efficiency', 'state_Urban Green Space Expansion',\n",
            "       'state_Flood defense infrastructure', 'state_Heatwave resilience',\n",
            "       'state_Sustainable Land-Use Zoning', 'state_Single-use plastics bans',\n",
            "       'state_Green Business Investments', 'action_Carbon Tax',\n",
            "       'action_Climate-Resilient Infrastructure Investment',\n",
            "       'action_Electric Vehicle (EV) Subsidies',\n",
            "       'action_Energy Efficiency Incentives',\n",
            "       'action_Flood defense infrastructure',\n",
            "       'action_Fossil Fuel Phase-Out Regulations',\n",
            "       'action_Fuel Economy Standards', 'action_Green Business Investments',\n",
            "       'action_Heatwave resilience', 'action_Public transport expansion',\n",
            "       'action_Recycling Rate', 'action_Renewable energy subsidies',\n",
            "       'action_Single-use plastics bans', 'action_Sustainable Land-Use Zoning',\n",
            "       'action_Urban Green Space Expansion',\n",
            "       'action_Vehicle emission standards', 'action_Waste Management Reforms',\n",
            "       'action_Water Consumption Tax', 'action_Water conservation measures',\n",
            "       'lr', 'eps', 'gamma', 'episode', 'step', 'reward', 'next_Temperature',\n",
            "       'next_Precipitation', 'next_Humidity', 'next_Air Pollution Index',\n",
            "       'next_Water Quality Index', 'next_Carbon Dioxide Emissions Per Capita',\n",
            "       'next_Electricity Consumption Per Capita', 'next_Renewable Share',\n",
            "       'next_Water Consumption Per Capita', 'next_Population', 'next_GDP',\n",
            "       'next_Employment Rate', 'next_Waste Management Efficiency',\n",
            "       'next_Energy Efficiency', 'next_Urban Green Space Expansion',\n",
            "       'next_Flood defense infrastructure', 'next_Heatwave resilience',\n",
            "       'next_Sustainable Land-Use Zoning', 'next_Single-use plastics bans',\n",
            "       'next_Green Business Investments'],\n",
            "      dtype='object')\n",
            "Index(['state_budget', 'state_tax_capacity', 'state_subsidy_capacity',\n",
            "       'state_investment_ceiling', 'state_investment_floor',\n",
            "       'state_economic_growth_priority', 'state_emissions_priority',\n",
            "       'state_sustainability_priority', 'state_energy_priority',\n",
            "       'state_social_welfare_priority', 'state_regulation_strictness',\n",
            "       'state_penalty_severity', 'state_incentive_intensity',\n",
            "       'state_Temperature', 'state_Precipitation', 'state_Humidity',\n",
            "       'state_Air Pollution Index', 'state_Water Quality Index',\n",
            "       'state_Carbon Dioxide Emissions Per Capita',\n",
            "       'state_Electricity Consumption Per Capita', 'state_Renewable Share',\n",
            "       'state_Water Consumption Per Capita', 'state_Population', 'state_GDP',\n",
            "       'state_Employment Rate', 'state_Waste Management Efficiency',\n",
            "       'state_Energy Efficiency', 'state_Urban Green Space Expansion',\n",
            "       'state_Flood defense infrastructure', 'state_Heatwave resilience',\n",
            "       'state_Sustainable Land-Use Zoning', 'state_Single-use plastics bans',\n",
            "       'state_Green Business Investments', 'action_Carbon Tax',\n",
            "       'action_Climate-Resilient Infrastructure Investment',\n",
            "       'action_Electric Vehicle (EV) Subsidies',\n",
            "       'action_Energy Efficiency Incentives',\n",
            "       'action_Flood defense infrastructure',\n",
            "       'action_Fossil Fuel Phase-Out Regulations',\n",
            "       'action_Fuel Economy Standards', 'action_Green Business Investments',\n",
            "       'action_Heatwave resilience', 'action_Public transport expansion',\n",
            "       'action_Recycling Rate', 'action_Renewable energy subsidies',\n",
            "       'action_Single-use plastics bans', 'action_Sustainable Land-Use Zoning',\n",
            "       'action_Urban Green Space Expansion',\n",
            "       'action_Vehicle emission standards', 'action_Waste Management Reforms',\n",
            "       'action_Water Consumption Tax', 'action_Water conservation measures',\n",
            "       'lr', 'eps', 'gamma', 'episode', 'step', 'reward', 'next_Temperature',\n",
            "       'next_Precipitation', 'next_Humidity', 'next_Air Pollution Index',\n",
            "       'next_Water Quality Index', 'next_Carbon Dioxide Emissions Per Capita',\n",
            "       'next_Electricity Consumption Per Capita', 'next_Renewable Share',\n",
            "       'next_Water Consumption Per Capita', 'next_Population', 'next_GDP',\n",
            "       'next_Employment Rate', 'next_Waste Management Efficiency',\n",
            "       'next_Energy Efficiency', 'next_Urban Green Space Expansion',\n",
            "       'next_Flood defense infrastructure', 'next_Heatwave resilience',\n",
            "       'next_Sustainable Land-Use Zoning', 'next_Single-use plastics bans',\n",
            "       'next_Green Business Investments'],\n",
            "      dtype='object')\n",
            "Index(['state_valuation', 'state_revenue', 'state_growth_rate',\n",
            "       'state_sustainability_index', 'state_energy_consumption',\n",
            "       'state_Temperature', 'state_Precipitation', 'state_Humidity',\n",
            "       'state_Air Pollution Index', 'state_Water Quality Index',\n",
            "       'state_Carbon Dioxide Emissions Per Capita',\n",
            "       'state_Electricity Consumption Per Capita', 'state_Renewable Share',\n",
            "       'state_Water Consumption Per Capita', 'state_Population', 'state_GDP',\n",
            "       'state_Employment Rate', 'state_Waste Management Efficiency',\n",
            "       'state_Energy Efficiency', 'state_Urban Green Space Expansion',\n",
            "       'state_Flood defense infrastructure', 'state_Heatwave resilience',\n",
            "       'state_Sustainable Land-Use Zoning', 'state_Single-use plastics bans',\n",
            "       'state_Green Business Investments', 'action_Carbon Tax',\n",
            "       'action_Climate-Resilient Infrastructure Investment',\n",
            "       'action_Electric Vehicle (EV) Subsidies',\n",
            "       'action_Energy Efficiency Incentives',\n",
            "       'action_Flood defense infrastructure',\n",
            "       'action_Fossil Fuel Phase-Out Regulations',\n",
            "       'action_Fuel Economy Standards', 'action_Green Business Investments',\n",
            "       'action_Heatwave resilience', 'action_Public transport expansion',\n",
            "       'action_Recycling Rate', 'action_Renewable energy subsidies',\n",
            "       'action_Single-use plastics bans', 'action_Sustainable Land-Use Zoning',\n",
            "       'action_Urban Green Space Expansion',\n",
            "       'action_Vehicle emission standards', 'action_Waste Management Reforms',\n",
            "       'action_Water Consumption Tax', 'action_Water conservation measures',\n",
            "       'lr', 'eps', 'gamma', 'episode', 'step', 'reward', 'next_Temperature',\n",
            "       'next_Precipitation', 'next_Humidity', 'next_Air Pollution Index',\n",
            "       'next_Water Quality Index', 'next_Carbon Dioxide Emissions Per Capita',\n",
            "       'next_Electricity Consumption Per Capita', 'next_Renewable Share',\n",
            "       'next_Water Consumption Per Capita', 'next_Population', 'next_GDP',\n",
            "       'next_Employment Rate', 'next_Waste Management Efficiency',\n",
            "       'next_Energy Efficiency', 'next_Urban Green Space Expansion',\n",
            "       'next_Flood defense infrastructure', 'next_Heatwave resilience',\n",
            "       'next_Sustainable Land-Use Zoning', 'next_Single-use plastics bans',\n",
            "       'next_Green Business Investments'],\n",
            "      dtype='object')\n",
            "Index(['state_capacity', 'state_production', 'state_efficiency',\n",
            "       'state_carbon_intensity', 'state_emissions', 'state_Temperature',\n",
            "       'state_Precipitation', 'state_Humidity', 'state_Air Pollution Index',\n",
            "       'state_Water Quality Index',\n",
            "       'state_Carbon Dioxide Emissions Per Capita',\n",
            "       'state_Electricity Consumption Per Capita', 'state_Renewable Share',\n",
            "       'state_Water Consumption Per Capita', 'state_Population', 'state_GDP',\n",
            "       'state_Employment Rate', 'state_Waste Management Efficiency',\n",
            "       'state_Energy Efficiency', 'state_Urban Green Space Expansion',\n",
            "       'state_Flood defense infrastructure', 'state_Heatwave resilience',\n",
            "       'state_Sustainable Land-Use Zoning', 'state_Single-use plastics bans',\n",
            "       'state_Green Business Investments', 'action_Carbon Tax',\n",
            "       'action_Climate-Resilient Infrastructure Investment',\n",
            "       'action_Electric Vehicle (EV) Subsidies',\n",
            "       'action_Energy Efficiency Incentives',\n",
            "       'action_Flood defense infrastructure',\n",
            "       'action_Fossil Fuel Phase-Out Regulations',\n",
            "       'action_Fuel Economy Standards', 'action_Green Business Investments',\n",
            "       'action_Heatwave resilience', 'action_Public transport expansion',\n",
            "       'action_Recycling Rate', 'action_Renewable energy subsidies',\n",
            "       'action_Single-use plastics bans', 'action_Sustainable Land-Use Zoning',\n",
            "       'action_Urban Green Space Expansion',\n",
            "       'action_Vehicle emission standards', 'action_Waste Management Reforms',\n",
            "       'action_Water Consumption Tax', 'action_Water conservation measures',\n",
            "       'lr', 'eps', 'gamma', 'episode', 'step', 'reward', 'next_Temperature',\n",
            "       'next_Precipitation', 'next_Humidity', 'next_Air Pollution Index',\n",
            "       'next_Water Quality Index', 'next_Carbon Dioxide Emissions Per Capita',\n",
            "       'next_Electricity Consumption Per Capita', 'next_Renewable Share',\n",
            "       'next_Water Consumption Per Capita', 'next_Population', 'next_GDP',\n",
            "       'next_Employment Rate', 'next_Waste Management Efficiency',\n",
            "       'next_Energy Efficiency', 'next_Urban Green Space Expansion',\n",
            "       'next_Flood defense infrastructure', 'next_Heatwave resilience',\n",
            "       'next_Sustainable Land-Use Zoning', 'next_Single-use plastics bans',\n",
            "       'next_Green Business Investments'],\n",
            "      dtype='object')\n",
            "Index(['state_class_population', 'state_class_wealth',\n",
            "       'state_public_satisfaction', 'state_Temperature', 'state_Precipitation',\n",
            "       'state_Humidity', 'state_Air Pollution Index',\n",
            "       'state_Water Quality Index',\n",
            "       'state_Carbon Dioxide Emissions Per Capita',\n",
            "       'state_Electricity Consumption Per Capita', 'state_Renewable Share',\n",
            "       'state_Water Consumption Per Capita', 'state_Population', 'state_GDP',\n",
            "       'state_Employment Rate', 'state_Waste Management Efficiency',\n",
            "       'state_Energy Efficiency', 'state_Urban Green Space Expansion',\n",
            "       'state_Flood defense infrastructure', 'state_Heatwave resilience',\n",
            "       'state_Sustainable Land-Use Zoning', 'state_Single-use plastics bans',\n",
            "       'state_Green Business Investments', 'action_Carbon Tax',\n",
            "       'action_Climate-Resilient Infrastructure Investment',\n",
            "       'action_Electric Vehicle (EV) Subsidies',\n",
            "       'action_Energy Efficiency Incentives',\n",
            "       'action_Flood defense infrastructure',\n",
            "       'action_Fossil Fuel Phase-Out Regulations',\n",
            "       'action_Fuel Economy Standards', 'action_Green Business Investments',\n",
            "       'action_Heatwave resilience', 'action_Public transport expansion',\n",
            "       'action_Recycling Rate', 'action_Renewable energy subsidies',\n",
            "       'action_Single-use plastics bans', 'action_Sustainable Land-Use Zoning',\n",
            "       'action_Urban Green Space Expansion',\n",
            "       'action_Vehicle emission standards', 'action_Waste Management Reforms',\n",
            "       'action_Water Consumption Tax', 'action_Water conservation measures',\n",
            "       'lr', 'eps', 'gamma', 'episode', 'step', 'reward', 'next_Temperature',\n",
            "       'next_Precipitation', 'next_Humidity', 'next_Air Pollution Index',\n",
            "       'next_Water Quality Index', 'next_Carbon Dioxide Emissions Per Capita',\n",
            "       'next_Electricity Consumption Per Capita', 'next_Renewable Share',\n",
            "       'next_Water Consumption Per Capita', 'next_Population', 'next_GDP',\n",
            "       'next_Employment Rate', 'next_Waste Management Efficiency',\n",
            "       'next_Energy Efficiency', 'next_Urban Green Space Expansion',\n",
            "       'next_Flood defense infrastructure', 'next_Heatwave resilience',\n",
            "       'next_Sustainable Land-Use Zoning', 'next_Single-use plastics bans',\n",
            "       'next_Green Business Investments'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import spektral\n",
        "from spektral.layers import GeneralConv\n",
        "from spektral.models import GeneralGNN\n",
        "\n",
        "\n",
        "class GraphConventionalModel(GeneralGNN):\n",
        "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer_1 = GeneralConv(input_dim,hidden_dim)\n",
        "        self.layer_2 = GeneralConv(hidden_dim, output_dim)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x, edge_index, edge_attr = inputs\n",
        "        x = self.layer_1([x, edge_index, edge_attr])\n",
        "        x = tf.nn.relu(x)\n",
        "        x= tf.nn.dropout(0.2)(x) if training else x\n",
        "        x = self.layer_2([x, edge_index, edge_attr])\n",
        "        return x\n",
        "\n",
        "\n",
        "class GraphActorCriticModel(GeneralGNN):\n",
        "    def __init__(self,input_dim, hidden_dim, output_dim,num_continuous_actions,num_discrete_actions,neurons=256):\n",
        "        super().__init__()\n",
        "        self.graph_layer=GraphConventionalModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "        self.layer_1=keras.layers.Dense(neurons,activation=\"relu\")\n",
        "        self.layer_2=keras.layers.Dense(neurons//2,activation=\"relu\")\n",
        "        self.layer_3=keras.layers.Dense(neurons//4,activation=\"relu\")\n",
        "\n",
        "        self.actor_continuous = keras.layers.Dense(num_continuous_actions, activation='tanh')\n",
        "        self.actor_discrete = keras.layers.Dense(num_discrete_actions, activation='softmax')\n",
        "        self.critic_layer=keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x, edge_index, edge_attr = inputs\n",
        "        x = tf.reduce_mean(x, axis=0, keepdims=True)\n",
        "        x = self.graph_layer([x, edge_index, edge_attr],training=training)\n",
        "        x = self.layer_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.layer_3(x)\n",
        "\n",
        "        actor_continuous = self.actor_continuous(x)\n",
        "        actor_discrete = self.actor_discrete(x)\n",
        "        critic = self.critic_layer(x)\n",
        "\n",
        "        return actor_continuous, actor_discrete, critic\n",
        "\n"
      ],
      "metadata": {
        "id": "0aHNbsyqWCHz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def action_spaces(env:GeographicRegion):\n",
        "  continuous_actions = []\n",
        "  discrete_actions = []\n",
        "\n",
        "  for name, space in env.action_space.spaces.items():\n",
        "      if isinstance(space, gym.spaces.Box):\n",
        "          continuous_actions.append((name, space.low, space.high))\n",
        "      else:\n",
        "          discrete_actions.append((name, space.n))\n",
        "  return continuous_actions, discrete_actions\n",
        "\n",
        "def flatten_state(state_dict):\n",
        "    return np.array(\n",
        "        [state_dict[key] for key in env.state_space.spaces.keys()],\n",
        "        dtype=np.float32\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KMkdq_1kZp8B"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "def dense(x, units, activation=None, kernel_initializer=None, name=None,use_bias=None):\n",
        "  layer = tf.keras.layers.Dense(\n",
        "      units=units,\n",
        "      activation=activation,\n",
        "      kernel_initializer=kernel_initializer,\n",
        "      use_bias=use_bias,\n",
        "      name=name)\n",
        "  return layer(x)"
      ],
      "metadata": {
        "id": "9UgD0BvHJpFD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import threading\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "no_of_workers = multiprocessing.cpu_count()\n",
        "no_of_ep_steps = 50\n",
        "no_of_episodes = 100\n",
        "global_net_scope = 'Global_Net'\n",
        "update_global = 64\n",
        "gamma = 0.99\n",
        "entropy_beta = 0.12\n",
        "lr_a = 0.00015\n",
        "lr_c = 0.00015\n",
        "render = False\n",
        "log_dir = 'logs'\n",
        "\n",
        "class A3C(object):\n",
        "    def __init__(self, scope, session, MAX_STATE_NO, action_continuous, action_discrete, global_net=None):\n",
        "        self.sess = session\n",
        "        self.state_no = MAX_STATE_NO\n",
        "        self.action_continuous = action_continuous\n",
        "        self.action_discrete = action_discrete\n",
        "        self.discrete_names = [d[0] for d in action_discrete]\n",
        "        self.n_cont = len(action_continuous)\n",
        "        self.cont_low = np.array([x[1] for x in action_continuous], dtype=np.float32)\n",
        "        self.cont_high = np.array([x[2] for x in action_continuous], dtype=np.float32)\n",
        "        self.unit_scales = [150.0, 2000000.0, 1.0, 1.0, 5000000.0, 2000000.0, 2000000.0, 3000000.0, 1.0, 1.0, 5.0, 2000000.0, 200.0, 1000000.0, 10.0, 2000000.0]\n",
        "        self.percent_scale = tf.constant(self.unit_scales, dtype=tf.float32)\n",
        "\n",
        "        self.actor_optimizer = tf.compat.v1.train.AdamOptimizer(lr_a)\n",
        "        self.critic_optimizer = tf.compat.v1.train.AdamOptimizer(lr_c)\n",
        "\n",
        "        with tf.compat.v1.variable_scope(scope):\n",
        "            self.s = tf.compat.v1.placeholder(tf.float32, [None, self.state_no], 'S')\n",
        "            self.mean_raw, self.delta, self.log_var, self.var_con, self.v, self.logits_dict, self.a_cont_params, self.a_dis_params, self.c_params = self._build_net(scope)\n",
        "            self.a_params = self.a_cont_params + self.a_dis_params\n",
        "\n",
        "            self.a_cont = tf.compat.v1.placeholder(tf.float32, [None, self.n_cont], 'A_continuous')\n",
        "            self.a_dis = {name: tf.compat.v1.placeholder(tf.int32, [None]) for name in self.discrete_names}\n",
        "            self.v_target = tf.compat.v1.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
        "            self.old_log_prob = tf.compat.v1.placeholder(tf.float32, [None], 'old_log_p_continuous')\n",
        "            self.old_log_prob_d = tf.compat.v1.placeholder(tf.float32, [None], 'old_log_p_discrete')\n",
        "            self.current_action = tf.compat.v1.placeholder(tf.float32, [None, self.n_cont], 'current_action')\n",
        "            self.dataset_mean = tf.compat.v1.placeholder(tf.float32, [None,self.n_cont], 'dataset_mean')\n",
        "            self.dataset_std = tf.compat.v1.placeholder(tf.float32, [None,self.n_cont], 'dataset_std')\n",
        "\n",
        "            self.mean = tf.clip_by_value(self.mean_raw, self.cont_low, (self.cont_high*1.5))\n",
        "\n",
        "            td = tf.subtract(self.v_target, self.v)\n",
        "            self.critic_loss = tf.compat.v1.losses.huber_loss(self.v_target, self.v)\n",
        "\n",
        "            std = tf.exp(self.log_var) + 1e-3\n",
        "            normal_dist = tf.compat.v1.distributions.Normal(self.mean, std)\n",
        "\n",
        "            self.log_prob_tensor = tf.reduce_sum(normal_dist.log_prob(self.a_cont), axis=1)\n",
        "            entropy = tf.reduce_sum(normal_dist.entropy(), axis=1)\n",
        "            entropy = tf.where(tf.math.is_nan(entropy), tf.zeros_like(entropy), entropy)\n",
        "            entropy = tf.maximum(entropy, 1e-4)\n",
        "\n",
        "            advantage = tf.stop_gradient(td[:, 0])\n",
        "\n",
        "            ratio = tf.exp(self.log_prob_tensor - self.old_log_prob)\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = tf.clip_by_value(ratio, 0.8, 1.2) * advantage\n",
        "            ppo_loss = tf.minimum(surr1, surr2)\n",
        "\n",
        "            self.actor_loss_c = -tf.reduce_mean(ppo_loss + (entropy_beta * entropy))\n",
        "            action_l2 = tf.reduce_mean(tf.square(self.delta))\n",
        "            self.actor_loss_c += (0.00001 * action_l2)\n",
        "\n",
        "            discrete_losses = []\n",
        "            discrete_entropies = []\n",
        "            for name in self.discrete_names:\n",
        "                logits = self.logits_dict[name]\n",
        "                loss_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=self.a_dis[name])\n",
        "                discrete_losses.append(loss_i)\n",
        "                probs = tf.nn.softmax(logits)\n",
        "                ent_i = -tf.reduce_sum(probs * tf.math.log(probs + 1e-8), axis=1)\n",
        "                discrete_entropies.append(ent_i)\n",
        "\n",
        "            total_discrete_loss = tf.add_n(discrete_losses)\n",
        "            self.discrete_log_prob_tensor = -total_discrete_loss\n",
        "            total_discrete_entropy = tf.add_n(discrete_entropies)\n",
        "\n",
        "            ratio_d = tf.exp(self.discrete_log_prob_tensor - self.old_log_prob_d)\n",
        "            surr1_d = ratio_d * advantage\n",
        "            surr2_d = tf.clip_by_value(ratio_d, 0.8, 1.2) * advantage\n",
        "\n",
        "            self.actor_loss_d = -tf.reduce_mean(tf.minimum(surr1_d, surr2_d) + entropy_beta * total_discrete_entropy)\n",
        "\n",
        "            if global_net is not None:\n",
        "                total_actor_loss = self.actor_loss_c + self.actor_loss_d\n",
        "                self.a_grads = tf.gradients(total_actor_loss, self.a_params)\n",
        "                self.c_grads = tf.gradients(self.critic_loss, self.c_params)\n",
        "\n",
        "                self.a_grads, _ = tf.clip_by_global_norm(self.a_grads, 5.0)\n",
        "                self.c_grads, _ = tf.clip_by_global_norm(self.c_grads, 5.0)\n",
        "\n",
        "                noise = tf.random.normal(shape=tf.shape(self.mean), mean=0.0, stddev=1e-3)\n",
        "                self.mean = self.mean + tf.stop_gradient(noise * self.percent_scale)\n",
        "\n",
        "                with tf.name_scope('push'):\n",
        "                    a_grads_vars = [(g, v) for g, v in zip(self.a_grads, global_net.a_params) if g is not None]\n",
        "                    c_grads_vars = [(g, v) for g, v in zip(self.c_grads, global_net.c_params) if g is not None]\n",
        "                    self.update_a = self.actor_optimizer.apply_gradients(a_grads_vars) if a_grads_vars else tf.no_op()\n",
        "                    self.update_c = self.critic_optimizer.apply_gradients(c_grads_vars) if c_grads_vars else tf.no_op()\n",
        "\n",
        "                with tf.name_scope('pull'):\n",
        "                    self.pull_a = [l.assign(g) for l, g in zip(self.a_params, global_net.a_params)]\n",
        "                    self.pull_c = [l.assign(g) for l, g in zip(self.c_params, global_net.c_params)]\n",
        "                    self.pull_op = tf.group(self.pull_a, self.pull_c)\n",
        "\n",
        "                self.train_op = tf.group(self.update_a, self.update_c)\n",
        "            else:\n",
        "                self.train_op_actor = tf.compat.v1.train.AdamOptimizer(lr_a).minimize(self.actor_loss_c + self.actor_loss_d, var_list=self.a_params)\n",
        "                self.train_op_critic = tf.compat.v1.train.AdamOptimizer(lr_c).minimize(self.critic_loss, var_list=self.c_params)\n",
        "                self.train_op = tf.group(self.train_op_actor, self.train_op_critic)\n",
        "                self.pull_op = tf.no_op()\n",
        "\n",
        "    def _build_net(self, scope):\n",
        "        w = tf.compat.v1.keras.initializers.he_normal()\n",
        "        small_w = tf.compat.v1.initializers.truncated_normal(stddev=0.0001)\n",
        "\n",
        "        with tf.compat.v1.variable_scope('actor_continuous'):\n",
        "            l_a = dense(self.s, 512, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "            l_a2 = dense(l_a, 256, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "\n",
        "            log_var = dense(l_a2, self.n_cont, kernel_initializer=w, name='log_var')\n",
        "            log_var = tf.clip_by_value(log_var, -1.0, 1.0)\n",
        "\n",
        "            delta = dense(l_a2, self.n_cont, activation=tf.nn.tanh, kernel_initializer=small_w, use_bias=False)\n",
        "            delta = tf.reshape(delta, [-1, self.n_cont])\n",
        "            mean_con = delta * self.percent_scale\n",
        "            var_con = dense(l_a2, self.n_cont, kernel_initializer=w)\n",
        "\n",
        "        logits_dict = {}\n",
        "        with tf.compat.v1.variable_scope('actor_discrete'):\n",
        "            l_d = dense(self.s, 512, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "            l_d2 = dense(l_d, 256, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "            for name, n in self.action_discrete:\n",
        "                clean_name = ''.join(c if c.isalnum() else '_' for c in name)\n",
        "                logits_dict[name] = dense(l_d2, n, kernel_initializer=w, name=clean_name)\n",
        "\n",
        "        with tf.compat.v1.variable_scope('critic'):\n",
        "            l_c = dense(self.s, 512, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "            l_c2 = dense(l_c, 256, activation=tf.nn.leaky_relu, kernel_initializer=w)\n",
        "            v = dense(l_c2, 1, kernel_initializer=w)\n",
        "\n",
        "        a_cont_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=tf.compat.v1.get_variable_scope().name + '/actor_continuous')\n",
        "        a_dis_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=tf.compat.v1.get_variable_scope().name + '/actor_discrete')\n",
        "        c_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=tf.compat.v1.get_variable_scope().name + '/critic')\n",
        "\n",
        "        return mean_con, delta, log_var, var_con, v, logits_dict, a_cont_params, a_dis_params, c_params"
      ],
      "metadata": {
        "id": "VxCviZRXiQGD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(states, actions_cont, actions_dis, rewards, next_states, batch_size=64):\n",
        "    n = len(states)\n",
        "    indices = np.arange(n)\n",
        "    np.random.shuffle(indices)\n",
        "    for start in range(0, n, batch_size):\n",
        "        end = start + batch_size\n",
        "        batch_idx = indices[start:end]\n",
        "        batch = {\n",
        "            \"states\": states[batch_idx],\n",
        "            \"actions_cont\": actions_cont[batch_idx],\n",
        "            \"actions_dis\": {k: v[batch_idx] for k, v in actions_dis.items()},\n",
        "            \"rewards\": rewards[batch_idx],\n",
        "            \"next_states\": next_states[batch_idx]\n",
        "        }\n",
        "        yield batch\n"
      ],
      "metadata": {
        "id": "wUmwsr6b5J86"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ],
      "metadata": {
        "id": "svQa_SIl3lh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "ALL_STATE_COLS = [\n",
        "    'state_Air Pollution Index', 'state_Carbon Dioxide Emissions Per Capita', 'state_Employment Rate', 'state_Energy Efficiency',\n",
        "    'state_Flood defense infrastructure', 'state_GDP','state_Green Business Investments','state_Humidity','state_Precipitation',\n",
        "    'state_Population','state_Single-use plastics bans','state_Sustainable Land-Use Zoning','state_Temperature','state_Urban Green Space Expansion',\n",
        "    'state_Waste Management Efficiency','state_budget', 'state_capacity', 'state_carbon_intensity', 'state_class_population',\n",
        "    'state_class_wealth', 'state_economic_growth_priority', 'state_efficiency', 'state_emissions', 'state_incentive_intensity', 'state_investment_ceiling',\n",
        "    'state_investment_floor', 'state_net_carbon_intensity', 'state_net_emissions', 'state_penalty_severity', 'state_production', 'state_production_level',\n",
        "    'state_revenue', 'state_valuation', 'state_sustainability_index', 'state_tax_capacity', 'state_water_consumption', 'state_waste_creation', 'state_waste_recycling'\n",
        "]\n",
        "\n",
        "POLICY_STATE_FEATURES = [\n",
        "    'state_Urban Green Space Expansion',\n",
        "    'state_Flood defense infrastructure',\n",
        "    'state_Heatwave resilience',\n",
        "    'state_Sustainable Land-Use Zoning',\n",
        "    'state_Single-use plastics bans',\n",
        "    'state_Green Business Investments'\n",
        "]\n",
        "\n",
        "def preprocess_batch(df, state_cols, cont_actions, dis_actions):\n",
        "    MAX_STATE_NO = len(state_cols)\n",
        "    states = np.zeros((len(df), MAX_STATE_NO), dtype=np.float32)\n",
        "    for i, col in enumerate(state_cols):\n",
        "        if col in df.columns:\n",
        "            states[:, i] = np.clip(df[col].values.astype(np.float64), -1e6, 1e6).astype(np.float32)\n",
        "\n",
        "    n_cont = len(cont_actions)\n",
        "    actions_cont = np.zeros((len(df), n_cont), dtype=np.float32)\n",
        "    for i, name in enumerate(cont_actions):\n",
        "        col_name = f\"action_{name}\"\n",
        "        if col_name in df.columns:\n",
        "            actions_cont[:, i] = df[col_name].values.astype(np.float32)\n",
        "        else:\n",
        "            actions_cont[:, i] = 0.0\n",
        "\n",
        "    actions_dis = {}\n",
        "    for item in dis_actions:\n",
        "        name = item[0] if isinstance(item, tuple) else item\n",
        "        col_name = name if name.startswith(\"action_\") else f\"action_{name}\"\n",
        "        if col_name in df.columns:\n",
        "            actions_dis[name] = df[col_name].values.astype(np.int32)\n",
        "        else:\n",
        "            actions_dis[name] = np.zeros(len(df), dtype=np.int32)\n",
        "\n",
        "    rewards = df[\"reward\"].values.astype(np.float32).reshape(-1, 1)\n",
        "    next_states = np.zeros((len(df), MAX_STATE_NO), dtype=np.float32)\n",
        "    for i, col in enumerate(state_cols):\n",
        "        next_col = col.replace(\"state_\", \"next_\")\n",
        "        if next_col in df.columns:\n",
        "            next_states[:, i] = df[next_col].values.astype(np.float32)\n",
        "\n",
        "    return states, actions_cont, actions_dis, rewards, next_states\n",
        "\n",
        "def compute_td_target(a3c_model, next_states, rewards, gamma=0.99):\n",
        "    v_next = a3c_model.sess.run(a3c_model.v, feed_dict={a3c_model.s: next_states})\n",
        "    v_next = np.clip(v_next, -1e6, 1e6)\n",
        "    td_target = rewards + gamma * v_next\n",
        "    return td_target\n",
        "\n",
        "for agent_class, df in data.items():\n",
        "    state_scaler = StandardScaler()\n",
        "    action_scaler = StandardScaler()\n",
        "\n",
        "    summary = []\n",
        "    print(agent_class, \"Report\")\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "    state_cols = [col for col in train_df.columns if col.startswith(\"state_\") and col not in POLICY_STATE_FEATURES]\n",
        "    dis_actions = [\n",
        "        ('action_Fossil Fuel Phase-Out Regulations', 3),\n",
        "        ('action_Fuel Economy Standards', 4),\n",
        "        ('state_Single-use plastics bans', 3),\n",
        "        ('action_Vehicle emission standards', 4)\n",
        "    ]\n",
        "    cont_actions = [col for col in train_df.columns if col.startswith(\"action_\") and col not in [d[0] for d in dis_actions]]\n",
        "    for col in train_df.columns:\n",
        "      col_max = train_df[train_df[col] < np.inf][col].max()\n",
        "      train_df.replace({col: np.inf}, col_max, inplace=True)\n",
        "      test_df.replace({col: np.inf}, col_max, inplace=True)\n",
        "\n",
        "    dataset_means = train_df[cont_actions].mean().values.astype(np.float32)\n",
        "    dataset_stds = train_df[cont_actions].std().values.astype(np.float32)\n",
        "\n",
        "    large_vals = df.columns[df.max() >= 1e3].tolist()\n",
        "    log_cols = []\n",
        "    for col in large_vals:\n",
        "        if col.startswith(\"state_\") or col.startswith(\"next_\") or col.startswith(\"action_\"):\n",
        "            train_df[col] = np.log1p(train_df[col].clip(0, 1e6))\n",
        "            test_df[col] = np.log1p(test_df[col].clip(0, 1e6))\n",
        "            log_cols.append(col)\n",
        "\n",
        "    large_scale = []\n",
        "    for i, action_name in enumerate(cont_actions):\n",
        "      median = np.median(train_df[action_name])\n",
        "      if median > 1e4:\n",
        "          large_scale.append(action_name)\n",
        "\n",
        "    train_df[state_cols] = state_scaler.fit_transform(train_df[state_cols])\n",
        "    test_df[state_cols] = state_scaler.transform(test_df[state_cols])\n",
        "    train_df[cont_actions] = action_scaler.fit_transform(train_df[cont_actions])\n",
        "    test_df[cont_actions] = action_scaler.transform(test_df[cont_actions])\n",
        "\n",
        "    joblib.dump(state_scaler, f\"{agent_class}_state_scaler.joblib\")\n",
        "    joblib.dump(action_scaler, f\"{agent_class}_action_scaler.joblib\")\n",
        "\n",
        "    states, actions_cont, actions_dis, rewards, next_states = preprocess_batch(train_df, state_cols, cont_actions, dis_actions)\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess = tf.compat.v1.Session()\n",
        "\n",
        "    action_continuous = []\n",
        "    for name in cont_actions:\n",
        "        col_min = train_df[name].min()\n",
        "        col_max = train_df[name].max()\n",
        "        if col_min == col_max:\n",
        "            col_min -= 1.0\n",
        "            col_max += 1.0\n",
        "        action_continuous.append((name, col_min, col_max))\n",
        "\n",
        "    global_a3c = A3C(scope=\"TrainNet\", session=sess, MAX_STATE_NO=len(state_cols), action_continuous=action_continuous, action_discrete=dis_actions)\n",
        "    worker = A3C(scope=\"WorkerNet\", session=sess, MAX_STATE_NO=len(state_cols), action_continuous=action_continuous, action_discrete=dis_actions, global_net=global_a3c)\n",
        "    global_target_net = A3C(scope=\"TargetNet\", session=sess, MAX_STATE_NO=len(state_cols), action_continuous=action_continuous, action_discrete=dis_actions)\n",
        "\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    num_epochs = 50\n",
        "    batch_size = 512\n",
        "    total_steps = 0\n",
        "    best_v_mean = -np.inf\n",
        "    patience = 5\n",
        "    wait = 0\n",
        "    min_delta = 0.0005\n",
        "\n",
        "    target_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=\"TargetNet\")\n",
        "    global_params = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=\"Global_Net\")\n",
        "    sync_target_op = [t.assign(g) for t, g in zip(target_params, global_params)]\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        idx = np.arange(len(states))\n",
        "        np.random.shuffle(idx)\n",
        "        states_shuffled = states[idx]\n",
        "        actions_cont_shuffled = actions_cont[idx]\n",
        "        actions_dis_shuffled = {name: val[idx] for name, val in actions_dis.items()}\n",
        "        rewards_shuffled = rewards[idx]\n",
        "        next_states_shuffled = next_states[idx]\n",
        "\n",
        "        sess.run(sync_target_op)\n",
        "        for start in range(0, len(states), batch_size):\n",
        "            total_steps += 1\n",
        "            end = start + batch_size\n",
        "            batch = {\n",
        "                \"states\": states_shuffled[start:end],\n",
        "                \"actions_cont\": actions_cont_shuffled[start:end],\n",
        "                \"actions_dis\": {name: val[start:end] for name, val in actions_dis_shuffled.items()},\n",
        "                \"rewards\": rewards_shuffled[start:end],\n",
        "                \"next_states\": next_states_shuffled[start:end],\n",
        "            }\n",
        "            sess.run(worker.pull_op)\n",
        "\n",
        "            current_batch_size = len(batch[\"states\"])\n",
        "            dynamic_mean = np.tile(dataset_means, (current_batch_size, 1))\n",
        "            dynamic_std = np.tile(dataset_stds, (current_batch_size, 1))\n",
        "\n",
        "            old_lp_cont, old_lp_dis = sess.run(\n",
        "                [worker.log_prob_tensor, worker.discrete_log_prob_tensor],\n",
        "                feed_dict={\n",
        "                    worker.s: batch[\"states\"],\n",
        "                    worker.a_cont: batch[\"actions_cont\"],\n",
        "                    worker.current_action: batch[\"actions_cont\"],\n",
        "                    worker.dataset_std: dynamic_std,\n",
        "                    worker.dataset_mean: dynamic_mean,\n",
        "                    **{worker.a_dis[name]: batch[\"actions_dis\"][name] for name in worker.discrete_names}\n",
        "                }\n",
        "            )\n",
        "\n",
        "            reward_std = np.std(batch[\"rewards\"])\n",
        "            if reward_std < 1e-6:\n",
        "                batch_rewards_scaled = batch[\"rewards\"] - np.mean(batch[\"rewards\"])\n",
        "            else:\n",
        "                batch_rewards_scaled = (batch[\"rewards\"] - np.mean(batch[\"rewards\"])) / reward_std\n",
        "\n",
        "            td_target = compute_td_target(global_target_net, batch[\"next_states\"], batch_rewards_scaled, gamma=0.8)\n",
        "\n",
        "            if total_steps % (150 + np.random.randint(0, 100)) == 0:\n",
        "                global_a3c.sess.run(global_a3c.pull_op)\n",
        "\n",
        "            feed_dict = {\n",
        "                worker.s: batch[\"states\"],\n",
        "                worker.a_cont: batch[\"actions_cont\"],\n",
        "                worker.current_action: batch[\"actions_cont\"],\n",
        "                worker.v_target: td_target,\n",
        "                worker.old_log_prob: old_lp_cont,\n",
        "                worker.old_log_prob_d: old_lp_dis,\n",
        "                worker.dataset_mean: dynamic_mean,\n",
        "                worker.dataset_std: dynamic_std,\n",
        "            }\n",
        "            for name in worker.discrete_names:\n",
        "                feed_dict[worker.a_dis[name]] = batch[\"actions_dis\"][name]\n",
        "\n",
        "            critic_loss, actor_loss_c, actor_loss_d, _ = sess.run(\n",
        "                [worker.critic_loss, worker.actor_loss_c, worker.actor_loss_d, worker.train_op],\n",
        "                feed_dict\n",
        "            )\n",
        "\n",
        "        print(f\"Critic: {critic_loss:.4f} Actor_cont: {actor_loss_c:.4f} Actor_dis: {actor_loss_d:.4f}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} completed\\n\")\n",
        "\n",
        "        test_states = test_df[state_cols].values\n",
        "        v_preds = sess.run(global_a3c.v, {global_a3c.s: test_states})\n",
        "        current_v_avg = np.mean(v_preds)\n",
        "        v_change = abs(current_v_avg - best_v_mean)\n",
        "\n",
        "        if v_change > min_delta:\n",
        "            best_v_mean = current_v_avg\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            print(f\"--- Early Stopping Watch: {wait}/{patience} (Delta: {v_change:.5f}) ---\")\n",
        "        if wait >= patience: break\n",
        "\n",
        "    global_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=\"TrainNet\")\n",
        "    saver = tf.compat.v1.train.Saver(var_list=global_vars)\n",
        "    model_path = saver.save(sess, f\"./models/{agent_class}_policy_model.ckpt\")\n",
        "\n",
        "    cont_low = np.array([x[1] for x in action_continuous], dtype=np.float32)\n",
        "    cont_high = np.array([x[2] for x in action_continuous], dtype=np.float32)\n",
        "    test_actions_baseline = test_df[cont_actions].values.astype(np.float32)\n",
        "    test_batch_size = len(test_df)\n",
        "    test_dynamic_std = np.tile(dataset_stds, (test_batch_size, 1))\n",
        "    test_dynamic_mean = np.tile(dataset_means, (test_batch_size, 1))\n",
        "\n",
        "    predicted_means, predicted_deltas, log_vars = global_a3c.sess.run(\n",
        "        [global_a3c.mean, global_a3c.delta, global_a3c.log_var],\n",
        "        feed_dict={\n",
        "            global_a3c.s: test_states,\n",
        "            global_a3c.current_action: test_actions_baseline,\n",
        "            global_a3c.dataset_std: test_dynamic_std,\n",
        "            global_a3c.dataset_mean: test_dynamic_mean\n",
        "        }\n",
        "    )\n",
        "    deltas = (predicted_means / test_actions_baseline) - 1\n",
        "    a_pred_unscaled = action_scaler.inverse_transform(predicted_means)\n",
        "    y_true_unscaled_all = action_scaler.inverse_transform(test_actions_baseline)\n",
        "\n",
        "    for i in range(a_pred_unscaled.shape[1]):\n",
        "      if np.std(a_pred_unscaled[:, i]) < 1e-6:\n",
        "          mean_val = np.mean(a_pred_unscaled[:, i])\n",
        "          noise = np.random.normal(0, 1e-4 * (mean_val + 1e-6), size=a_pred_unscaled[:, i].shape)\n",
        "          a_pred_unscaled[:, i] += noise\n",
        "\n",
        "    a_pred_real = np.zeros_like(a_pred_unscaled)\n",
        "    y_true_real = np.zeros_like(y_true_unscaled_all)\n",
        "    threshold = 1e4\n",
        "\n",
        "    for i, action_name in enumerate(cont_actions):\n",
        "\n",
        "        test_rewards = test_df[\"reward\"].values.flatten()\n",
        "        delta = predicted_means[:, i].astype(np.float64)\n",
        "\n",
        "        y_true = test_df[action_name].values.astype(np.float64)\n",
        "        is_large = np.median(np.expm1(y_true)) > threshold\n",
        "        is_log = action_name in log_cols\n",
        "\n",
        "        if action_name in log_cols:\n",
        "            a_pred_real[:, i] = np.expm1(a_pred_unscaled[:, i])\n",
        "            y_true_real[:, i] = np.expm1(y_true_unscaled_all[:, i])\n",
        "        else:\n",
        "            a_pred_real[:, i] = a_pred_unscaled[:, i]\n",
        "            y_true_real[:, i] = y_true_unscaled_all[:, i]\n",
        "\n",
        "\n",
        "        y_true = y_true_real[:, i]\n",
        "        y_pred = a_pred_real[:, i]\n",
        "        diff = y_pred - y_true\n",
        "        test_rewards = test_df[\"reward\"].values.flatten()\n",
        "\n",
        "        p_std = np.std(y_pred)\n",
        "        d_std = np.std(y_true)\n",
        "        r_std = np.std(test_rewards)\n",
        "\n",
        "        action_corr = np.corrcoef(a_pred_unscaled[:, i], y_true_unscaled_all[:, i])[0, 1]\n",
        "        reward_corr = np.corrcoef(a_pred_unscaled[:, i], test_rewards)[0, 1]\n",
        "        num_changed = np.sum(np.abs(diff) > (1e-4 * np.abs(np.mean(y_true) + 1e-8)))\n",
        "        if p_std < 1e-7:\n",
        "          vol_display = \"Optimal\"\n",
        "          corr_display = \"Converged\"\n",
        "        else:\n",
        "            vol_display = f\"{p_std:.4f}\"\n",
        "            corr_display = f\"{reward_corr:.4f}\"\n",
        "\n",
        "        print(action_name)\n",
        "        print(f\"Mean Recommendation: {np.mean(y_pred):.4f}\")\n",
        "        print(f\"Avg Change from Baseline: {np.mean(diff):+.4f}\")\n",
        "        print(f\"MAE: {np.mean(np.abs(diff)):.4f}\")\n",
        "        print(f\"Modified Rows: {num_changed}/{len(diff)}\")\n",
        "        print(f\"Policy Volatility: {vol_display}\")\n",
        "        print(f\"Correlation to Reward: {corr_display}\")\n",
        "        print(f\"Model Confidence (LogVar): {np.mean(log_vars[:, i]):.4f}\\n\")\n",
        "\n",
        "        summary.append({\n",
        "            \"Agent\": agent_class,\n",
        "            \"Action\": action_name,\n",
        "            \"Mean_Rec\": np.mean(y_pred),\n",
        "            \"Avg_Change\": np.mean(diff),\n",
        "            \"MAE\": np.mean(np.abs(diff)),\n",
        "            \"Volatility\": vol_display,\n",
        "            \"Reward_Corr\": corr_display,\n",
        "            \"Confidence\": np.mean(log_vars[:, i])\n",
        "        })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary)\n",
        "        summary_df.to_csv(f\"{agent_class}_results.csv\", index=False)\n",
        "\n",
        "    sample_df = test_df.sample(frac=0.05)\n",
        "    state_cols = [c for c in sample_df.columns if 'state_' in c and sample_df[c].var() > 1e-8]\n",
        "    action_cols = [a for a in sample_df.columns if 'action_' in a and sample_df[a].var() > 1e-8]\n",
        "\n",
        "    corr_matrix = sample_df[state_cols + action_cols].corr()\n",
        "\n",
        "    signature_data = []\n",
        "\n",
        "    for s_col in state_cols:\n",
        "        for a_col in action_cols:\n",
        "            corr_val = corr_matrix.loc[s_col, a_col]\n",
        "            if abs(corr_val) > 0.005:\n",
        "                direction = \"INCREASES\" if corr_val > 0 else \"DECREASES\"\n",
        "\n",
        "                signature_data.append({\n",
        "                    \"Agent\": agent_class,\n",
        "                    \"State_Variable\": s_col,\n",
        "                    \"Action\": a_col,\n",
        "                    \"Impact\": direction,\n",
        "                    \"Correlation_r\": round(corr_val, 4)\n",
        "                })\n",
        "\n",
        "    signature_df = pd.DataFrame(signature_data)\n",
        "    signature_df.to_csv(f\"{agent_class}_policy_signature.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "QHlck4IdFgw9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65330f56-a839-4cf8-f3b7-ecd01a4f5cc6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sustainability Report\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-4083775122.py:58: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Critic: 0.5433 Actor_cont: -1.5646 Actor_dis: 0.2374\n",
            "Epoch 1/50 completed\n",
            "\n",
            "Critic: 0.2306 Actor_cont: -2.0355 Actor_dis: -0.0209\n",
            "Epoch 2/50 completed\n",
            "\n",
            "Critic: 0.1824 Actor_cont: -2.3408 Actor_dis: 0.1297\n",
            "Epoch 3/50 completed\n",
            "\n",
            "Critic: 0.2052 Actor_cont: -2.2819 Actor_dis: 0.2629\n",
            "Epoch 4/50 completed\n",
            "\n",
            "Critic: 0.0974 Actor_cont: -2.8126 Actor_dis: -0.0544\n",
            "Epoch 5/50 completed\n",
            "\n",
            "Critic: 0.0648 Actor_cont: -2.7851 Actor_dis: 0.0526\n",
            "Epoch 6/50 completed\n",
            "\n",
            "Critic: 0.0911 Actor_cont: -2.7501 Actor_dis: 0.0427\n",
            "Epoch 7/50 completed\n",
            "\n",
            "Critic: 0.1038 Actor_cont: -2.7277 Actor_dis: 0.0667\n",
            "Epoch 8/50 completed\n",
            "\n",
            "Critic: 0.0419 Actor_cont: -3.2803 Actor_dis: 0.0153\n",
            "Epoch 9/50 completed\n",
            "\n",
            "Critic: 0.0860 Actor_cont: -3.5812 Actor_dis: -0.0727\n",
            "Epoch 10/50 completed\n",
            "\n",
            "Critic: 0.0780 Actor_cont: -3.4559 Actor_dis: 0.0548\n",
            "Epoch 11/50 completed\n",
            "\n",
            "Critic: 0.1001 Actor_cont: -4.1430 Actor_dis: 0.0320\n",
            "Epoch 12/50 completed\n",
            "\n",
            "Critic: 0.0683 Actor_cont: -4.1774 Actor_dis: -0.0395\n",
            "Epoch 13/50 completed\n",
            "\n",
            "Critic: 0.0746 Actor_cont: -4.1778 Actor_dis: 0.0055\n",
            "Epoch 14/50 completed\n",
            "\n",
            "Critic: 0.0532 Actor_cont: -4.0451 Actor_dis: 0.2108\n",
            "Epoch 15/50 completed\n",
            "\n",
            "Critic: 0.0678 Actor_cont: -4.3135 Actor_dis: -0.0932\n",
            "Epoch 16/50 completed\n",
            "\n",
            "Critic: 0.0565 Actor_cont: -4.1348 Actor_dis: 0.0610\n",
            "Epoch 17/50 completed\n",
            "\n",
            "Critic: 0.0610 Actor_cont: -4.1290 Actor_dis: 0.0943\n",
            "Epoch 18/50 completed\n",
            "\n",
            "Critic: 0.0816 Actor_cont: -4.3942 Actor_dis: -0.2266\n",
            "Epoch 19/50 completed\n",
            "\n",
            "Critic: 0.0345 Actor_cont: -4.2915 Actor_dis: -0.0672\n",
            "Epoch 20/50 completed\n",
            "\n",
            "Critic: 0.0479 Actor_cont: -4.1745 Actor_dis: 0.0997\n",
            "Epoch 21/50 completed\n",
            "\n",
            "Critic: 0.0430 Actor_cont: -4.3505 Actor_dis: 0.0048\n",
            "Epoch 22/50 completed\n",
            "\n",
            "Critic: 0.0430 Actor_cont: -4.4785 Actor_dis: 0.0016\n",
            "Epoch 23/50 completed\n",
            "\n",
            "Critic: 0.0308 Actor_cont: -4.4876 Actor_dis: 0.0195\n",
            "Epoch 24/50 completed\n",
            "\n",
            "Critic: 0.0430 Actor_cont: -4.4752 Actor_dis: -0.0206\n",
            "Epoch 25/50 completed\n",
            "\n",
            "Critic: 0.0459 Actor_cont: -4.4613 Actor_dis: 0.0001\n",
            "Epoch 26/50 completed\n",
            "\n",
            "Critic: 0.0493 Actor_cont: -4.4805 Actor_dis: -0.0678\n",
            "Epoch 27/50 completed\n",
            "\n",
            "Critic: 0.0644 Actor_cont: -4.6729 Actor_dis: -0.2620\n",
            "Epoch 28/50 completed\n",
            "\n",
            "Critic: 0.0290 Actor_cont: -4.4736 Actor_dis: 0.0006\n",
            "Epoch 29/50 completed\n",
            "\n",
            "Critic: 0.0574 Actor_cont: -4.3421 Actor_dis: 0.1224\n",
            "Epoch 30/50 completed\n",
            "\n",
            "Critic: 0.0225 Actor_cont: -4.5461 Actor_dis: -0.0202\n",
            "Epoch 31/50 completed\n",
            "\n",
            "Critic: 0.0800 Actor_cont: -4.6024 Actor_dis: -0.1001\n",
            "Epoch 32/50 completed\n",
            "\n",
            "Critic: 0.0435 Actor_cont: -4.7097 Actor_dis: -0.1672\n",
            "Epoch 33/50 completed\n",
            "\n",
            "Critic: 0.0362 Actor_cont: -4.3016 Actor_dis: 0.1926\n",
            "Epoch 34/50 completed\n",
            "\n",
            "Critic: 0.0554 Actor_cont: -4.5866 Actor_dis: -0.0867\n",
            "Epoch 35/50 completed\n",
            "\n",
            "Critic: 0.0255 Actor_cont: -4.3626 Actor_dis: 0.1405\n",
            "Epoch 36/50 completed\n",
            "\n",
            "Critic: 0.0265 Actor_cont: -4.5652 Actor_dis: -0.0595\n",
            "Epoch 37/50 completed\n",
            "\n",
            "Critic: 0.0523 Actor_cont: -4.3792 Actor_dis: 0.1205\n",
            "Epoch 38/50 completed\n",
            "\n",
            "Critic: 0.0449 Actor_cont: -4.4663 Actor_dis: 0.0547\n",
            "Epoch 39/50 completed\n",
            "\n",
            "Critic: 0.0350 Actor_cont: -4.5200 Actor_dis: 0.0123\n",
            "Epoch 40/50 completed\n",
            "\n",
            "Critic: 0.0360 Actor_cont: -4.4243 Actor_dis: 0.0936\n",
            "Epoch 41/50 completed\n",
            "\n",
            "Critic: 0.0422 Actor_cont: -4.4085 Actor_dis: 0.1388\n",
            "Epoch 42/50 completed\n",
            "\n",
            "Critic: 0.0353 Actor_cont: -4.4784 Actor_dis: 0.0608\n",
            "Epoch 43/50 completed\n",
            "\n",
            "Critic: 0.0489 Actor_cont: -4.4534 Actor_dis: 0.0467\n",
            "Epoch 44/50 completed\n",
            "\n",
            "Critic: 0.0345 Actor_cont: -4.4451 Actor_dis: 0.0721\n",
            "Epoch 45/50 completed\n",
            "\n",
            "Critic: 0.0154 Actor_cont: -4.5174 Actor_dis: 0.0043\n",
            "Epoch 46/50 completed\n",
            "\n",
            "Critic: 0.0614 Actor_cont: -4.3399 Actor_dis: 0.2054\n",
            "Epoch 47/50 completed\n",
            "\n",
            "Critic: 0.0767 Actor_cont: -4.4832 Actor_dis: 0.0736\n",
            "Epoch 48/50 completed\n",
            "\n",
            "Critic: 0.0572 Actor_cont: -4.6453 Actor_dis: -0.1682\n",
            "Epoch 49/50 completed\n",
            "\n",
            "Critic: 0.0295 Actor_cont: -4.5751 Actor_dis: -0.0580\n",
            "Epoch 50/50 completed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_Carbon Tax\n",
            "Mean Recommendation: 237.1055\n",
            "Avg Change from Baseline: +139.0440\n",
            "MAE: 139.0440\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 0.0001\n",
            "Correlation to Reward: nan\n",
            "Model Confidence (LogVar): 0.8881\n",
            "\n",
            "action_Climate-Resilient Infrastructure Investment\n",
            "Mean Recommendation: 1040508.1250\n",
            "Avg Change from Baseline: +40507.9883\n",
            "MAE: 1098040.5000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1521394.6250\n",
            "Correlation to Reward: -0.2658\n",
            "Model Confidence (LogVar): 0.9991\n",
            "\n",
            "action_Electric Vehicle (EV) Subsidies\n",
            "Mean Recommendation: 0.2978\n",
            "Avg Change from Baseline: -0.1948\n",
            "MAE: 0.2588\n",
            "Modified Rows: 199944/200000\n",
            "Policy Volatility: 0.1115\n",
            "Correlation to Reward: -0.5294\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Energy Efficiency Incentives\n",
            "Mean Recommendation: 0.3605\n",
            "Avg Change from Baseline: -0.1239\n",
            "MAE: 0.2542\n",
            "Modified Rows: 199980/200000\n",
            "Policy Volatility: 0.1718\n",
            "Correlation to Reward: -0.2540\n",
            "Model Confidence (LogVar): 0.9953\n",
            "\n",
            "action_Flood defense infrastructure\n",
            "Mean Recommendation: 779095.7500\n",
            "Avg Change from Baseline: -220904.2969\n",
            "MAE: 916964.3750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1233924.7500\n",
            "Correlation to Reward: -0.5442\n",
            "Model Confidence (LogVar): 0.9209\n",
            "\n",
            "action_Green Business Investments\n",
            "Mean Recommendation: 851375.5625\n",
            "Avg Change from Baseline: -148624.6250\n",
            "MAE: 967031.2500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1324857.0000\n",
            "Correlation to Reward: -0.4037\n",
            "Model Confidence (LogVar): 0.9053\n",
            "\n",
            "action_Heatwave resilience\n",
            "Mean Recommendation: 2431144.5000\n",
            "Avg Change from Baseline: +1431144.3750\n",
            "MAE: 2061306.8750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 2056894.6250\n",
            "Correlation to Reward: 0.4565\n",
            "Model Confidence (LogVar): 0.9632\n",
            "\n",
            "action_Public transport expansion\n",
            "Mean Recommendation: 1142037.0000\n",
            "Avg Change from Baseline: +142036.9375\n",
            "MAE: 1168368.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1607924.6250\n",
            "Correlation to Reward: -0.4782\n",
            "Model Confidence (LogVar): 0.9526\n",
            "\n",
            "action_Recycling Rate\n",
            "Mean Recommendation: 0.6773\n",
            "Avg Change from Baseline: +0.1880\n",
            "MAE: 0.2583\n",
            "Modified Rows: 199984/200000\n",
            "Policy Volatility: 0.1133\n",
            "Correlation to Reward: 0.5208\n",
            "Model Confidence (LogVar): 0.9967\n",
            "\n",
            "action_Renewable energy subsidies\n",
            "Mean Recommendation: 0.2963\n",
            "Avg Change from Baseline: -0.1950\n",
            "MAE: 0.2578\n",
            "Modified Rows: 199935/200000\n",
            "Policy Volatility: 0.1134\n",
            "Correlation to Reward: -0.5299\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Single-use plastics bans\n",
            "Mean Recommendation: -0.0000\n",
            "Avg Change from Baseline: -1.0076\n",
            "MAE: 1.0076\n",
            "Modified Rows: 133055/200000\n",
            "Policy Volatility: Optimal\n",
            "Correlation to Reward: Converged\n",
            "Model Confidence (LogVar): 0.8955\n",
            "\n",
            "action_Sustainable Land-Use Zoning\n",
            "Mean Recommendation: 815338.4375\n",
            "Avg Change from Baseline: -184661.6250\n",
            "MAE: 942069.1250\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1280837.7500\n",
            "Correlation to Reward: -0.3913\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Urban Green Space Expansion\n",
            "Mean Recommendation: 5.1074\n",
            "Avg Change from Baseline: -96.5847\n",
            "MAE: 96.5847\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 0.0000\n",
            "Correlation to Reward: nan\n",
            "Model Confidence (LogVar): 0.7904\n",
            "\n",
            "action_Waste Management Reforms\n",
            "Mean Recommendation: 3549769.5000\n",
            "Avg Change from Baseline: +2549769.2500\n",
            "MAE: 2836165.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1721997.7500\n",
            "Correlation to Reward: 0.3846\n",
            "Model Confidence (LogVar): 0.9888\n",
            "\n",
            "action_Water Consumption Tax\n",
            "Mean Recommendation: 6.5163\n",
            "Avg Change from Baseline: +3.4801\n",
            "MAE: 4.0236\n",
            "Modified Rows: 199999/200000\n",
            "Policy Volatility: 2.0732\n",
            "Correlation to Reward: 0.2875\n",
            "Model Confidence (LogVar): 0.9605\n",
            "\n",
            "action_Water conservation measures\n",
            "Mean Recommendation: 2306020.0000\n",
            "Avg Change from Baseline: +1306020.0000\n",
            "MAE: 1974639.8750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 2053475.3750\n",
            "Correlation to Reward: -0.1645\n",
            "Model Confidence (LogVar): 0.9225\n",
            "\n",
            "DecisionAgents Report\n",
            "Critic: 23.3141 Actor_cont: 21.7812 Actor_dis: 23.5425\n",
            "Epoch 1/50 completed\n",
            "\n",
            "Critic: 12.1362 Actor_cont: 4.2828 Actor_dis: 6.0285\n",
            "Epoch 2/50 completed\n",
            "\n",
            "Critic: 5.1695 Actor_cont: 0.8344 Actor_dis: 2.5871\n",
            "Epoch 3/50 completed\n",
            "\n",
            "Critic: 3.9648 Actor_cont: -0.1493 Actor_dis: 1.6107\n",
            "Epoch 4/50 completed\n",
            "\n",
            "Critic: 3.4638 Actor_cont: -1.5846 Actor_dis: 0.1818\n",
            "Epoch 5/50 completed\n",
            "\n",
            "Critic: 4.2256 Actor_cont: -1.0375 Actor_dis: 0.7177\n",
            "Epoch 6/50 completed\n",
            "\n",
            "Critic: 3.1316 Actor_cont: -1.8490 Actor_dis: -0.0938\n",
            "Epoch 7/50 completed\n",
            "\n",
            "Critic: 3.9171 Actor_cont: -0.2602 Actor_dis: 1.5028\n",
            "Epoch 8/50 completed\n",
            "\n",
            "Critic: 2.1680 Actor_cont: -0.7484 Actor_dis: 1.0171\n",
            "Epoch 9/50 completed\n",
            "\n",
            "Critic: 1.9516 Actor_cont: -2.2831 Actor_dis: -0.5252\n",
            "Epoch 10/50 completed\n",
            "\n",
            "Critic: 2.4558 Actor_cont: -1.6988 Actor_dis: 0.0587\n",
            "Epoch 11/50 completed\n",
            "\n",
            "Critic: 3.0652 Actor_cont: 0.8137 Actor_dis: 2.5723\n",
            "Epoch 12/50 completed\n",
            "\n",
            "Critic: 1.0052 Actor_cont: -1.9157 Actor_dis: -0.1706\n",
            "Epoch 13/50 completed\n",
            "\n",
            "Critic: 3.9899 Actor_cont: 0.4281 Actor_dis: 2.1799\n",
            "Epoch 14/50 completed\n",
            "\n",
            "Critic: 3.7393 Actor_cont: 0.5756 Actor_dis: 2.3215\n",
            "Epoch 15/50 completed\n",
            "\n",
            "Critic: 1.6191 Actor_cont: -1.7855 Actor_dis: -0.0380\n",
            "Epoch 16/50 completed\n",
            "\n",
            "Critic: 1.3331 Actor_cont: -0.7835 Actor_dis: 0.9408\n",
            "Epoch 17/50 completed\n",
            "\n",
            "Critic: 1.3492 Actor_cont: -1.7683 Actor_dis: -0.0173\n",
            "Epoch 18/50 completed\n",
            "\n",
            "Critic: 1.9862 Actor_cont: -1.5806 Actor_dis: 0.1622\n",
            "Epoch 19/50 completed\n",
            "\n",
            "Critic: 2.3521 Actor_cont: 0.0104 Actor_dis: 1.7595\n",
            "Epoch 20/50 completed\n",
            "\n",
            "Critic: 0.9686 Actor_cont: -1.3893 Actor_dis: 0.3446\n",
            "Epoch 21/50 completed\n",
            "\n",
            "Critic: 1.9124 Actor_cont: -0.4100 Actor_dis: 1.3335\n",
            "Epoch 22/50 completed\n",
            "\n",
            "Critic: 0.9690 Actor_cont: -1.5477 Actor_dis: 0.1913\n",
            "Epoch 23/50 completed\n",
            "\n",
            "Critic: 0.7499 Actor_cont: -1.5612 Actor_dis: 0.1931\n",
            "Epoch 24/50 completed\n",
            "\n",
            "Critic: 1.9790 Actor_cont: -1.0812 Actor_dis: 0.6435\n",
            "Epoch 25/50 completed\n",
            "\n",
            "Critic: 1.7329 Actor_cont: -0.2917 Actor_dis: 1.4618\n",
            "Epoch 26/50 completed\n",
            "\n",
            "Critic: 0.7606 Actor_cont: -0.9736 Actor_dis: 0.7730\n",
            "Epoch 27/50 completed\n",
            "\n",
            "Critic: 0.6426 Actor_cont: -1.1579 Actor_dis: 0.5803\n",
            "Epoch 28/50 completed\n",
            "\n",
            "Critic: 0.8196 Actor_cont: -1.6395 Actor_dis: 0.1102\n",
            "Epoch 29/50 completed\n",
            "\n",
            "Critic: 1.1532 Actor_cont: -1.3361 Actor_dis: 0.4143\n",
            "Epoch 30/50 completed\n",
            "\n",
            "Critic: 0.5410 Actor_cont: -2.1558 Actor_dis: -0.4040\n",
            "Epoch 31/50 completed\n",
            "\n",
            "Critic: 1.2673 Actor_cont: -1.0629 Actor_dis: 0.6843\n",
            "Epoch 32/50 completed\n",
            "\n",
            "Critic: 0.3156 Actor_cont: -1.7110 Actor_dis: 0.0192\n",
            "Epoch 33/50 completed\n",
            "\n",
            "Critic: 0.4555 Actor_cont: -1.5513 Actor_dis: 0.1918\n",
            "Epoch 34/50 completed\n",
            "\n",
            "Critic: 0.3248 Actor_cont: -1.2812 Actor_dis: 0.4672\n",
            "Epoch 35/50 completed\n",
            "\n",
            "Critic: 0.4906 Actor_cont: -1.3030 Actor_dis: 0.4448\n",
            "Epoch 36/50 completed\n",
            "\n",
            "Critic: 0.5683 Actor_cont: -1.6725 Actor_dis: 0.0515\n",
            "Epoch 37/50 completed\n",
            "\n",
            "Critic: 0.5572 Actor_cont: -1.4328 Actor_dis: 0.3175\n",
            "Epoch 38/50 completed\n",
            "\n",
            "Critic: 0.5089 Actor_cont: -1.8791 Actor_dis: -0.1360\n",
            "Epoch 39/50 completed\n",
            "\n",
            "Critic: 0.2137 Actor_cont: -1.9488 Actor_dis: -0.1970\n",
            "Epoch 40/50 completed\n",
            "\n",
            "Critic: 0.4986 Actor_cont: -1.4082 Actor_dis: 0.3346\n",
            "Epoch 41/50 completed\n",
            "\n",
            "Critic: 0.3015 Actor_cont: -1.8420 Actor_dis: -0.0815\n",
            "Epoch 42/50 completed\n",
            "\n",
            "Critic: 1.0562 Actor_cont: -1.2709 Actor_dis: 0.4819\n",
            "Epoch 43/50 completed\n",
            "\n",
            "Critic: 0.1418 Actor_cont: -1.6647 Actor_dis: 0.0826\n",
            "Epoch 44/50 completed\n",
            "\n",
            "Critic: 0.4057 Actor_cont: -1.8020 Actor_dis: -0.0588\n",
            "Epoch 45/50 completed\n",
            "\n",
            "Critic: 0.2630 Actor_cont: -1.6981 Actor_dis: 0.0634\n",
            "Epoch 46/50 completed\n",
            "\n",
            "Critic: 0.4135 Actor_cont: -1.8877 Actor_dis: -0.1281\n",
            "Epoch 47/50 completed\n",
            "\n",
            "Critic: 0.6085 Actor_cont: -1.1450 Actor_dis: 0.6008\n",
            "Epoch 48/50 completed\n",
            "\n",
            "Critic: 0.2313 Actor_cont: -1.8793 Actor_dis: -0.1246\n",
            "Epoch 49/50 completed\n",
            "\n",
            "Critic: 0.7880 Actor_cont: -1.7584 Actor_dis: -0.0031\n",
            "Epoch 50/50 completed\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_Carbon Tax\n",
            "Mean Recommendation: 205.4506\n",
            "Avg Change from Baseline: +99.1630\n",
            "MAE: 99.1630\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0000\n",
            "Correlation to Reward: nan\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Climate-Resilient Infrastructure Investment\n",
            "Mean Recommendation: 4478401.5000\n",
            "Avg Change from Baseline: +3478401.5000\n",
            "MAE: 3479412.7500\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 116309.6719\n",
            "Correlation to Reward: -0.0093\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Electric Vehicle (EV) Subsidies\n",
            "Mean Recommendation: 0.7820\n",
            "Avg Change from Baseline: +0.1737\n",
            "MAE: 0.2005\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0046\n",
            "Correlation to Reward: -0.2343\n",
            "Model Confidence (LogVar): 0.9606\n",
            "\n",
            "action_Energy Efficiency Incentives\n",
            "Mean Recommendation: 0.2841\n",
            "Avg Change from Baseline: -0.2551\n",
            "MAE: 0.2926\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0026\n",
            "Correlation to Reward: 0.1748\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Flood defense infrastructure\n",
            "Mean Recommendation: 4453718.5000\n",
            "Avg Change from Baseline: +3453718.5000\n",
            "MAE: 3462315.2500\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 338078.4062\n",
            "Correlation to Reward: 0.1239\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Green Business Investments\n",
            "Mean Recommendation: 4481555.5000\n",
            "Avg Change from Baseline: +3481555.2500\n",
            "MAE: 3481555.2500\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 6791.7222\n",
            "Correlation to Reward: 0.0011\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Heatwave resilience\n",
            "Mean Recommendation: 4160814.7500\n",
            "Avg Change from Baseline: +3160815.0000\n",
            "MAE: 3259426.0000\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 1103208.0000\n",
            "Correlation to Reward: 0.2735\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Public transport expansion\n",
            "Mean Recommendation: 367880.5000\n",
            "Avg Change from Baseline: -632119.6250\n",
            "MAE: 632119.6250\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 477.2319\n",
            "Correlation to Reward: -0.0008\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Recycling Rate\n",
            "Mean Recommendation: 0.6410\n",
            "Avg Change from Baseline: +0.1325\n",
            "MAE: 0.1421\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0021\n",
            "Correlation to Reward: -0.2023\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Renewable energy subsidies\n",
            "Mean Recommendation: 0.2765\n",
            "Avg Change from Baseline: -0.2281\n",
            "MAE: 0.2631\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0023\n",
            "Correlation to Reward: 0.1781\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Single-use plastics bans\n",
            "Mean Recommendation: 2.4704\n",
            "Avg Change from Baseline: +1.4192\n",
            "MAE: 1.4192\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0003\n",
            "Correlation to Reward: -0.0033\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Sustainable Land-Use Zoning\n",
            "Mean Recommendation: 4481705.5000\n",
            "Avg Change from Baseline: +3481705.2500\n",
            "MAE: 3481705.2500\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 6835.4204\n",
            "Correlation to Reward: -0.0218\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Urban Green Space Expansion\n",
            "Mean Recommendation: 219.9716\n",
            "Avg Change from Baseline: +145.8712\n",
            "MAE: 145.8712\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0000\n",
            "Correlation to Reward: nan\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Waste Management Reforms\n",
            "Mean Recommendation: 389270.7188\n",
            "Avg Change from Baseline: -610729.3750\n",
            "MAE: 646939.0625\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 295879.0312\n",
            "Correlation to Reward: 0.0365\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Water Consumption Tax\n",
            "Mean Recommendation: 0.6745\n",
            "Avg Change from Baseline: -2.7750\n",
            "MAE: 2.7750\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 0.0001\n",
            "Correlation to Reward: 0.0192\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Water conservation measures\n",
            "Mean Recommendation: 367873.6250\n",
            "Avg Change from Baseline: -632126.5000\n",
            "MAE: 632126.5000\n",
            "Modified Rows: 5000/5000\n",
            "Policy Volatility: 472.3831\n",
            "Correlation to Reward: -0.0316\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "BusinessAgents Report\n",
            "Critic: 0.3478 Actor_cont: -3.3801 Actor_dis: -0.2846\n",
            "Epoch 1/50 completed\n",
            "\n",
            "Critic: 0.2078 Actor_cont: -3.7350 Actor_dis: -0.3228\n",
            "Epoch 2/50 completed\n",
            "\n",
            "Critic: 0.1060 Actor_cont: -3.5800 Actor_dis: 0.1127\n",
            "Epoch 3/50 completed\n",
            "\n",
            "Critic: 0.0703 Actor_cont: -3.6147 Actor_dis: 0.1215\n",
            "Epoch 4/50 completed\n",
            "\n",
            "Critic: 0.1080 Actor_cont: -3.6958 Actor_dis: 0.0126\n",
            "Epoch 5/50 completed\n",
            "\n",
            "Critic: 0.0408 Actor_cont: -3.8928 Actor_dis: -0.0060\n",
            "Epoch 6/50 completed\n",
            "\n",
            "Critic: 0.0345 Actor_cont: -4.1610 Actor_dis: -0.1254\n",
            "Epoch 7/50 completed\n",
            "\n",
            "Critic: 0.0343 Actor_cont: -4.0069 Actor_dis: 0.0850\n",
            "Epoch 8/50 completed\n",
            "\n",
            "Critic: 0.0854 Actor_cont: -4.0550 Actor_dis: -0.0671\n",
            "Epoch 9/50 completed\n",
            "\n",
            "Critic: 0.0276 Actor_cont: -4.1251 Actor_dis: -0.0635\n",
            "Epoch 10/50 completed\n",
            "\n",
            "Critic: 0.0461 Actor_cont: -4.0480 Actor_dis: -0.0696\n",
            "Epoch 11/50 completed\n",
            "\n",
            "Critic: 0.0370 Actor_cont: -4.0241 Actor_dis: 0.0878\n",
            "Epoch 12/50 completed\n",
            "\n",
            "Critic: 0.0637 Actor_cont: -3.9703 Actor_dis: 0.1069\n",
            "Epoch 13/50 completed\n",
            "\n",
            "Critic: 0.0332 Actor_cont: -4.0452 Actor_dis: 0.0679\n",
            "Epoch 14/50 completed\n",
            "\n",
            "Critic: 0.0712 Actor_cont: -4.3579 Actor_dis: -0.2427\n",
            "Epoch 15/50 completed\n",
            "\n",
            "Critic: 0.0402 Actor_cont: -4.1023 Actor_dis: 0.0521\n",
            "Epoch 16/50 completed\n",
            "\n",
            "Critic: 0.0394 Actor_cont: -4.2643 Actor_dis: -0.1045\n",
            "Epoch 17/50 completed\n",
            "\n",
            "Critic: 0.0520 Actor_cont: -4.1851 Actor_dis: -0.0574\n",
            "Epoch 18/50 completed\n",
            "\n",
            "Critic: 0.0509 Actor_cont: -4.1356 Actor_dis: -0.0604\n",
            "Epoch 19/50 completed\n",
            "\n",
            "Critic: 0.0522 Actor_cont: -4.2513 Actor_dis: -0.0805\n",
            "Epoch 20/50 completed\n",
            "\n",
            "Critic: 0.0357 Actor_cont: -4.0710 Actor_dis: 0.0883\n",
            "Epoch 21/50 completed\n",
            "\n",
            "Critic: 0.0467 Actor_cont: -4.2016 Actor_dis: 0.0098\n",
            "Epoch 22/50 completed\n",
            "\n",
            "Critic: 0.0403 Actor_cont: -4.2444 Actor_dis: -0.0551\n",
            "Epoch 23/50 completed\n",
            "\n",
            "Critic: 0.0406 Actor_cont: -4.1726 Actor_dis: 0.0116\n",
            "Epoch 24/50 completed\n",
            "\n",
            "Critic: 0.0549 Actor_cont: -4.2971 Actor_dis: -0.1028\n",
            "Epoch 25/50 completed\n",
            "\n",
            "Critic: 0.0386 Actor_cont: -4.2734 Actor_dis: -0.0650\n",
            "Epoch 26/50 completed\n",
            "\n",
            "Critic: 0.0567 Actor_cont: -4.3451 Actor_dis: -0.1241\n",
            "Epoch 27/50 completed\n",
            "\n",
            "Critic: 0.0440 Actor_cont: -4.2877 Actor_dis: -0.0391\n",
            "Epoch 28/50 completed\n",
            "\n",
            "Critic: 0.0219 Actor_cont: -4.2226 Actor_dis: -0.0033\n",
            "Epoch 29/50 completed\n",
            "\n",
            "Critic: 0.0464 Actor_cont: -4.3255 Actor_dis: -0.0902\n",
            "Epoch 30/50 completed\n",
            "\n",
            "Critic: 0.0309 Actor_cont: -4.1336 Actor_dis: 0.0702\n",
            "Epoch 31/50 completed\n",
            "\n",
            "Critic: 0.0430 Actor_cont: -4.1942 Actor_dis: 0.0545\n",
            "Epoch 32/50 completed\n",
            "\n",
            "Critic: 0.0577 Actor_cont: -4.0045 Actor_dis: 0.1849\n",
            "Epoch 33/50 completed\n",
            "\n",
            "Critic: 0.0336 Actor_cont: -4.1992 Actor_dis: 0.0841\n",
            "Epoch 34/50 completed\n",
            "\n",
            "Critic: 0.0352 Actor_cont: -4.0266 Actor_dis: 0.1449\n",
            "Epoch 35/50 completed\n",
            "\n",
            "Critic: 0.0457 Actor_cont: -4.4405 Actor_dis: -0.1863\n",
            "Epoch 36/50 completed\n",
            "\n",
            "Critic: 0.0392 Actor_cont: -4.2129 Actor_dis: 0.0210\n",
            "Epoch 37/50 completed\n",
            "\n",
            "Critic: 0.0609 Actor_cont: -4.0378 Actor_dis: 0.2486\n",
            "Epoch 38/50 completed\n",
            "\n",
            "Critic: 0.0488 Actor_cont: -4.1713 Actor_dis: 0.1378\n",
            "Epoch 39/50 completed\n",
            "\n",
            "Critic: 0.0392 Actor_cont: -4.2455 Actor_dis: 0.0209\n",
            "Epoch 40/50 completed\n",
            "\n",
            "Critic: 0.0257 Actor_cont: -4.2478 Actor_dis: 0.0214\n",
            "Epoch 41/50 completed\n",
            "\n",
            "Critic: 0.0358 Actor_cont: -4.2744 Actor_dis: -0.0543\n",
            "Epoch 42/50 completed\n",
            "\n",
            "Critic: 0.0323 Actor_cont: -4.1607 Actor_dis: 0.1201\n",
            "Epoch 43/50 completed\n",
            "\n",
            "Critic: 0.0549 Actor_cont: -4.1212 Actor_dis: 0.1749\n",
            "Epoch 44/50 completed\n",
            "\n",
            "Critic: 0.0478 Actor_cont: -4.2233 Actor_dis: -0.0029\n",
            "Epoch 45/50 completed\n",
            "\n",
            "Critic: 0.0349 Actor_cont: -4.3117 Actor_dis: -0.1024\n",
            "Epoch 46/50 completed\n",
            "\n",
            "Critic: 0.0468 Actor_cont: -4.2772 Actor_dis: 0.0383\n",
            "Epoch 47/50 completed\n",
            "\n",
            "Critic: 0.0472 Actor_cont: -4.2661 Actor_dis: -0.0618\n",
            "Epoch 48/50 completed\n",
            "\n",
            "Critic: 0.0297 Actor_cont: -4.2694 Actor_dis: 0.0230\n",
            "Epoch 49/50 completed\n",
            "\n",
            "Critic: 0.0252 Actor_cont: -4.2699 Actor_dis: -0.0226\n",
            "Epoch 50/50 completed\n",
            "\n",
            "action_Carbon Tax\n",
            "Mean Recommendation: 59.4922\n",
            "Avg Change from Baseline: -38.5694\n",
            "MAE: 100.5224\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 94.9300\n",
            "Correlation to Reward: -0.5380\n",
            "Model Confidence (LogVar): 0.7521\n",
            "\n",
            "action_Climate-Resilient Infrastructure Investment\n",
            "Mean Recommendation: 3914212.2500\n",
            "Avg Change from Baseline: +2914212.5000\n",
            "MAE: 3088608.2500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1418616.8750\n",
            "Correlation to Reward: 0.5267\n",
            "Model Confidence (LogVar): 0.9940\n",
            "\n",
            "action_Electric Vehicle (EV) Subsidies\n",
            "Mean Recommendation: 0.7091\n",
            "Avg Change from Baseline: +0.2164\n",
            "MAE: 0.2613\n",
            "Modified Rows: 199961/200000\n",
            "Policy Volatility: 0.0697\n",
            "Correlation to Reward: 0.2944\n",
            "Model Confidence (LogVar): 0.9199\n",
            "\n",
            "action_Energy Efficiency Incentives\n",
            "Mean Recommendation: 0.7063\n",
            "Avg Change from Baseline: +0.2218\n",
            "MAE: 0.2733\n",
            "Modified Rows: 199971/200000\n",
            "Policy Volatility: 0.0714\n",
            "Correlation to Reward: 0.2963\n",
            "Model Confidence (LogVar): 0.9233\n",
            "\n",
            "action_Flood defense infrastructure\n",
            "Mean Recommendation: 4242721.5000\n",
            "Avg Change from Baseline: +3242721.0000\n",
            "MAE: 3316160.2500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 962276.8750\n",
            "Correlation to Reward: 0.2206\n",
            "Model Confidence (LogVar): 0.9727\n",
            "\n",
            "action_Green Business Investments\n",
            "Mean Recommendation: 4041988.2500\n",
            "Avg Change from Baseline: +3041987.5000\n",
            "MAE: 3177115.7500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1271032.7500\n",
            "Correlation to Reward: 0.4962\n",
            "Model Confidence (LogVar): 0.9777\n",
            "\n",
            "action_Heatwave resilience\n",
            "Mean Recommendation: 676497.4375\n",
            "Avg Change from Baseline: -323502.8438\n",
            "MAE: 845896.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1083675.7500\n",
            "Correlation to Reward: -0.2650\n",
            "Model Confidence (LogVar): 0.9862\n",
            "\n",
            "action_Public transport expansion\n",
            "Mean Recommendation: 426809.3438\n",
            "Avg Change from Baseline: -573190.8125\n",
            "MAE: 672941.1875\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 488831.1250\n",
            "Correlation to Reward: -0.2383\n",
            "Model Confidence (LogVar): 0.9756\n",
            "\n",
            "action_Recycling Rate\n",
            "Mean Recommendation: 0.2763\n",
            "Avg Change from Baseline: -0.2129\n",
            "MAE: 0.2546\n",
            "Modified Rows: 199984/200000\n",
            "Policy Volatility: 0.0688\n",
            "Correlation to Reward: -0.2946\n",
            "Model Confidence (LogVar): 0.9174\n",
            "\n",
            "action_Renewable energy subsidies\n",
            "Mean Recommendation: 0.7085\n",
            "Avg Change from Baseline: +0.2172\n",
            "MAE: 0.2627\n",
            "Modified Rows: 199928/200000\n",
            "Policy Volatility: 0.0697\n",
            "Correlation to Reward: 0.2981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Confidence (LogVar): 0.9197\n",
            "\n",
            "action_Single-use plastics bans\n",
            "Mean Recommendation: 1.1464\n",
            "Avg Change from Baseline: +0.1388\n",
            "MAE: 1.2039\n",
            "Modified Rows: 166508/200000\n",
            "Policy Volatility: 1.2102\n",
            "Correlation to Reward: 0.2372\n",
            "Model Confidence (LogVar): 0.8443\n",
            "\n",
            "action_Sustainable Land-Use Zoning\n",
            "Mean Recommendation: 4096557.7500\n",
            "Avg Change from Baseline: +3096557.0000\n",
            "MAE: 3214915.2500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1198351.3750\n",
            "Correlation to Reward: 0.2668\n",
            "Model Confidence (LogVar): 0.9844\n",
            "\n",
            "action_Urban Green Space Expansion\n",
            "Mean Recommendation: 63.6343\n",
            "Avg Change from Baseline: -38.0578\n",
            "MAE: 105.6936\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 99.6972\n",
            "Correlation to Reward: -0.5609\n",
            "Model Confidence (LogVar): -0.8836\n",
            "\n",
            "action_Waste Management Reforms\n",
            "Mean Recommendation: 3890825.2500\n",
            "Avg Change from Baseline: +2890825.5000\n",
            "MAE: 3072409.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1442773.0000\n",
            "Correlation to Reward: 0.4744\n",
            "Model Confidence (LogVar): 0.9762\n",
            "\n",
            "action_Water Consumption Tax\n",
            "Mean Recommendation: 4.0268\n",
            "Avg Change from Baseline: +0.9907\n",
            "MAE: 3.4708\n",
            "Modified Rows: 199999/200000\n",
            "Policy Volatility: 3.4065\n",
            "Correlation to Reward: 0.2776\n",
            "Model Confidence (LogVar): 0.7172\n",
            "\n",
            "action_Water conservation measures\n",
            "Mean Recommendation: 3555755.0000\n",
            "Avg Change from Baseline: +2555755.0000\n",
            "MAE: 2840311.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1718072.8750\n",
            "Correlation to Reward: 0.5258\n",
            "Model Confidence (LogVar): 0.9806\n",
            "\n",
            "EnergyAgents Report\n",
            "Critic: 0.4724 Actor_cont: -2.1293 Actor_dis: 0.0448\n",
            "Epoch 1/50 completed\n",
            "\n",
            "Critic: 0.3056 Actor_cont: -2.7822 Actor_dis: -0.0473\n",
            "Epoch 2/50 completed\n",
            "\n",
            "Critic: 0.2120 Actor_cont: -3.3670 Actor_dis: -0.0377\n",
            "Epoch 3/50 completed\n",
            "\n",
            "Critic: 0.1410 Actor_cont: -3.3107 Actor_dis: 0.1101\n",
            "Epoch 4/50 completed\n",
            "\n",
            "Critic: 0.1847 Actor_cont: -3.5682 Actor_dis: -0.0819\n",
            "Epoch 5/50 completed\n",
            "\n",
            "Critic: 0.1465 Actor_cont: -3.5201 Actor_dis: 0.1282\n",
            "Epoch 6/50 completed\n",
            "\n",
            "Critic: 0.1355 Actor_cont: -3.6566 Actor_dis: 0.0481\n",
            "Epoch 7/50 completed\n",
            "\n",
            "Critic: 0.1504 Actor_cont: -3.6028 Actor_dis: 0.0805\n",
            "Epoch 8/50 completed\n",
            "\n",
            "Critic: 0.1255 Actor_cont: -3.6154 Actor_dis: 0.1937\n",
            "Epoch 9/50 completed\n",
            "\n",
            "Critic: 0.1222 Actor_cont: -3.5548 Actor_dis: 0.1705\n",
            "Epoch 10/50 completed\n",
            "\n",
            "Critic: 0.0882 Actor_cont: -3.7858 Actor_dis: 0.0660\n",
            "Epoch 11/50 completed\n",
            "\n",
            "Critic: 0.0981 Actor_cont: -3.8316 Actor_dis: 0.0096\n",
            "Epoch 12/50 completed\n",
            "\n",
            "Critic: 0.0965 Actor_cont: -3.7697 Actor_dis: 0.0092\n",
            "Epoch 13/50 completed\n",
            "\n",
            "Critic: 0.1093 Actor_cont: -3.7433 Actor_dis: 0.0338\n",
            "Epoch 14/50 completed\n",
            "\n",
            "Critic: 0.0749 Actor_cont: -3.7872 Actor_dis: 0.1037\n",
            "Epoch 15/50 completed\n",
            "\n",
            "Critic: 0.1163 Actor_cont: -3.7245 Actor_dis: 0.1059\n",
            "Epoch 16/50 completed\n",
            "\n",
            "Critic: 0.0900 Actor_cont: -3.7527 Actor_dis: 0.0816\n",
            "Epoch 17/50 completed\n",
            "\n",
            "Critic: 0.1011 Actor_cont: -3.7193 Actor_dis: 0.0716\n",
            "Epoch 18/50 completed\n",
            "\n",
            "Critic: 0.0736 Actor_cont: -3.8367 Actor_dis: -0.0220\n",
            "Epoch 19/50 completed\n",
            "\n",
            "Critic: 0.0823 Actor_cont: -3.7997 Actor_dis: 0.0540\n",
            "Epoch 20/50 completed\n",
            "\n",
            "Critic: 0.0829 Actor_cont: -3.9000 Actor_dis: -0.0771\n",
            "Epoch 21/50 completed\n",
            "\n",
            "Critic: 0.1025 Actor_cont: -3.8266 Actor_dis: -0.0296\n",
            "Epoch 22/50 completed\n",
            "\n",
            "Critic: 0.1325 Actor_cont: -4.0491 Actor_dis: -0.2066\n",
            "Epoch 23/50 completed\n",
            "\n",
            "Critic: 0.1085 Actor_cont: -3.9303 Actor_dis: -0.0808\n",
            "Epoch 24/50 completed\n",
            "\n",
            "Critic: 0.0778 Actor_cont: -3.9036 Actor_dis: -0.0094\n",
            "Epoch 25/50 completed\n",
            "\n",
            "Critic: 0.1015 Actor_cont: -3.8152 Actor_dis: 0.0348\n",
            "Epoch 26/50 completed\n",
            "\n",
            "Critic: 0.0956 Actor_cont: -3.8920 Actor_dis: -0.0334\n",
            "Epoch 27/50 completed\n",
            "\n",
            "Critic: 0.0827 Actor_cont: -3.9160 Actor_dis: -0.0382\n",
            "Epoch 28/50 completed\n",
            "\n",
            "Critic: 0.1055 Actor_cont: -3.9817 Actor_dis: -0.0895\n",
            "Epoch 29/50 completed\n",
            "\n",
            "Critic: 0.1287 Actor_cont: -4.0807 Actor_dis: -0.2412\n",
            "Epoch 30/50 completed\n",
            "\n",
            "Critic: 0.0925 Actor_cont: -3.9466 Actor_dis: -0.0914\n",
            "Epoch 31/50 completed\n",
            "\n",
            "Critic: 0.0766 Actor_cont: -3.9117 Actor_dis: -0.0614\n",
            "Epoch 32/50 completed\n",
            "\n",
            "Critic: 0.0637 Actor_cont: -3.9700 Actor_dis: -0.0444\n",
            "Epoch 33/50 completed\n",
            "\n",
            "Critic: 0.0922 Actor_cont: -3.9587 Actor_dis: -0.1204\n",
            "Epoch 34/50 completed\n",
            "\n",
            "Critic: 0.1042 Actor_cont: -4.0033 Actor_dis: -0.1419\n",
            "Epoch 35/50 completed\n",
            "\n",
            "Critic: 0.0798 Actor_cont: -3.9147 Actor_dis: -0.0029\n",
            "Epoch 36/50 completed\n",
            "\n",
            "Critic: 0.0571 Actor_cont: -3.8788 Actor_dis: 0.0570\n",
            "Epoch 37/50 completed\n",
            "\n",
            "Critic: 0.0649 Actor_cont: -3.8068 Actor_dis: 0.1241\n",
            "Epoch 38/50 completed\n",
            "\n",
            "Critic: 0.0919 Actor_cont: -3.6830 Actor_dis: 0.2807\n",
            "Epoch 39/50 completed\n",
            "\n",
            "Critic: 0.0816 Actor_cont: -3.7773 Actor_dis: 0.1523\n",
            "Epoch 40/50 completed\n",
            "\n",
            "Critic: 0.0671 Actor_cont: -3.8075 Actor_dis: 0.1313\n",
            "Epoch 41/50 completed\n",
            "\n",
            "Critic: 0.0865 Actor_cont: -3.7851 Actor_dis: 0.1659\n",
            "Epoch 42/50 completed\n",
            "\n",
            "Critic: 0.0877 Actor_cont: -3.9700 Actor_dis: -0.1249\n",
            "Epoch 43/50 completed\n",
            "\n",
            "Critic: 0.0919 Actor_cont: -3.9385 Actor_dis: -0.0033\n",
            "Epoch 44/50 completed\n",
            "\n",
            "Critic: 0.0827 Actor_cont: -3.8670 Actor_dis: 0.0879\n",
            "Epoch 45/50 completed\n",
            "\n",
            "Critic: 0.0713 Actor_cont: -4.0312 Actor_dis: -0.1086\n",
            "Epoch 46/50 completed\n",
            "\n",
            "Critic: 0.0599 Actor_cont: -3.9884 Actor_dis: -0.0587\n",
            "Epoch 47/50 completed\n",
            "\n",
            "Critic: 0.0798 Actor_cont: -3.9003 Actor_dis: -0.0376\n",
            "Epoch 48/50 completed\n",
            "\n",
            "Critic: 0.0704 Actor_cont: -3.8741 Actor_dis: 0.0248\n",
            "Epoch 49/50 completed\n",
            "\n",
            "Critic: 0.0669 Actor_cont: -3.8660 Actor_dis: -0.0144\n",
            "Epoch 50/50 completed\n",
            "\n",
            "action_Carbon Tax\n",
            "Mean Recommendation: 49.3100\n",
            "Avg Change from Baseline: -48.7516\n",
            "MAE: 98.1909\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 87.3030\n",
            "Correlation to Reward: -0.5899\n",
            "Model Confidence (LogVar): 0.8685\n",
            "\n",
            "action_Climate-Resilient Infrastructure Investment\n",
            "Mean Recommendation: 1504937.1250\n",
            "Avg Change from Baseline: +504937.0312\n",
            "MAE: 1419742.7500\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1839767.6250\n",
            "Correlation to Reward: -0.5634\n",
            "Model Confidence (LogVar): 0.8697\n",
            "\n",
            "action_Electric Vehicle (EV) Subsidies\n",
            "Mean Recommendation: 0.5522\n",
            "Avg Change from Baseline: +0.0595\n",
            "MAE: 0.2500\n",
            "Modified Rows: 199971/200000\n",
            "Policy Volatility: 0.1959\n",
            "Correlation to Reward: -0.1658\n",
            "Model Confidence (LogVar): 0.7863\n",
            "\n",
            "action_Energy Efficiency Incentives\n",
            "Mean Recommendation: 0.5270\n",
            "Avg Change from Baseline: +0.0426\n",
            "MAE: 0.2582\n",
            "Modified Rows: 199972/200000\n",
            "Policy Volatility: 0.2088\n",
            "Correlation to Reward: 0.3257\n",
            "Model Confidence (LogVar): 0.7667\n",
            "\n",
            "action_Flood defense infrastructure\n",
            "Mean Recommendation: 1674940.3750\n",
            "Avg Change from Baseline: +674940.3750\n",
            "MAE: 1537501.3750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1915358.6250\n",
            "Correlation to Reward: -0.1322\n",
            "Model Confidence (LogVar): 0.8850\n",
            "\n",
            "action_Green Business Investments\n",
            "Mean Recommendation: 1714597.5000\n",
            "Avg Change from Baseline: +714597.5625\n",
            "MAE: 1564971.3750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1930414.2500\n",
            "Correlation to Reward: -0.5616\n",
            "Model Confidence (LogVar): 0.8782\n",
            "\n",
            "action_Heatwave resilience\n",
            "Mean Recommendation: 1887377.5000\n",
            "Avg Change from Baseline: +887377.7500\n",
            "MAE: 1684653.3750\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1985461.8750\n",
            "Correlation to Reward: -0.3341\n",
            "Model Confidence (LogVar): 0.9828\n",
            "\n",
            "action_Public transport expansion\n",
            "Mean Recommendation: 1080761.8750\n",
            "Avg Change from Baseline: +80761.6562\n",
            "MAE: 1125923.5000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1557069.5000\n",
            "Correlation to Reward: 0.1030\n",
            "Model Confidence (LogVar): 0.8886\n",
            "\n",
            "action_Recycling Rate\n",
            "Mean Recommendation: 0.5295\n",
            "Avg Change from Baseline: +0.0403\n",
            "MAE: 0.2473\n",
            "Modified Rows: 199977/200000\n",
            "Policy Volatility: 0.2013\n",
            "Correlation to Reward: 0.3297\n",
            "Model Confidence (LogVar): 0.7742\n",
            "\n",
            "action_Renewable energy subsidies\n",
            "Mean Recommendation: 0.3164\n",
            "Avg Change from Baseline: -0.1749\n",
            "MAE: 0.2554\n",
            "Modified Rows: 199933/200000\n",
            "Policy Volatility: 0.1351\n",
            "Correlation to Reward: -0.5285\n",
            "Model Confidence (LogVar): 0.7597\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_Single-use plastics bans\n",
            "Mean Recommendation: 0.8460\n",
            "Avg Change from Baseline: -0.1616\n",
            "MAE: 0.9906\n",
            "Modified Rows: 174018/200000\n",
            "Policy Volatility: 0.9041\n",
            "Correlation to Reward: -0.2301\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Sustainable Land-Use Zoning\n",
            "Mean Recommendation: 1338574.5000\n",
            "Avg Change from Baseline: +338574.4375\n",
            "MAE: 1304506.1250\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 1746714.5000\n",
            "Correlation to Reward: -0.3506\n",
            "Model Confidence (LogVar): 0.8890\n",
            "\n",
            "action_Urban Green Space Expansion\n",
            "Mean Recommendation: 49.4053\n",
            "Avg Change from Baseline: -52.2868\n",
            "MAE: 103.4739\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 90.2681\n",
            "Correlation to Reward: -0.5770\n",
            "Model Confidence (LogVar): 0.8694\n",
            "\n",
            "action_Waste Management Reforms\n",
            "Mean Recommendation: 2699629.7500\n",
            "Avg Change from Baseline: +1699629.7500\n",
            "MAE: 2247287.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 2038462.1250\n",
            "Correlation to Reward: 0.6219\n",
            "Model Confidence (LogVar): 0.8620\n",
            "\n",
            "action_Water Consumption Tax\n",
            "Mean Recommendation: 3.0353\n",
            "Avg Change from Baseline: -0.0009\n",
            "MAE: 2.2292\n",
            "Modified Rows: 199990/200000\n",
            "Policy Volatility: 2.3082\n",
            "Correlation to Reward: -0.0761\n",
            "Model Confidence (LogVar): -0.9949\n",
            "\n",
            "action_Water conservation measures\n",
            "Mean Recommendation: 2515577.5000\n",
            "Avg Change from Baseline: +1515577.5000\n",
            "MAE: 2119797.0000\n",
            "Modified Rows: 200000/200000\n",
            "Policy Volatility: 2054902.1250\n",
            "Correlation to Reward: 0.6310\n",
            "Model Confidence (LogVar): 0.8770\n",
            "\n",
            "ConsumerAgents Report\n",
            "Critic: 30.0543 Actor_cont: 28.3734 Actor_dis: 30.4568\n",
            "Epoch 1/50 completed\n",
            "\n",
            "Critic: 20.9760 Actor_cont: 19.5111 Actor_dis: 21.1770\n",
            "Epoch 2/50 completed\n",
            "\n",
            "Critic: 15.3445 Actor_cont: 11.7396 Actor_dis: 13.1910\n",
            "Epoch 3/50 completed\n",
            "\n",
            "Critic: 9.1880 Actor_cont: 2.7166 Actor_dis: 4.1335\n",
            "Epoch 4/50 completed\n",
            "\n",
            "Critic: 4.9735 Actor_cont: 0.1701 Actor_dis: 1.5891\n",
            "Epoch 5/50 completed\n",
            "\n",
            "Critic: 3.3983 Actor_cont: 0.6787 Actor_dis: 2.1197\n",
            "Epoch 6/50 completed\n",
            "\n",
            "Critic: 3.1169 Actor_cont: -0.9809 Actor_dis: 0.4475\n",
            "Epoch 7/50 completed\n",
            "\n",
            "Critic: 2.6264 Actor_cont: -0.5296 Actor_dis: 0.9067\n",
            "Epoch 8/50 completed\n",
            "\n",
            "Critic: 2.3357 Actor_cont: -0.6073 Actor_dis: 0.8240\n",
            "Epoch 9/50 completed\n",
            "\n",
            "Critic: 1.7472 Actor_cont: -1.0361 Actor_dis: 0.3911\n",
            "Epoch 10/50 completed\n",
            "\n",
            "Critic: 2.4770 Actor_cont: -1.8840 Actor_dis: -0.4601\n",
            "Epoch 11/50 completed\n",
            "\n",
            "Critic: 1.3295 Actor_cont: -1.0730 Actor_dis: 0.3713\n",
            "Epoch 12/50 completed\n",
            "\n",
            "Critic: 0.9781 Actor_cont: -1.1173 Actor_dis: 0.3076\n",
            "Epoch 13/50 completed\n",
            "\n",
            "Critic: 0.8680 Actor_cont: -0.9396 Actor_dis: 0.4912\n",
            "Epoch 14/50 completed\n",
            "\n",
            "Critic: 0.8952 Actor_cont: -1.0724 Actor_dis: 0.3578\n",
            "Epoch 15/50 completed\n",
            "\n",
            "Critic: 0.7037 Actor_cont: -1.3436 Actor_dis: 0.0912\n",
            "Epoch 16/50 completed\n",
            "\n",
            "Critic: 0.6376 Actor_cont: -1.2219 Actor_dis: 0.2205\n",
            "Epoch 17/50 completed\n",
            "\n",
            "Critic: 0.4956 Actor_cont: -1.1919 Actor_dis: 0.2537\n",
            "Epoch 18/50 completed\n",
            "\n",
            "Critic: 0.4723 Actor_cont: -1.2640 Actor_dis: 0.1833\n",
            "Epoch 19/50 completed\n",
            "\n",
            "Critic: 0.4763 Actor_cont: -1.2544 Actor_dis: 0.1954\n",
            "Epoch 20/50 completed\n",
            "\n",
            "Critic: 0.5003 Actor_cont: -1.2873 Actor_dis: 0.1604\n",
            "Epoch 21/50 completed\n",
            "\n",
            "Critic: 0.4483 Actor_cont: -1.4710 Actor_dis: -0.0170\n",
            "Epoch 22/50 completed\n",
            "\n",
            "Critic: 0.3597 Actor_cont: -1.2920 Actor_dis: 0.1632\n",
            "Epoch 23/50 completed\n",
            "\n",
            "Critic: 0.2691 Actor_cont: -1.3779 Actor_dis: 0.0719\n",
            "Epoch 24/50 completed\n",
            "\n",
            "Critic: 0.2891 Actor_cont: -1.3937 Actor_dis: 0.0545\n",
            "Epoch 25/50 completed\n",
            "\n",
            "Critic: 0.2549 Actor_cont: -1.2815 Actor_dis: 0.1581\n",
            "Epoch 26/50 completed\n",
            "\n",
            "Critic: 0.3183 Actor_cont: -1.2023 Actor_dis: 0.2360\n",
            "Epoch 27/50 completed\n",
            "\n",
            "Critic: 0.3593 Actor_cont: -1.2586 Actor_dis: 0.1951\n",
            "Epoch 28/50 completed\n",
            "\n",
            "Critic: 0.2379 Actor_cont: -1.4279 Actor_dis: 0.0189\n",
            "Epoch 29/50 completed\n",
            "\n",
            "Critic: 0.3555 Actor_cont: -1.3316 Actor_dis: 0.1110\n",
            "Epoch 30/50 completed\n",
            "\n",
            "Critic: 0.3661 Actor_cont: -1.2831 Actor_dis: 0.1881\n",
            "Epoch 31/50 completed\n",
            "\n",
            "Critic: 0.2685 Actor_cont: -1.2094 Actor_dis: 0.2372\n",
            "Epoch 32/50 completed\n",
            "\n",
            "Critic: 0.3727 Actor_cont: -1.3645 Actor_dis: 0.1017\n",
            "Epoch 33/50 completed\n",
            "\n",
            "Critic: 0.2891 Actor_cont: -1.2250 Actor_dis: 0.2233\n",
            "Epoch 34/50 completed\n",
            "\n",
            "Critic: 0.1950 Actor_cont: -1.3610 Actor_dis: 0.0956\n",
            "Epoch 35/50 completed\n",
            "\n",
            "Critic: 0.3616 Actor_cont: -1.4474 Actor_dis: 0.0126\n",
            "Epoch 36/50 completed\n",
            "\n",
            "Critic: 0.2983 Actor_cont: -1.4097 Actor_dis: 0.0389\n",
            "Epoch 37/50 completed\n",
            "\n",
            "Critic: 0.3466 Actor_cont: -1.3164 Actor_dis: 0.1303\n",
            "Epoch 38/50 completed\n",
            "\n",
            "Critic: 0.1803 Actor_cont: -1.3605 Actor_dis: 0.0922\n",
            "Epoch 39/50 completed\n",
            "\n",
            "Critic: 0.2957 Actor_cont: -1.2310 Actor_dis: 0.2242\n",
            "Epoch 40/50 completed\n",
            "\n",
            "Critic: 0.2328 Actor_cont: -1.3328 Actor_dis: 0.1265\n",
            "Epoch 41/50 completed\n",
            "\n",
            "Critic: 0.2740 Actor_cont: -1.2199 Actor_dis: 0.2460\n",
            "Epoch 42/50 completed\n",
            "\n",
            "Critic: 0.2684 Actor_cont: -1.4165 Actor_dis: 0.0364\n",
            "Epoch 43/50 completed\n",
            "\n",
            "Critic: 0.2207 Actor_cont: -1.4674 Actor_dis: -0.0095\n",
            "Epoch 44/50 completed\n",
            "\n",
            "Critic: 0.1726 Actor_cont: -1.3916 Actor_dis: 0.0610\n",
            "Epoch 45/50 completed\n",
            "\n",
            "Critic: 0.2012 Actor_cont: -1.3623 Actor_dis: 0.0902\n",
            "Epoch 46/50 completed\n",
            "\n",
            "Critic: 0.3879 Actor_cont: -1.2046 Actor_dis: 0.2591\n",
            "Epoch 47/50 completed\n",
            "\n",
            "Critic: 0.1672 Actor_cont: -1.4988 Actor_dis: -0.0387\n",
            "Epoch 48/50 completed\n",
            "\n",
            "Critic: 0.3320 Actor_cont: -1.2669 Actor_dis: 0.1872\n",
            "Epoch 49/50 completed\n",
            "\n",
            "Critic: 0.2780 Actor_cont: -1.3720 Actor_dis: 0.0886\n",
            "Epoch 50/50 completed\n",
            "\n",
            "action_Carbon Tax\n",
            "Mean Recommendation: 188.7636\n",
            "Avg Change from Baseline: +90.2367\n",
            "MAE: 90.2367\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0000\n",
            "Correlation to Reward: nan\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Climate-Resilient Infrastructure Investment\n",
            "Mean Recommendation: 367894.2812\n",
            "Avg Change from Baseline: -632105.8125\n",
            "MAE: 632105.8125\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 464.7455\n",
            "Correlation to Reward: -0.0004\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Electric Vehicle (EV) Subsidies\n",
            "Mean Recommendation: 0.4914\n",
            "Avg Change from Baseline: -0.1613\n",
            "MAE: 0.1948\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0001\n",
            "Correlation to Reward: 0.0351\n",
            "Model Confidence (LogVar): 1.0000\n",
            "\n",
            "action_Energy Efficiency Incentives\n",
            "Mean Recommendation: 0.2765\n",
            "Avg Change from Baseline: -0.1953\n",
            "MAE: 0.2071\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0001\n",
            "Correlation to Reward: 0.0570\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Flood defense infrastructure\n",
            "Mean Recommendation: 367879.9688\n",
            "Avg Change from Baseline: -632120.1250\n",
            "MAE: 632120.1250\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 470.0787\n",
            "Correlation to Reward: 0.0041\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Green Business Investments\n",
            "Mean Recommendation: 4481681.5000\n",
            "Avg Change from Baseline: +3481681.2500\n",
            "MAE: 3481681.2500\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 6796.7881\n",
            "Correlation to Reward: -0.0485\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Heatwave resilience\n",
            "Mean Recommendation: 396675.6250\n",
            "Avg Change from Baseline: -603324.5000\n",
            "MAE: 652068.1875\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 342979.6250\n",
            "Correlation to Reward: 0.0035\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Public transport expansion\n",
            "Mean Recommendation: 4481922.5000\n",
            "Avg Change from Baseline: +3481923.0000\n",
            "MAE: 3481923.0000\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 6763.1123\n",
            "Correlation to Reward: -0.0230\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Recycling Rate\n",
            "Mean Recommendation: 0.3716\n",
            "Avg Change from Baseline: -0.1094\n",
            "MAE: 0.1478\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0001\n",
            "Correlation to Reward: 0.0203\n",
            "Model Confidence (LogVar): 0.6758\n",
            "\n",
            "action_Renewable energy subsidies\n",
            "Mean Recommendation: 0.7633\n",
            "Avg Change from Baseline: +0.1676\n",
            "MAE: 0.1790\n",
            "Modified Rows: 2998/3000\n",
            "Policy Volatility: 0.0007\n",
            "Correlation to Reward: -0.0687\n",
            "Model Confidence (LogVar): 0.7887\n",
            "\n",
            "action_Single-use plastics bans\n",
            "Mean Recommendation: -0.0000\n",
            "Avg Change from Baseline: -1.2463\n",
            "MAE: 1.2463\n",
            "Modified Rows: 2339/3000\n",
            "Policy Volatility: Optimal\n",
            "Correlation to Reward: Converged\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Sustainable Land-Use Zoning\n",
            "Mean Recommendation: 4481768.0000\n",
            "Avg Change from Baseline: +3481768.0000\n",
            "MAE: 3481768.0000\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 6840.5347\n",
            "Correlation to Reward: 0.0072\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Urban Green Space Expansion\n",
            "Mean Recommendation: 17.7991\n",
            "Avg Change from Baseline: -65.2585\n",
            "MAE: 65.2585\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0018\n",
            "Correlation to Reward: 0.0115\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Waste Management Reforms\n",
            "Mean Recommendation: 367871.9062\n",
            "Avg Change from Baseline: -632128.2500\n",
            "MAE: 632128.2500\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 476.2759\n",
            "Correlation to Reward: -0.0148\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Water Consumption Tax\n",
            "Mean Recommendation: 4.9001\n",
            "Avg Change from Baseline: +1.9281\n",
            "MAE: 1.9281\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 0.0005\n",
            "Correlation to Reward: 0.0253\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n",
            "action_Water conservation measures\n",
            "Mean Recommendation: 367877.2812\n",
            "Avg Change from Baseline: -632122.8750\n",
            "MAE: 632122.8750\n",
            "Modified Rows: 3000/3000\n",
            "Policy Volatility: 472.1480\n",
            "Correlation to Reward: -0.0043\n",
            "Model Confidence (LogVar): -1.0000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results={}\n",
        "for agent_class in agents:\n",
        "  print(f\"Agent: {agent_class}\")\n",
        "  file_name = f\"{agent_class}_results.csv\"\n",
        "  summary = pd.read_csv(file_name)\n",
        "  results[agent_class]=summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWkuOXb_0tRr",
        "outputId": "ba89a2c0-54b5-4c7d-ebab-c421fe3eb39d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent: Sustainability\n",
            "Agent: DecisionAgents\n",
            "Agent: BusinessAgents\n",
            "Agent: EnergyAgents\n",
            "Agent: ConsumerAgents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_signature={}\n",
        "for agent_class in agents:\n",
        "  print(f\"Agent: {agent_class}\")\n",
        "  file_name = f\"{agent_class}_policy_signature.csv\"\n",
        "  summary = pd.read_csv(file_name)\n",
        "  policy_signature[agent_class]=summary\n",
        "  print(summary.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GFvdCj107vl",
        "outputId": "8fed08a5-3a6f-4b11-8891-e91eba143c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent: Sustainability\n",
            "Index(['Agent', 'State_Variable', 'Action', 'Impact', 'Correlation_r'], dtype='object')\n",
            "Agent: DecisionAgents\n",
            "Index(['Agent', 'State_Variable', 'Action', 'Impact', 'Correlation_r'], dtype='object')\n",
            "Agent: BusinessAgents\n",
            "Index(['Agent', 'State_Variable', 'Action', 'Impact', 'Correlation_r'], dtype='object')\n",
            "Agent: EnergyAgents\n",
            "Index(['Agent', 'State_Variable', 'Action', 'Impact', 'Correlation_r'], dtype='object')\n",
            "Agent: ConsumerAgents\n",
            "Index(['Agent', 'State_Variable', 'Action', 'Impact', 'Correlation_r'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PZXq_IaJxy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!pip install spacy_transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_aauhK5f1Xe",
        "outputId": "6d5d9d43-fdaf-4580-9ffe-a348f1fcd8cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting spacy_transformers\n",
            "  Downloading spacy_transformers-1.3.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: spacy<4.1.0,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from spacy_transformers) (3.8.11)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy_transformers) (2.0.2)\n",
            "Collecting transformers<4.50.0,>=3.4.0 (from spacy_transformers)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from spacy_transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from spacy_transformers) (2.5.2)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers)\n",
            "  Downloading spacy_alignments-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (1.1.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy<4.1.0,>=3.5.0->spacy_transformers) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (3.6.1)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->spacy_transformers) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers) (2025.11.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.50.0,>=3.4.0->spacy_transformers)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy_transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy_transformers) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy_transformers) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.1.0,>=3.5.0->spacy_transformers) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.1.0,>=3.5.0->spacy_transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->spacy_transformers) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy_transformers) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4.1.0,>=3.5.0->spacy_transformers) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy<4.1.0,>=3.5.0->spacy_transformers) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy_transformers) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy_transformers) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy<4.1.0,>=3.5.0->spacy_transformers) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<4.1.0,>=3.5.0->spacy_transformers) (2.0.1)\n",
            "Downloading spacy_transformers-1.3.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.8/795.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_alignments-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-alignments, tokenizers, transformers, spacy_transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.3\n",
            "    Uninstalling transformers-4.57.3:\n",
            "      Successfully uninstalled transformers-4.57.3\n",
            "Successfully installed spacy-alignments-0.9.2 spacy_transformers-1.3.9 tokenizers-0.21.4 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsGl6Tjox4Ok",
        "outputId": "7028568e-752f-4621-ec16-c96b548560d9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [\n",
        "\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"celsius\", \"kelvin\", \"fahrenheit\", \"c\", \"f\", \"k\", \"mm\", \"inch\", \"cm\", \"ton\", \"kg\", \"t\", \"liter\", \"m3\"]}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"TEXT\": {\"REGEX\": r\"^([°º][CcFf]?|[CcFfKk])$\"}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"TEXT\": {\"IN\": [\"%\", \"percent\"]}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"IS_CURRENCY\": True, \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"million\", \"billion\", \"trillion\"]}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"hectare\", \"ha\", \"acre\", \"sqkm\", \"km2\", \"m2\"]}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"TEXT\": {\"REGEX\": \"(?i)kg/kwh|g/kwh|t/mwh|co2e\"}}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"m3\", \"liter\", \"gal\", \"tonne\", \"ton\"]}}, {\"TEXT\": {\"IN\": [\"/day\", \"/year\", \"capita\"]}, \"OP\": \"?\"}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"year\", \"month\", \"day\", \"week\"]}}]},\n",
        "\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"temperature\", \"precipitation\", \"humidity\", \"population\", \"gdp\", \"employment\", \"growth\"]}}, {\"LEMMA\": \"rate\", \"OP\": \"?\"}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"air\", \"water\"]}}, {\"LEMMA\": {\"IN\": [\"pollution\", \"quality\"]}}, {\"LEMMA\": \"index\"}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LEMMA\": \"carbon\"}, {\"LEMMA\": \"dioxide\"}, {\"LEMMA\": \"emission\"}]},\n",
        "    {\"label\": \"STATE_VARIABLE\", \"pattern\": [{\"LEMMA\": \"renewable\"}, {\"LEMMA\": \"share\"}]},\n",
        "\n",
        "    {\"label\": \"ENERGY_VARIABLE\", \"pattern\": [{\"LIKE_NUM\": True}, {\"LEMMA\": {\"IN\": [\"mw\", \"gw\", \"kw\", \"kwh\", \"mwh\", \"gj\"]}}]},\n",
        "    {\"label\": \"ENERGY_VARIABLE\", \"pattern\": [{\"LEMMA\": \"energy\"}, {\"LEMMA\": {\"IN\": [\"type\", \"capacity\", \"production\", \"intensity\", \"consumption\"]}}]},\n",
        "\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"carbon\", \"water\"]}}, {\"LEMMA\": \"tax\"}]},\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"renewable\", \"electric\", \"vehicle\", \"ev\", \"energy\"]}}, {\"LEMMA\": {\"IN\": [\"subsidy\", \"incentive\", \"standard\"]}}]},\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": \"fossil\"}, {\"LEMMA\": \"fuel\"}, {\"LEMMA\": \"phase-out\"}]},\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"urban\", \"public\", \"transport\"]}}, {\"LEMMA\": \"expansion\"}]},\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"waste\", \"land-use\", \"flood\", \"heatwave\"]}}, {\"LEMMA\": {\"IN\": [\"reform\", \"zoning\", \"defense\", \"resilience\", \"management\"]}}]},\n",
        "    {\"label\": \"ACTION\", \"pattern\": [{\"LEMMA\": \"single-use\"}, {\"LEMMA\": \"plastic\"}, {\"LEMMA\": \"ban\"}]},\n",
        "\n",
        "    {\"label\": \"BUSINESS_VARIABLE\", \"pattern\": [{\"LEMMA\": \"business\"}, {\"LEMMA\": {\"IN\": [\"name\", \"type\", \"investment\"]}}]},\n",
        "    {\"label\": \"BUSINESS_VARIABLE\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"valuation\", \"revenue\", \"wealth\"]}}]},\n",
        "    {\"label\": \"BUSINESS_VARIABLE\", \"pattern\": [{\"IS_CURRENCY\": True}, {\"LIKE_NUM\": True}]},\n",
        "\n",
        "    {\"label\": \"DECISION_VARIABLE\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"authority\", \"budget\", \"sector\", \"regulation\", \"penalty\"]}}, {\"LEMMA\": {\"IN\": [\"strictness\", \"severity\", \"capacity\", \"ceiling\", \"floor\"]}, \"OP\": \"?\"}]},\n",
        "    {\"label\": \"DECISION_VARIABLE\", \"pattern\": [{\"LEMMA\": {\"IN\": [\"economic\", \"emission\", \"sustainability\", \"social\"]}}, {\"LEMMA\": \"priority\"}]},\n",
        "    {\"label\": \"CONSUMER_VARIABLE\", \"pattern\": [{\"LEMMA\": \"consumer\"}, {\"LEMMA\": \"class\"}]},\n",
        "    {\"label\": \"CONSUMER_VARIABLE\", \"pattern\": [{\"LEMMA\": \"public\"}, {\"LEMMA\": \"satisfaction\"}]}\n",
        "]"
      ],
      "metadata": {
        "id": "BBn1iGJx2m7h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [\"production_level\", \"net_emissions\", \"net_carbon_intensity\", \"energy_consumption\",\n",
        "\"water_consumption\", \"resources_consumption\", \"waste_creation\", \"waste_recycling\",\n",
        "\"Temperature\", \"Precipitation\", \"Humidity\", \"Air Pollution Index\",\n",
        "\"Water Quality Index\", \"Carbon Dioxide Emissions Per Capita\", \"Electricity Consumption Per Capita\", \\\n",
        "\"Renewable Share\", \"Water Consumption Per Capita\", \"Population\", \"GDP\",\n",
        "\"Employment Rate\", \"Waste Management Efficiency\", \"Energy Efficiency\",\n",
        "\"Urban Green Space Expansion\", \"Flood defense infrastructure\", \"Heatwave resilience\",\n",
        "\"Sustainable Land-Use Zoning\", \"Single-use plastics bans\", \"Green Business Investments\",\n",
        "\"Carbon Tax\", \"Climate-Resilient Infrastructure Investment\",\n",
        "\"Electric Vehicle (EV) Subsidies\", \"Energy Efficiency Incentives\",\n",
        "\"Fossil Fuel Phase-Out Regulations\", \"Fuel Economy Standards\", \"Public transport expansion\",\n",
        "\"Recycling Rate\", \"Renewable energy subsidies\", \"Vehicle emission standards\",\n",
        "\"Waste Management Reforms\", \"Water Consumption Tax\", \"Water conservation measures\",\n",
        "\"budget\", \"tax_capacity\", \"subsidy_capacity\", \"investment_ceiling\", \"investment_floor\", \"economic_growth_priority\",\n",
        "\"emissions_priority\", \"sustainability_priority\", \"energy_priority\", \"social_welfare_priority\",\n",
        "\"regulation_strictness\", \"penalty_severity\", \"incentive_intensity\", \"valuation\", \"revenue\",\n",
        "\"growth_rate\", \"sustainability_index\", \"capacity\", \"production\", \"efficiency\", \"carbon_intensity\",\n",
        "\"emissions\", \"class_population\", \"class_wealth\", \"public_satisfaction\"]"
      ],
      "metadata": {
        "id": "BekS1mArD386"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy.matcher import DependencyMatcher\n",
        "\n",
        "def feature_extraction(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    if \"entity_ruler\" not in nlp.pipe_names:\n",
        "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\", config={\"overwrite_ents\": True})\n",
        "        ruler.add_patterns(patterns)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    unique_res = {}\n",
        "    target_params = [p.lower() for p in params]\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.lower()\n",
        "\n",
        "        sent_ents = [\n",
        "            ent for ent in sent.ents\n",
        "            if ent.label_ in [\"MONEY\", \"QUANTITY\", \"PERCENT\", \"CARDINAL\"]\n",
        "            or any(char.isdigit() for char in ent.text)\n",
        "        ]\n",
        "\n",
        "        if not sent_ents:\n",
        "            continue\n",
        "\n",
        "        for p in target_params:\n",
        "            if p in sent_text:\n",
        "                param_start_pos = sent_text.find(p)\n",
        "\n",
        "                best_ent = None\n",
        "                min_dist = float('inf')\n",
        "\n",
        "                for ent in sent_ents:\n",
        "                    ent_text = ent.text.lower()\n",
        "                    if ent_text == p or ent_text.isdigit():\n",
        "                        continue\n",
        "\n",
        "                    ent_start_pos = sent.text.lower().find(ent_text)\n",
        "                    dist = abs(ent_start_pos - param_start_pos)\n",
        "\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        best_ent = ent.text.strip()\n",
        "\n",
        "                if best_ent:\n",
        "                    unique_res[p] = best_ent\n",
        "\n",
        "    return unique_res"
      ],
      "metadata": {
        "id": "KI3TVFvy4fu6"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def quantify(text):\n",
        "    if not text:\n",
        "        return 0.0\n",
        "    text = str(text).lower().replace(',', '').strip()\n",
        "\n",
        "    number_match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n",
        "    if not number_match:\n",
        "        return 0.0\n",
        "    val = float(number_match.group())\n",
        "\n",
        "    if \"trillion\" in text or \"t\" in text.split():\n",
        "        val *= 1e12\n",
        "    elif \"billion\" in text or \"b\" in text.split():\n",
        "        val *= 1e9\n",
        "    elif \"million\" in text or \"m\" in text.split():\n",
        "        val *= 1e6\n",
        "    if \"%\" in text or \"percent\" in text:\n",
        "        val /= 100.0\n",
        "    if \"gw\" in text:\n",
        "        val *= 1000\n",
        "    elif \"kw\" in text:\n",
        "        val /= 1000\n",
        "    elif \"kwh\" in text:\n",
        "        val /= 1000\n",
        "    if \"ton\" in text or \"t\" in text.split():\n",
        "        pass\n",
        "    if \"mm\" in text:\n",
        "        pass\n",
        "    if \"sqkm\" in text or \"km2\" in text:\n",
        "        val = val *10000\n",
        "    elif \"hectare\" in text or \"ha\" in text:\n",
        "        val = val\n",
        "    if \"f\" in text or \"fahrenheit\" in text:\n",
        "        val = (val - 32) * 5/9\n",
        "    elif \"k\" in text or \"kelvin\" in text:\n",
        "        val = val - 273.15\n",
        "\n",
        "    return val"
      ],
      "metadata": {
        "id": "qemTBsZZasC-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "MAPPING = {\n",
        "    'Air Pollution Index': 'state_Air Pollution Index',\n",
        "    'Carbon Dioxide Emissions Per Capita': 'state_Carbon Dioxide Emissions Per Capita',\n",
        "    'Employment Rate': 'state_Employment Rate',\n",
        "    'Energy Efficiency': 'state_Energy Efficiency',\n",
        "    'Flood defense infrastructure': 'state_Flood defense infrastructure',\n",
        "    'GDP': 'state_GDP',\n",
        "    'Green Business Investments': 'state_Green Business Investments',\n",
        "    'Humidity': 'state_Humidity',\n",
        "    'Precipitation': 'state_Precipitation',\n",
        "    'Population': 'state_Population',\n",
        "    'Single-use plastics bans': 'state_Single-use plastics bans',\n",
        "    'Sustainable Land-Use Zoning': 'state_Sustainable Land-Use Zoning',\n",
        "    'Temperature': 'state_Temperature',\n",
        "    'Urban Green Space Expansion': 'state_Urban Green Space Expansion',\n",
        "    'Waste Management Efficiency': 'state_Waste Management Efficiency',\n",
        "    'budget': 'state_budget',\n",
        "    'capacity': 'state_capacity',\n",
        "    'carbon_intensity': 'state_carbon_intensity',\n",
        "    'class_population': 'state_class_population',\n",
        "    'class_wealth': 'state_class_wealth',\n",
        "    'economic_growth_priority': 'state_economic_growth_priority',\n",
        "    'efficiency': 'state_efficiency',\n",
        "    'emissions': 'state_emissions',\n",
        "    'incentive_intensity': 'state_incentive_intensity',\n",
        "    'investment_ceiling': 'state_investment_ceiling',\n",
        "    'investment_floor': 'state_investment_floor',\n",
        "    'net_carbon_intensity': 'state_net_carbon_intensity',\n",
        "    'net_emissions': 'state_net_emissions',\n",
        "    'penalty_severity': 'state_penalty_severity',\n",
        "    'production': 'state_production',\n",
        "    'production_level': 'state_production_level',\n",
        "    'revenue': 'state_revenue',\n",
        "    'valuation': 'state_valuation',\n",
        "    'sustainability_index': 'state_sustainability_index',\n",
        "    'tax_capacity': 'state_tax_capacity',\n",
        "    'water_consumption': 'state_water_consumption',\n",
        "    'waste_creation': 'state_waste_creation',\n",
        "    'waste_recycling': 'state_waste_recycling',\n",
        "    \"tax rebates\": \"state_incentive_intensity\",\n",
        "    \"green investment\": \"state_Green Business Investments\",\n",
        "    \"factory output\": \"state_production_level\"\n",
        "}\n",
        "\n",
        "POLICY_STATE_FEATURES_DICT = {\n",
        "    'Urban Green Space Expansion': 'state_Urban Green Space Expansion',\n",
        "    'Flood defense infrastructure': 'state_Flood defense infrastructure',\n",
        "    'Heatwave resilience': 'state_Heatwave resilience',\n",
        "    'Sustainable Land-Use Zoning': 'state_Sustainable Land-Use Zoning',\n",
        "    'Single-use plastics bans': 'state_Single-use plastics bans',\n",
        "    'Green Business Investments': 'state_Green Business Investments'\n",
        "}\n",
        "\n",
        "action_mapping = {\n",
        "    \"carbon_tax\": \"action_Carbon Tax\",\n",
        "    \"climate_resilient_infrastructure_investment\": \"action_Climate-Resilient Infrastructure Investment\",\n",
        "    \"electric_vehicle_subsidies\": \"action_Electric Vehicle (EV) Subsidies\",\n",
        "    \"energy_efficiency_incentives\": \"action_Energy Efficiency Incentives\",\n",
        "    \"flood_defense_infrastructure\": \"action_Flood defense infrastructure\",\n",
        "    \"fossil_fuel_phase_out_regulations\": \"action_Fossil Fuel Phase-Out Regulations\",\n",
        "    \"fuel_economy_standards\": \"action_Fuel Economy Standards\",\n",
        "    \"green_business_investments\": \"action_Green Business Investments\",\n",
        "    \"heatwave_resilience\": \"action_Heatwave resilience\",\n",
        "    \"public_transport_expansion\": \"action_Public transport expansion\",\n",
        "    \"recycling_rate\": \"action_Recycling Rate\",\n",
        "    \"renewable_energy_subsidies\": \"action_Renewable energy subsidies\",\n",
        "    \"single_use_plastics_bans\": \"action_Single-use plastics bans\",\n",
        "    \"sustainable_land_use_zoning\": \"action_Sustainable Land-Use Zoning\",\n",
        "    \"urban_green_space_expansion\": \"action_Urban Green Space Expansion\",\n",
        "    \"vehicle_emission_standards\": \"action_Vehicle emission standards\",\n",
        "    \"waste_management_reforms\": \"action_Waste Management Reforms\",\n",
        "    \"water_consumption_tax\": \"action_Water Consumption Tax\",\n",
        "    \"water_conservation_measures\": \"action_Water conservation measures\"\n",
        "}\n",
        "\n",
        "\n",
        "def col_match(nlp_key,dataset_cols):\n",
        "  max_match=None\n",
        "  highest_score=0\n",
        "  nlp_key_c=nlp_key.lower()\n",
        "  for col in dataset_cols:\n",
        "    if col.startswith('state_'):\n",
        "      col_c=col.replace('state_',\"\").replace('_',\" \").lower()\n",
        "    elif col.startswith('action_'):\n",
        "      col_c=col.replace('action_',\"\").replace('_',\" \").lower()\n",
        "    elif col.startswith('next_'):\n",
        "      col_c=col.replace('next_',\"\").replace('_',\" \").lower()\n",
        "\n",
        "    if nlp_key_c in col_c or col_c in nlp_key_c:\n",
        "      return col\n",
        "\n",
        "    score=SequenceMatcher(None,nlp_key_c,col_c).ratio()\n",
        "    if score>highest_score:\n",
        "      highest_score=score\n",
        "      max_match=col\n",
        "\n",
        "  if highest_score>0.55:\n",
        "    return max_match\n",
        "\n",
        "def input_vector(nlp_res, agent_state_cols, agent_action_cols):\n",
        "    final_states = {col: 0.0 for col in agent_state_cols}\n",
        "    final_actions = {col: 0.0 for col in agent_action_cols}\n",
        "\n",
        "    for k, v in nlp_res.items():\n",
        "        k_lower = k.lower()\n",
        "        target_state = MAPPING.get(k_lower)\n",
        "        if not target_state:\n",
        "            target_state = POLICY_STATE_FEATURES_DICT.get(k.title())\n",
        "\n",
        "        target_action = action_mapping.get(k_lower.replace(\" \", \"_\"))\n",
        "        if not target_state and not target_action:\n",
        "            potential_col = col_match(k, list(agent_state_cols) + list(agent_action_cols))\n",
        "            if potential_col:\n",
        "                if potential_col.startswith('state_'):\n",
        "                    target_state = potential_col\n",
        "                elif potential_col.startswith('action_'):\n",
        "                    target_action = potential_col\n",
        "\n",
        "        if target_state and target_state in final_states:\n",
        "            final_states[target_state] = v\n",
        "        elif target_action and target_action in final_actions:\n",
        "            final_actions[target_action] = v\n",
        "\n",
        "    return final_states, final_actions\n",
        "\n",
        "def prepare_for_model(vector_data, state_scaler, log_cols):\n",
        "  if isinstance(vector_data, dict):\n",
        "        vector_df = pd.DataFrame([vector_data])\n",
        "  else:\n",
        "        vector_df = vector_data.copy()\n",
        "  for col in log_cols:\n",
        "      if col in vector_df.columns:\n",
        "          vector_df[col] = np.log1p(vector_df[col].clip(0, 1e6))\n",
        "  scaled_vector = state_scaler.transform(vector_df)\n",
        "  return scaled_vector\n",
        "\n",
        "def get_cont_info(agent_class):\n",
        "    a_scaler = joblib.load(f\"{agent_class}_action_scaler.joblib\")\n",
        "    cont_actions = a_scaler.feature_names_in_\n",
        "\n",
        "    action_info = []\n",
        "    for i, name in enumerate(cont_actions):\n",
        "        a_min = a_scaler.data_min_[i] if hasattr(a_scaler, 'data_min_') else -1.0\n",
        "        a_max = a_scaler.data_max_[i] if hasattr(a_scaler, 'data_max_') else 1.0\n",
        "        if a_min == a_max:\n",
        "            a_min -= 1.0\n",
        "            a_max += 1.0\n",
        "\n",
        "        action_info.append((name, a_min, a_max))\n",
        "\n",
        "    return action_info"
      ],
      "metadata": {
        "id": "BF9TWv69cKLX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1yULimH_O3PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "def extract_and_analyze(policy_text, agent_classes):\n",
        "    extracted_features = feature_extraction(policy_text)\n",
        "    print(extracted_features)\n",
        "    nlp_res = {k: quantify(v) for k, v in extracted_features.items()}\n",
        "    analysis_report = []\n",
        "\n",
        "    for agent_class in agent_classes:\n",
        "        agent_graph = tf.Graph()\n",
        "        with agent_graph.as_default():\n",
        "            try:\n",
        "                s_scaler = joblib.load(f\"{agent_class}_state_scaler.joblib\")\n",
        "                a_scaler = joblib.load(f\"{agent_class}_action_scaler.joblib\")\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            state_cols = s_scaler.feature_names_in_.tolist()\n",
        "            action_cols = a_scaler.feature_names_in_.tolist()\n",
        "            state_dict, action_dict = input_vector(nlp_res, state_cols, action_cols)\n",
        "\n",
        "            log_keywords = ['budget', 'gdp', 'emissions', 'population', 'revenue', 'valuation', 'infrastructure', 'capacity']\n",
        "\n",
        "            s_df = pd.DataFrame([state_dict])[state_cols]\n",
        "            for col in state_cols:\n",
        "                if any(k in col.lower() for k in log_keywords):\n",
        "                    s_df[col] = np.log1p(s_df[col].clip(0, None))\n",
        "            s_scaled = s_scaler.transform(s_df).astype(np.float32)\n",
        "            s_scaled = np.clip(s_scaled, -3.0, 3.0)\n",
        "\n",
        "            a_df = pd.DataFrame([action_dict])[action_cols]\n",
        "            for col in action_cols:\n",
        "                if any(k in col.lower() for k in log_keywords):\n",
        "                    a_df[col] = np.log1p(a_df[col].clip(0, None))\n",
        "            a_scaled = a_scaler.transform(a_df).astype(np.float32)\n",
        "\n",
        "            train_action_mean = a_scaler.mean_.reshape(1, -1).astype(np.float32)\n",
        "            train_action_std = np.sqrt(a_scaler.var_).reshape(1, -1).astype(np.float32)\n",
        "\n",
        "            sess = tf.compat.v1.Session()\n",
        "            action_info = get_cont_info(agent_class)\n",
        "            dis_actions = [\n",
        "                ('action_Fossil Fuel Phase-Out Regulations', 3),\n",
        "                ('action_Fuel Economy Standards', 4),\n",
        "                ('state_Single-use plastics bans', 3),\n",
        "                ('action_Vehicle emission standards', 4)\n",
        "            ]\n",
        "\n",
        "            model = A3C(scope=\"TrainNet\", session=sess,\n",
        "                        MAX_STATE_NO=len(state_cols),\n",
        "                        action_continuous=action_info,\n",
        "                        action_discrete=dis_actions)\n",
        "\n",
        "            checkpoint_path = f\"./models/{agent_class}_policy_model.ckpt\"\n",
        "            reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)\n",
        "            ckpt_vars = reader.get_variable_to_shape_map().keys()\n",
        "\n",
        "            graph_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope=\"TrainNet\")\n",
        "            vars_to_restore = [v for v in graph_vars if v.name.split(':')[0] in ckpt_vars and \"Adam\" not in v.name]\n",
        "\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "            if vars_to_restore:\n",
        "                saver = tf.compat.v1.train.Saver(var_list=vars_to_restore)\n",
        "                saver.restore(sess, checkpoint_path)\n",
        "\n",
        "            v_val, a_mean = sess.run(\n",
        "                [model.v, model.mean],\n",
        "                feed_dict={\n",
        "                    model.s: s_scaled,\n",
        "                    model.current_action: a_scaled,\n",
        "                    model.dataset_mean: train_action_mean,\n",
        "                    model.dataset_std: train_action_std\n",
        "                }\n",
        "            )\n",
        "\n",
        "            rec_unscaled = a_scaler.inverse_transform(a_mean)\n",
        "            res = {\"Agent\": agent_class, \"Overall_Policy_Score\": float(v_val[0][0])}\n",
        "\n",
        "            for i, col in enumerate(action_cols):\n",
        "                val = rec_unscaled[0, i]\n",
        "                if any(k in col.lower() for k in log_keywords):\n",
        "                    val = np.expm1(val)\n",
        "                res[f\"Rec_{col}\"] = float(val)\n",
        "                res[f\"Original_{col}\"] = action_dict[col]\n",
        "\n",
        "            analysis_report.append(res)\n",
        "            sess.close()\n",
        "\n",
        "    return pd.DataFrame(analysis_report)\n",
        "\n",
        "df_results = extract_and_analyze(sample_policy_text, agents)\n"
      ],
      "metadata": {
        "id": "Sb2iuPiODFkS"
      },
      "execution_count": 113,
      "outputs": []
    }
  ]
}